# NodeLLM Full Documentation

> Auto-generated context for LLMs containing all project documentation.



<!-- FILE: llms.txt -->

# üìÑ llms.txt

---
layout: null
permalink: /llms.txt
---
# NodeLLM

> The Backend-First AI SDK for Node.js

NodeLLM is an open-source infrastructure layer for building provider-agnostic, production-grade LLM systems in Node.js. It standardizes integrations across OpenAI, Anthropic, Gemini, DeepSeek, Bedrock, and Ollama into a single, predictable API.

## Project Details

- **Type**: Node.js LLM SDK / Infrastructure Layer
- **Primary Use**: Building scalable AI workers, APIs, and agents
- **Languages**: JavaScript, TypeScript
- **License**: MIT
- **Package**: `@node-llm/core`
- **Testing Package**: `@node-llm/testing`
- **Repository**: `https://github.com/node-llm/node-llm`
- **Documentation**: `https://node-llm.eshaiju.com`

## Core Features

- **Provider Agnostic**: Unified API for 540+ models (OpenAI, Anthropic, Gemini, Bedrock, DeepSeek, Ollama).
- **Backend-First**: Optimized for long-running processes, CRON jobs, and persistent agents (not just frontend streaming).
- **Static Model Registry**: Zero-latency, offline access to model metadata (context window, pricing, capabilities).
- **Agentic Zero Trust**: Built-in PII redaction, guardrails, and audit logging.
- **ORM Integration**: First-class support for persisting chat history and tools via `@node-llm/orm`.
- **Deterministic Testing**: Native support for recording/replaying LLM interactions (VCR) and fluent mocking via `@node-llm/testing`.

## Architecture

1.  **Core**: Light-weight, zero-dependency abstraction layer.
2.  **Providers**: Pluggable adapters that normalize inputs/outputs.
3.  **Registry**: Static JSON database of model capabilities.
4.  **Middleware**: Concept for attaching Evals, Logging, and Redaction.

## Usage Example

```ts
import { NodeLLM } from "@node-llm/core";

// 1. Unified Interface
const chat = NodeLLM.chat("gpt-4o");

// 2. Standardized Response
const response = await chat.ask("Explain infrastructure-as-code");
```

## Comparisons

- **vs Vercel AI SDK**: NodeLLM is optimized for backend/workers, whereas Vercel AI SDK is optimized for Frontend/Next.js streaming.
- **vs LangChain**: NodeLLM is a thin infrastructure layer with a static registry, whereas LangChain is a comprehensive framework with complex abstractions.
- **vs OpenAI SDK**: NodeLLM wraps the OpenAI SDK to provide multi-provider support with identical code.

## Quick Links

- **Get Started**: https://node-llm.eshaiju.com/getting-started/overview
- **Architecture**: https://node-llm.eshaiju.com/architecture
- **Testing**: https://node-llm.eshaiju.com/core-features/testing
- **Contributing**: https://github.com/node-llm/node-llm/blob/main/CONTRIBUTING.md


<!-- END FILE: llms.txt -->
----------------------------------------

<!-- FILE: intro.md -->

# üìÑ intro.md

---
layout: default
title: Introduction
nav_order: 1
permalink: /docs/intro
---

<p align="left">
  <img src="/assets/images/logo.png" alt="NodeLLM" width="200" />
</p>

# Introduction

**A simple way to use Large Language Models in Node.js.**

**NodeLLM is an open-source infrastructure layer for building provider-agnostic, production-grade LLM systems in Node.js.**

Integrating multiple LLM providers often means juggling different SDKs, API styles, and update cycles. NodeLLM gives you a single, unified API that stays consistent even when providers change.

<p class="fs-3 text-grey-dk-000 mb-3">Unified support for</p>
<div class="provider-icons">
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openai.svg" alt="OpenAI" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openai-text.svg" alt="OpenAI" class="logo-medium">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/anthropic-text.svg" alt="Anthropic" class="logo-medium">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/gemini-color.svg" alt="Gemini" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/gemini-text.svg" alt="Gemini" class="logo-small">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/deepseek-color.svg" alt="DeepSeek" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/deepseek-text.svg" alt="DeepSeek" class="logo-small">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openrouter.svg" alt="OpenRouter" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openrouter-text.svg" alt="OpenRouter" class="logo-medium">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/ollama.svg" alt="Ollama" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/ollama-text.svg" alt="Ollama" class="logo-medium">
  </div>
  <div class="provider-logo">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/bedrock-color.svg" alt="Bedrock" class="logo-medium">
    <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/bedrock-text.svg" alt="Bedrock" class="logo-small">
  </div>
</div>

```
Your App
   ‚Üì
NodeLLM (Unified API + State + Security)
   ‚Üì
OpenAI | Anthropic | Bedrock | Ollama
```

---

## üèóÔ∏è The "Backend-First" AI SDK

While most AI SDKs (like Vercel AI SDK) are heavily optimized for **Frontend Streaming** (Next.js, React Server Components), NodeLLM is built for the **Backend**.

It is the "AI SDK for the rest of us"‚Äîbackend engineers building workers, cron jobs, CLI tools, Slack bots, and REST/GraphQL APIs that *aren't* Next.js.

### Strategic Principles

- **Provider Isolation**: Decouple your services from vendor SDKs.
- **Production-Ready**: Native support for streaming, retries, and unified error handling.
- **Predictable API**: Consistent behavior for Tools, Vision, and Structured Outputs across all models.

---

## ‚ö° The 5-Minute Path

```ts
import { NodeLLM } from "@node-llm/core";

// 1. Install & Configure (Uses env vars)
const chat = NodeLLM.chat("gpt-4o");

// 2. Chat (High-level request/response)
const response = await chat.ask("Explain event-driven architecture");
console.log(response.content);

// 3. Streaming (Standard AsyncIterator)
for await (const chunk of chat.stream("Explain event-driven architecture")) {
  process.stdout.write(chunk.content);
}
```

---

## üõë What NodeLLM is NOT

NodeLLM represents a clear architectural boundary between your system and LLM vendors.

NodeLLM is **NOT**:

- A wrapper around a single provider SDK (like `openai` or `@google/generative-ai`)
- A prompt-engineering framework
- An agent playground or experimental toy

---

## üöÄ Why Use This Over Official SDKs?

| Feature            | NodeLLM                       | Official SDKs               | Architectural Impact      |
| :----------------- | :---------------------------- | :-------------------------- | :------------------------ |
| **Provider Logic** | Transparently Handled         | Exposed to your code        | **Low Coupling**          |
| **Streaming**      | Standard `AsyncIterator`      | Vendor-specific Events      | **Predictable Data Flow** |
| **Tool Loops**     | Automated Recursion           | Manual implementation       | **Reduced Boilerplate**   |
| **Files/Vision**   | Intelligent Path/URL handling | Base64/Buffer management    | **Cleaner Service Layer** |
| **Configuration**  | Centralized & Global          | Per-instance initialization | **Easier Lifecycle Mgmt** |

---

## üîÆ Capabilities

### üí¨ Unified Chat

Stop rewriting code for every provider. `NodeLLM` normalizes inputs and outputs into a single, predictable mental model.

```ts
import { NodeLLM } from "@node-llm/core";

// Uses NODELLM_PROVIDER from environment (defaults to GPT-4o)
const chat = NodeLLM.chat();
await chat.ask("Hello world");
```

### üõ†Ô∏è Auto-Executing Tools

Define tools once using our clean **Class-Based DSL**; NodeLLM manages the recursive execution loop for you.

```ts
import { Tool, z } from "@node-llm/core";

class WeatherTool extends Tool {
  name = "get_weather";
  description = "Get current weather";
  schema = z.object({ loc: z.string() });

  async handler({ loc }) {
    return `Sunny in ${loc}`;
  }
}

await chat.withTool(WeatherTool).ask("Weather in Tokyo?");
```

### üíæ [Persistence Layer](/orm/prisma)

Automatically track chat history, tool executions, and API metrics with **@node-llm/orm**. Now with full support for **Extended Thinking** persistence.

```ts
import { createChat } from "@node-llm/orm/prisma";

// Chat state is automatically saved to your database (Postgres/MySQL/SQLite)
const chat = await createChat(prisma, llm, { model: "claude-3-7-sonnet" });

await chat.withThinking({ budget: 16000 }).ask("Develop a strategy");
```

### üß™ [Deterministic Testing](/core-features/testing)

Validate your AI agents with **VCR cassettes** (record/replay) and a **Fluent Mocker** for unit tests. No more flaky or expensive test runs.

```ts
import { vcr, Mocker } from "@node-llm/testing";

// 1. Integration Tests (VCR)
await vcr.useCassette("pricing_flow", async () => {
  const res = await chat.ask("How much?");
  expect(res.content).toContain("$20/mo");
});

// 2. Unit Tests (Mocker)
const mock = new Mocker()
  .chat("Next step?", "Login User")
  .tool("getCurrentUser", { id: 1 });
```

### üõ°Ô∏è [Security & Compliance](/advanced/security)

Implement custom security, PII detection, and compliance logic using pluggable asynchronous hooks (`beforeRequest` and `afterResponse`).

### üîß Strategic Configuration

NodeLLM provides a flexible configuration system designed for enterprise usage:

```ts
// Switch providers at the framework level
const llm = createLLM({ provider: "anthropic" });
```

### ‚ö° Scoped Parallelism

Run multiple providers in parallel safely without global configuration side effects using isolated contexts.

```ts
const [gpt, claude] = await Promise.all([
  NodeLLM.withProvider("openai").chat("gpt-4o").ask(prompt),
  NodeLLM.withProvider("anthropic").chat("claude-3-5-sonnet").ask(prompt)
]);
```

### üß† [Extended Thinking](/core-features/reasoning)

Direct access to the thought process of modern reasoning models like **Claude 3.7**, **DeepSeek R1**, or **OpenAI o1/o3** using a unified interface.

```ts
const res = await chat
  .withThinking({ budget: 16000 })
  .ask("Solve this logical puzzle");

console.log(res.thinking.text); // Full chain-of-thought
```

---

## üìã Supported Providers

| Provider                                                                                                                             | Supported Features                                                                                    |
| :----------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openai.svg" height="18"> **OpenAI**            | Chat, Streaming, Tools, Vision, Audio, Images, Transcription, **Reasoning**, **Smart Developer Role** |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/gemini-color.svg" height="18"> **Gemini**      | Chat, Streaming, Tools, Vision, Audio, Video, Embeddings                                              |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/anthropic-text.svg" height="12"> **Anthropic** | Chat, Streaming, Tools, Vision, PDF, Structured Output, **Extended Thinking (Claude 3.7)**            |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/deepseek-color.svg" height="18"> **DeepSeek**  | Chat (V3), **Extended Thinking (R1)**, Tools, Streaming                                              |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/bedrock-color.svg" height="18"> **Bedrock**    | Chat, Streaming, Tools, Image Gen (Titan/SD), Embeddings, **Prompt Caching**                         |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/openrouter.svg" height="18"> **OpenRouter**    | **Aggregator**, Chat, Streaming, Tools, Vision, Embeddings, **Reasoning**                             |
| <img src="https://registry.npmmirror.com/@lobehub/icons-static-svg/latest/files/icons/ollama.svg" height="18"> **Ollama**            | **Local Inference**, Chat, Streaming, Tools, Vision, Embeddings                                       |

---

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](https://github.com/node-llm/node-llm/blob/main/CONTRIBUTING.md) for more details on how to get started.

---

## ü´∂ Credits

Heavily inspired by the elegant design of [RubyLLM](https://rubyllm.com/).

---

**Upgrading to v1.6.0?** Read the [Migration Guide](/getting_started/migration-v1-6.html) to understand the new strict provider requirements and typed error hierarchy.


<!-- END FILE: intro.md -->
----------------------------------------

<!-- FILE: getting_started/configuration.md -->

# üìÑ getting_started/configuration.md

---
layout: default
title: Configuration
nav_order: 3
parent: Getting Started
permalink: /getting-started/configuration
description: Learn how to configure NodeLLM with API keys, custom base URLs, security limits, and per-request overrides.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` provides three ways to configure providers: **Zero-Config** (via environment variables), **Explicit Factory** (via `createLLM`), and **Isolated Branching** (via `.withProvider`).

---

## 1. Zero-Config (The "Direct" Pattern)

The simplest way to use NodeLLM is by relying on environment variables. NodeLLM will automatically snapshot your environment at load time.

**Environment variables (`.env`):**

```env
NODELLM_PROVIDER=openai
OPENAI_API_KEY=sk-....
```

**Code:**

```typescript
import "dotenv/config";
import { NodeLLM } from "@node-llm/core";

// Zero setup required
const chat = NodeLLM.chat();
```

---

## 2. Explicit Factory (`createLLM`)

Recommended for production applications where you want to explicitly define provider behavior or manage multiple providers in one application.

### Switching Providers

Since `NodeLLM` is immutable, you switch providers by creating a new instance using `createLLM()` or `withProvider()`.

```typescript
// Create an Anthropic instance
const llm = createLLM({
  provider: "anthropic",
  anthropicApiKey: process.env.ANTHROPIC_API_KEY
});
```

### Provider Configuration

#### API Keys

Configure API keys in the configuration object.

```typescript
const llm = createLLM({
  openaiApiKey: process.env.OPENAI_API_KEY,
  anthropicApiKey: process.env.ANTHROPIC_API_KEY,
  geminiApiKey: process.env.GEMINI_API_KEY,
  deepseekApiKey: process.env.DEEPSEEK_API_KEY,
  openrouterApiKey: process.env.OPENROUTER_API_KEY
});
```

#### Custom Base URLs

Override the default API endpoints for custom deployments (e.g., Azure OpenAI):

```typescript
const llm = createLLM({
  provider: "openai",
  openaiApiKey: process.env.AZURE_OPENAI_API_KEY,
  openaiApiBase: process.env.AZURE_OPENAI_API_BASE_ENDPOINT
});
```

#### Loop Protection & Security Limits

Prevent runaway costs, infinite loops, and hanging requests by setting execution and timeout limits:

```typescript
const llm = createLLM({
  maxToolCalls: 5, // Stop after 5 sequential tool execution turns
  maxRetries: 2, // Retry network/server errors 2 times
  requestTimeout: 30000, // Timeout requests after 30 seconds (default)
  maxTokens: 4096 // Limit output to 4K tokens (default)
});
```

**Security Benefits:**

- **`maxToolCalls`**: Prevents infinite tool execution loops
- **`maxRetries`**: Prevents retry storms that could exhaust resources
- **`requestTimeout`**: Prevents hanging requests and DoS attacks
- **`maxTokens`**: Prevents excessive output generation and cost overruns

---

## Supported Configuration Keys

| Key                         | Description                         | Default                           |
| --------------------------- | ----------------------------------- | --------------------------------- |
| `openaiApiKey`              | OpenAI API key                      | `process.env.OPENAI_API_KEY`      |
| `openaiApiBase`             | OpenAI API base URL                 | `process.env.OPENAI_API_BASE`     |
| `anthropicApiKey`           | Anthropic API key                   | `process.env.ANTHROPIC_API_KEY`   |
| `anthropicApiBase`          | Anthropic API base URL              | `process.env.ANTHROPIC_API_BASE`  |
| `geminiApiKey`              | Google Gemini API key               | `process.env.GEMINI_API_KEY`      |
| `geminiApiBase`             | Gemini API base URL                 | `process.env.GEMINI_API_BASE`     |
| `deepseekApiKey`            | DeepSeek API key                    | `process.env.DEEPSEEK_API_KEY`    |
| `deepseekApiBase`           | DeepSeek API base URL               | `process.env.DEEPSEEK_API_BASE`   |
| `openrouterApiKey`          | OpenRouter API key                  | `process.env.OPENROUTER_API_KEY`  |
| `openrouterApiBase`         | OpenRouter API base URL             | `process.env.OPENROUTER_API_BASE` |
| `defaultChatModel`          | Default model for `.chat()`         | Provider default                  |
| `defaultTranscriptionModel` | Default model for `.transcribe()`   | Provider default                  |
| `defaultModerationModel`    | Default model for `.moderate()`     | Provider default                  |
| `defaultEmbeddingModel`     | Default model for `.embed()`        | Provider default                  |
| `maxToolCalls`              | Max sequential tool execution turns | `5`                               |
| `maxRetries`                | Max retries for provider errors     | `2`                               |
| `requestTimeout`            | Request timeout in milliseconds     | `30000` (30s)                     |
| `maxTokens`                 | Max output tokens per request       | `4096`                            |
| `retry`                     | Retry configuration (legacy)        | `{ attempts: 1, delayMs: 0 }`     |

---

## Inspecting Configuration

You can inspect the current internal configuration at any time.

```typescript
console.log(NodeLLM.config.openaiApiKey);
```

---

## Error Handling

Attempting to use an unconfigured provider will raise a clear error:

```typescript
// If API key is not set
const llm = createLLM({ provider: "openai" });
// Error: openaiApiKey is not set in config...
```

### Snapshotting & Instance Initialization

When you create an LLM instance (including the default `NodeLLM` export), it **snapshots** all relevant environment variables.

In the global `NodeLLM` instance, this initialization is **lazy**. It only snapshots `process.env` the first time you access a property or method (like `.chat()`). This makes it safe to use with `dotenv/config` or similar libraries in ESM, even if they are imported after the core library.

```typescript
// ‚úÖ Safe in NodeLLM v1.6.0+: Initialized on first call
import { NodeLLM } from "@node-llm/core";
import "dotenv/config";

const chat = NodeLLM.chat(); // Snapshots environment NOW
```

---

## Best Practices

### Use Dotenv for Local Development

```typescript
import "dotenv/config";
import { createLLM } from "@node-llm/core";

const llm = createLLM({ provider: "openai" });
```

### Configure Once at Startup

```typescript
// app.ts
const llm = createLLM({
  openaiApiKey: process.env.OPENAI_API_KEY,
  anthropicApiKey: process.env.ANTHROPIC_API_KEY
});
```

### Scoped Configuration (Isolation)

`NodeLLM` is a **frozen, immutable instance**. It cannot be mutated at runtime. This design ensures that configurations do not leak between parallel requests, making it safe for multi-tenant applications.

Use `createLLM()` or `.withProvider()` to create an **isolated context**.

#### Isolated Provider State

Run multiple providers in parallel safely without any side effects:

```ts
const [gpt, claude] = await Promise.all([
  NodeLLM.withProvider("openai").chat("gpt-4o").ask(prompt),
  NodeLLM.withProvider("anthropic").chat("claude-3-5-sonnet").ask(prompt)
]);
```

#### Scoped Credentials

You can also pass a second argument to `withProvider` to override configuration keys (like API keys) for that specific instance only. This is useful for multi-tenant applications.

```ts
const userA = NodeLLM.withProvider("openai", {
  openaiApiKey: "USER_A_KEY"
});

const userB = NodeLLM.withProvider("openai", {
  openaiApiKey: "USER_B_KEY"
});

// These calls use different API keys simultaneously
const [resA, resB] = await Promise.all([
  userA.chat().ask("Hello from A"),
  userB.chat().ask("Hello from B")
]);
```

This ensures each parallel call uses the correct provider and credentials without interfering with others.


<!-- END FILE: getting_started/configuration.md -->
----------------------------------------

<!-- FILE: getting_started/getting-started.md -->

# üìÑ getting_started/getting-started.md

---
layout: default
title: Quick Start
nav_order: 2
parent: Getting Started
permalink: /getting-started/quick-start
description: A 5-minute guide to get started with NodeLLM. Install, configure, and run your first chat, image generation, and embedding scripts.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Start building AI apps in Node.js in 5 minutes. Chat, generate images, and create embeddings with one unified API.

---

## Installation

```bash
npm install @node-llm/core
# or
pnpm add @node-llm/core
```

---

## Configuration

The fastest way to start is using **Zero-Config**. NodeLLM automatically reads your API keys and the active provider from environment variables.

```ts
import "dotenv/config";
import { NodeLLM } from "@node-llm/core";

// 1. Ensure NODELLM_PROVIDER=openai and OPENAI_API_KEY=... are in .env
const llm = NodeLLM; // Exported singleton, ready to go!
```

Alternatively, use **Explicit Configuration** for multi-tenant or multi-provider apps:

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({
  provider: "openai",
  openaiApiKey: process.env.OPENAI_API_KEY
});
```

---

## Quick Start Examples

### Chat

```ts
const chat = llm.chat(); // Uses default model
const response = await chat.ask("Explain quantum computing in 5 words.");
console.log(response.content);
// => "Computing using quantum mechanical phenomena."
```

### Generate Images

```ts
const image = await llm.paint("A cyberpunk city with neon rain");
console.log(image.url);
```

### Create Embeddings

```ts
const embedding = await llm.embed("Semantic search is powerful.");
console.log(`Vector dimensions: ${embedding.dimensions}`);
```

### Streaming

Real-time responses are essential for good UX.

```ts
for await (const chunk of chat.stream("Write a poem")) {
  process.stdout.write(chunk.content);
}
```

---

## Next Steps

- [Chat Features](/core-features/chat.html): Learn about history, system prompts, and JSON mode.
- [Multimodal](/core-features/multimodal.html): Send images, audio, and documents.
- [Tool Calling](/core-features/tools.html): Give your AI ability to execute code.
- [Deterministic Testing](/core-features/testing): Setup reliable, zero-cost integration tests.
- [Migration Guide (v1.6)](/getting_started/migration-v1-6): Moving from legacy mutable versions.


<!-- END FILE: getting_started/getting-started.md -->
----------------------------------------

<!-- FILE: getting_started/index.md -->

# üìÑ getting_started/index.md

---
layout: default
title: Getting Started
nav_order: 2
has_children: true
permalink: /getting-started
description: New to NodeLLM? Start here to understand the core philosophy and get your first model running in minutes.
back_to_top: false
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }


<!-- END FILE: getting_started/index.md -->
----------------------------------------

<!-- FILE: getting_started/migration-v1-6.md -->

# üìÑ getting_started/migration-v1-6.md

---
layout: default
title: Migration Guide (v1.6)
parent: Getting Started
nav_order: 10
permalink: /getting-started/migration-guide-v1-6
description: Guide for migrating to NodeLLM v1.6.0 strict provider configuration.
---

# Migrating to NodeLLM v1.6.0
{: .no_toc }

NodeLLM v1.6.0 builds upon the **Immutable Architecture** introduced in v1.5.0 and introduces stricter configuration requirements to eliminate ambiguity when working with multiple providers.

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Strict Provider Requirement

The most significant change in v1.6.0 is the removal of "Automatic Provider Detection."

### Legacy Behavior (v1.5 and below)

NodeLLM would previously attempt to guess which provider you wanted based on the presence of API keys (e.g., defaulting to OpenAI if `OPENAI_API_KEY` was found).

### New Behavior (v1.6.0)

If you use the direct `NodeLLM` singleton, you **must now explicitly set** the `NODELLM_PROVIDER` environment variable.

```bash
# .env - REQUIRED for Zero-Config
NODELLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

If this variable is missing, `NodeLLM.chat()` will now throw a `ProviderNotConfiguredError` rather than guessing.

---

## Immutable Global Configuration (Reminder)

While introduced in v1.5.0, v1.6.0 reinforces that the global `NodeLLM` instance is **Frozen**.

### ‚ùå No-Op: `NodeLLM.configure()`

Programmatic mutation of the global singleton is no longer supported.

```javascript
// This pattern has been deprecated since v1.5 and remains a no-op in v1.6
import { NodeLLM } from "@node-llm/core";

NodeLLM.configure({ ... }); // ‚ö†Ô∏è WARNING: No effect.
```

### ‚úÖ Use Scoped Instances

For programmatic configuration, always use `createLLM()` or `.withProvider()`.

```javascript
import { createLLM } from "@node-llm/core";

const llm = createLLM({
  provider: "anthropic",
  anthropicApiKey: "sk-..."
});
```

---

## Typed Error Hierarchy

In v1.6.0, we have moved from generic `Error` strings to a robust, typed error hierarchy. This allows for better programmatic handling of LLM failures.

| Feature          | Legacy Error                            | New v1.6 Error               |
| :--------------- | :-------------------------------------- | :--------------------------- |
| Missing Feature  | `Error: ... does not support ...`       | `UnsupportedFeatureError`    |
| Missing Provider | `Error: LLM provider not configured`    | `ProviderNotConfiguredError` |
| Model Mismatch   | `Error: Model ... does not support ...` | `ModelCapabilityError`       |

---

## Design Rationale

These changes complete the architectural transition started in v1.5.0:

1. **No Ambiguity**: By requiring `NODELLM_PROVIDER`, we ensure that a single model (like `llama3`) is never accidentally routed to the wrong provider (Ollama vs OpenRouter).
2. **Stable Contracts**: The Immutable Singleton ensures that your application configuration is predictable and thread-safe from the moment of first access.
3. **Production Observability**: Typed errors make it easier to build automated monitors and fallback logic around specific provider failure modes.


<!-- END FILE: getting_started/migration-v1-6.md -->
----------------------------------------

<!-- FILE: getting_started/overview.md -->

# üìÑ getting_started/overview.md

---
layout: default
title: Overview
nav_order: 1
parent: Getting Started
permalink: /getting-started/overview
description: High-level overview of NodeLLM components, design principles, and how the framework works.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` provides a seamless, unified interface for interacting with multiple Large Language Model (LLM) providers. Whether you are building a simple chat bot or a complex multi-modal agentic workflow, `NodeLLM` abstracts away the provider-specific complexities.

---

## Core Components

Understanding these components will help you use the framework effectively.

### Chat

The primary interface for conversational AI. `NodeLLM.chat()` creates a stateful object that manages conversation history.

```ts
const chat = llm.chat("gpt-4o");
```

### Providers

Adapters that translate the unified `NodeLLM` format into provider-specific API calls (OpenAI, Anthropic, Gemini). You rarely interact with them directly; the library handles this based on the model ID you choose.

### Tools

Functions that the AI can execute. You define the schema and the handler, and `NodeLLM` manages the execution loop automatically.

### Configuration

Global settings for API keys and defaults.

```ts
const llm = createLLM({
  openaiApiKey: "sk-...",
  provider: "openai"
});
```

---

## Design Principles

### Unified Interface

Every provider works differently. `NodeLLM` normalizes inputs (messages, images) and outputs (content, usage stats) so your code doesn't change when you switch models.

### Streaming First

AI responses are slow. `NodeLLM` is built around `AsyncIterator` to make streaming text to the user as easy as a `for await` loop.

### Progressive Disclosure

Start simple with `NodeLLM.chat().ask("Hello")`. As your needs grow, you can access advanced features like raw API responses, custom headers, and token usage tracking without breaking your initial code.

---

## Configuration Patterns

NodeLLM supports two primary styles of configuration to match your preferred architectural pattern.

### 1. Fluent Builder API
Ideal for step-by-step configuration and readable "action" chains.

```ts
const chat = NodeLLM.chat("claude-3-7-sonnet")
  .withInstructions("You are a logic expert")
  .withTemperature(0.2)
  .withThinking({ budget: 16000 });

await chat.ask("Solve this puzzle");
```

### 2. Direct Configuration Object (Stateless)
Ideal for integrations that pass configuration dynamically or from a centralized settings object.

**Enhanced in v1.7.0**
{: .label .label-green }

```ts
// All options can be passed together at initialization
const chat = NodeLLM.chat("gpt-4o", {
  instructions: "You are a helpful assistant",
  temperature: 0.7,
  maxTokens: 500,
  thinking: { effort: "high" },
  headers: { "X-Tenant-ID": "123" }
});

// Or per-request for granular override
await chat.ask("Hello", {
  temperature: 0.1,
  maxToolCalls: 5
});
```

---

## How It Works

1.  **Normalization**: Your inputs (text, images, files) are converted into a standardized format.
2.  **Configuration**: The library uses the provider and model you specify (e.g., GPT-4o with OpenAI).
3.  **Execution**: The request is sent. If tools are called, the library executes them and feeds the result back to the model.
4.  **Response**: The final response is normalized into a consistent object.


<!-- END FILE: getting_started/overview.md -->
----------------------------------------

<!-- FILE: core-features/audio-transcription.md -->

# üìÑ core-features/audio-transcription.md

---
layout: default
title: Audio Transcription
parent: Core Features
nav_order: 6
description: Convert speech to text using specialized models like Whisper or leverage multimodal models for native audio understanding and analysis.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Convert audio files to text using models like OpenAI's Whisper or Google's Gemini. `NodeLLM` supports both raw transcription and multimodal chat analysis.

---

## Basic Transcription

Use `NodeLLM.transcribe()` for direct speech-to-text conversion.

```ts
const text = await NodeLLM.transcribe("meeting.mp3", {
  model: "whisper-1"
});

console.log(text.toString());
```

---

## Advanced Options

### Speed vs Accuracy

You can choose different models or parameters depending on your needs.

```ts
await NodeLLM.transcribe("audio.mp3", {
  model: "whisper-1",
  language: "en", // ISO-639-1 code hint to improve accuracy
  prompt: "ZyntriQix, API" // Guide the model with domain-specific terms
});
```

### Accessing Segments & Timestamps

The `transcribe` method returns a `Transcription` object that contains more than just text. You can access detailed timing information if supported by the provider (e.g., using `response_format: 'verbose_json'` with OpenAI).

```ts
const response = await NodeLLM.transcribe("interview.mp3", {
  params: { response_format: "verbose_json" }
});

console.log(`Duration: ${response.duration}s`);

for (const segment of response.segments) {
  console.log(`[${segment.start}s - ${segment.end}s]: ${segment.text}`);
}
```

---

## Multimodal Chat vs. Transcription

There are two ways to work with audio:

1.  **Transcription (`NodeLLM.transcribe`)**: Best when you need the verbatim text.
    - _Result_: "Hello everyone today we are..."
2.  **Multimodal Chat (`chat.ask`)**: Best when you need to **analyze** or **summarize** the audio directly, without seeing the raw text first. Supported by models like `gemini-1.5-pro` and `gpt-4o`.

```ts
// Multimodal Chat Example
const chat = NodeLLM.chat("gemini-1.5-pro");

await chat.ask("What is the main topic of this podcast?", {
  files: ["podcast.mp3"]
});
```

---

## Error Handling

Audio files can be large and prone to timeouts.

```ts
try {
  await NodeLLM.transcribe("large-file.mp3");
} catch (error) {
  console.error("Transcription failed:", error.message);
}
```


<!-- END FILE: core-features/audio-transcription.md -->
----------------------------------------

<!-- FILE: core-features/chat.md -->

# üìÑ core-features/chat.md

---
layout: default
title: Chat
parent: Core Features
nav_order: 1
permalink: /core-features/chat
description: A unified interface for stateful conversations across all providers. Learn how to manage history, instructions, and lifecycle hooks.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` provides a unified chat interface across all providers (OpenAI, Gemini, Anthropic). It normalizes the differences in their APIs, allowing you to use a single set of methods for interacting with them.

```bash
npm install @node-llm/core
```

---

## Starting a Conversation

The core entry point is `NodeLLM.chat(model_id?, options?)`.

```ts
import "dotenv/config";
import { NodeLLM } from "@node-llm/core";

// 1. Get a chat instance
// (No setup required if NODELLM_PROVIDER is in env)
const chat = NodeLLM.chat("gpt-4o-mini");

// 2. Ask a question
const response = await chat.ask("What is the capital of France?");

console.log(response.content); // "The capital of France is Paris."
```

### Continuing the Conversation

The `chat` object maintains a history of the conversation, so you can ask follow-up questions naturally.

```ts
await chat.ask("What is the capital of France?");
// => "Paris"

await chat.ask("What is the population there?");
// => "The population of Paris is approximately..."
```

---

## System Prompts (Instructions)

Guide the AI's behavior, personality, or constraints using system prompts. You can set this when creating the chat or update it later.

```ts
// Option 1: Set at initialization
const chat = llm.chat("gpt-4o", {
  systemPrompt: "You are a helpful assistant that answers in rhyming couplets."
});

// Option 2: Set or update later
chat.withInstructions("Now speak like a pirate.");

// Option 3: Standard Alias <span style="background-color: #0d47a1; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.6.0</span>
chat.system("You are a helpful assistant.");

await chat.ask("Hello");
// => "Ahoy matey! The seas are calm today."
```

---

## Manual History Management <span style="background-color: #0d47a1; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.6.0</span>

While NodeLLM handles history automatically during a session, you can manually inject messages into the conversation. This is especially useful for **Session Rehydration** from a database.

```ts
const chat = NodeLLM.chat("gpt-4o");

// Rehydrate previous turns from your DB
chat
  .add("user", "What is my name?")
  .add("assistant", "You told me your name is Alice.");

const response = await chat.ask("What did I just say?");
// => "You asked me what your name is."
```

The `.add()` method correctly isolates `system` and `developer` roles while maintaining chronological order for `user` and `assistant` messages.

---

## Custom HTTP Headers

Some providers offer beta features or require specific headers (like for observability proxies).

```ts
// Enable Anthropic's beta features
const chat = llm.chat("claude-3-5-sonnet").withRequestOptions({
  headers: {
    "anthropic-beta": "max-tokens-3-5-sonnet-2024-07-15"
  }
});

await chat.ask("Tell me about the weather");
```

---

## Raw Content Blocks (Advanced)

For advanced use cases like **Anthropic Prompt Caching**, you can pass provider-specific content blocks directly. `NodeLLM` attempts to pass array content through to the provider.

```ts
// Example: Anthropic Prompt Caching
const systemBlock = {
  type: "text",
  text: "You are a coding assistant. (Cached context...)",
  cache_control: { type: "ephemeral" }
};

const chat = llm.chat("claude-3-5-sonnet", {
  systemPrompt: systemBlock as any // Cast if strict types complain
});
```

---

## Working with Multiple Providers

### Isolation and Multi-Tenancy

`NodeLLM` is a **frozen, immutable instance**. It cannot be mutated at runtime. This design ensures that configurations (like API keys) do not leak between different parts of your application, making it safe for multi-tenant environments like SaaS or serverless functions.

If you need isolated configurations for different users or requests, use `createLLM()` or `NodeLLM.withProvider()`.

```ts
import { createLLM } from "@node-llm/core";

// Safe for multi-tenant apps
const userA = createLLM({ provider: "openai", openaiApiKey: "..." });
const userB = createLLM({ provider: "anthropic", anthropicApiKey: "..." });

await userA.chat().ask("Hello!"); // Uses User A's key
await userB.chat().ask("Hello!"); // Uses User B's key
```

### ‚ö° Scoped Instances

Use `withProvider()` to create isolated instances with their own configuration. Each instance maintains separate state without affecting others.

```ts
// ‚úÖ SAFE: Each instance is isolated
const tenant1 = NodeLLM.withProvider("openai", {
  openaiApiKey: tenant1Key,
  requestTimeout: 30000
});

const tenant2 = NodeLLM.withProvider("openai", {
  openaiApiKey: tenant2Key,
  requestTimeout: 60000
});

// No interference - each has its own config
await Promise.all([tenant1.chat("gpt-4o").ask(prompt), tenant2.chat("gpt-4o").ask(prompt)]);
```

**Multi-provider parallelism:**

```ts
const [gpt, claude, gemini] = await Promise.all([
  NodeLLM.withProvider("openai").chat("gpt-4o").ask(prompt),
  NodeLLM.withProvider("anthropic").chat("claude-3-5-sonnet").ask(prompt),
  NodeLLM.withProvider("gemini").chat("gemini-2.0-flash").ask(prompt)
]);
```

**Per-request isolation in Express/Fastify:**

```ts
app.post("/chat", async (req, res) => {
  const userApiKey = req.user.openaiApiKey; // From database

  // Create isolated instance per request
  const llm = NodeLLM.withProvider("openai", {
    openaiApiKey: userApiKey
  });

  const response = await llm.chat("gpt-4o").ask(req.body.message);
  res.json(response);
});
```

---

## Temperature & Creativity

Adjust the randomness of the model's responses using `.withTemperature(0.0 - 1.0)`.

```ts
// Deterministic / Factual (Low Temperature)
const factual = NodeLLM.chat("gpt-4o").withTemperature(0.0);

// Creative / Random (High Temperature)
const creative = NodeLLM.chat("gpt-4o").withTemperature(0.9);
```

---

## Lifecycle Events <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">Enhanced in v1.5.0</span>

Hook into the chat lifecycle for logging, UI updates, audit trails, or debugging.

```ts
chat
  .onNewMessage(() => console.log("AI started typing..."))
  .onToolCallStart((call) => console.log(`Starting tool: ${call.function.name}`))
  .onToolCallEnd((call, res) => console.log(`Tool ${call.id} finished with: ${res}`))
  .onToolCallError((call, err) =>
    console.error(`Tool ${call.function.name} failed: ${err.message}`)
  )
  .onEndMessage((response) => {
    console.log(`Finished. Total tokens: ${response.total_tokens}`);
  });

await chat.ask("What's the weather?");
```

---

## üõ°Ô∏è Content Policy Hooks

NodeLLM allows you to plug in custom security and compliance logic through asynchronous hooks. This is useful for PII detection, redaction, and enterprise moderation policies.

- **`beforeRequest(handler)`**: Analyze or modify the message history before it is sent to the provider.
- **`afterResponse(handler)`**: Analyze or modify the AI's response before it is returned to your application.

```ts
chat
  .beforeRequest(async (messages) => {
    // Redact SSNs from user input
    return messages.map((m) => ({
      ...m,
      content: m.content.replace(/\d{3}-\d{2}-\d{4}/g, "[REDACTED]")
    }));
  })
  .afterResponse(async (response) => {
    // Block responses containing prohibited words
    if (response.content.includes("Prohibited")) {
      throw new Error("Compliance Violation");
    }
  });
```

---

## Retry Logic & Safety üõ°Ô∏è

By default, `NodeLLM` handles network instabilities or temporary provider errors (like 500s or 429 Rate Limits) by retrying the request.

- **Default Retries**: 2 retries (3 total attempts).
- **Request Timeout**: 30 seconds (prevents hanging requests).
- **Loop Guard**: Tool calling is limited to 5 turns to prevent infinite loops.

You can configure these limits globally:

```ts
const llm = createLLM({
  maxRetries: 3, // Increase retries for unstable connections
  maxToolCalls: 10, // Allow deeper tool calling sequences
  requestTimeout: 60000 // 60 second timeout for long-running requests
});
```

Or override per-request:

```ts
// Long-running task with extended timeout
await chat.ask("Analyze this large dataset", {
  requestTimeout: 120000 // 2 minutes
});
```

### Request Cancellation <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.3+</span>

You can cancel long-running requests using the standard `AbortController` API. This is useful for interactive UIs where users might navigate away or click "Stop".

```ts
const controller = new AbortController();

// Cancel after 5 seconds
setTimeout(() => controller.abort(), 5000);

try {
  const response = await chat.ask("Write a very long essay...", {
    signal: controller.signal
  });
} catch (error) {
  if (error.name === "AbortError") {
    console.log("Request was cancelled");
  }
}
```

The signal is propagated through all tool-calling turns, so even multi-step agentic workflows can be cancelled cleanly.

See the [Configuration Guide](/getting-started/configuration) for more details.

---

## üß± Smart Context Isolation

NodeLLM provides **Zero-Config Context Isolation** to ensure maximum instruction following and security.

Inspired by modern LLM architectures (like OpenAI's Developer Role), NodeLLM internally separates your system instructions from the conversation history. This prevents "instruction drift" as the conversation grows and provides a strong layer of protection against prompt injection.

### How It Works

- **Implicit Untangling**: If you pass a mixed array of messages to the Chat constructor, NodeLLM automatically identifies and isolates system-level instructions.
- **Dynamic Role Mapping**: On the official OpenAI API, instructions for modern models (`gpt-4o`, `o1`, `o3`) are automatically promoted to the high-privilege `developer` role.
- **Safe Fallbacks**: For older models or local providers (like Ollama or DeepSeek), NodeLLM safely maps instructions back to the standard `system` role to ensure perfect compatibility.

This behavior is **enabled by default** for all chats.


<!-- END FILE: core-features/chat.md -->
----------------------------------------

<!-- FILE: core-features/embeddings.md -->

# üìÑ core-features/embeddings.md

---
layout: default
title: Embeddings
parent: Core Features
nav_order: 3
description: Generate high-dimensional vector representations for semantic search, RAG, and clustering with single and batch embedding operations.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Embeddings are vector representations of text used for semantic search, clustering, and similarity comparisons. \`NodeLLM\` provides a unified interface for generating embeddings across different providers.

## Basic Usage

### Single Text

```ts
import { createLLM } from "@node-llm/core";

const embedding = await NodeLLM.embed("Ruby is a programmer's best friend");

console.log(embedding.vector); // Float32Array[] (e.g., 1536 dimensions)
console.log(embedding.dimensions); // 1536
console.log(embedding.model); // "text-embedding-3-small" (default)
console.log(embedding.usage.total_tokens); // Token count
```

### Batch Embeddings

Always batch multiple texts in a single call when possible. This is much more efficient than calling `embed` in a loop.

```ts
const embeddings = await NodeLLM.embed(["First text", "Second text", "Third text"]);

console.log(embeddings.vectors.length); // 3
console.log(embeddings.vectors[0]); // Vector for "First text"
```

## Configuring Models

By default, `NodeLLM` uses `text-embedding-3-small`. You can change this globally or per request.

### Global Configuration

```ts
const llm = createLLM({
  defaultEmbeddingModel: "text-embedding-3-large"
});
```

### Per-Request

```ts
const embedding = await NodeLLM.embed("Text", {
  model: "text-embedding-004" // Google Gemini model
});
```

### Custom Models

For models not in the registry (e.g., Azure deployments or new releases), use `assumeModelExists`.

```ts
const embedding = await NodeLLM.embed("Text", {
  model: "new-embedding-v2",
  provider: "openai",
  assumeModelExists: true
});
```

## Reducing Dimensions

Some models (like `text-embedding-3-large`) allow you to reduce the output dimensions to save on storage and compute, with minimal loss in accuracy.

```ts
const embedding = await NodeLLM.embed("Text", {
  model: "text-embedding-3-large",
  dimensions: 256
});

console.log(embedding.vector.length); // 256
```

## Best Practices

1.  **Batching**: Use `NodeLLM.embed(["text1", "text2"])` instead of serial calls.
2.  **Caching**: Embeddings are deterministic for a given model and text. Cache them in your database to save costs.
3.  **Cosine Similarity**: To compare two vectors, calculate the cosine similarity. `NodeLLM` does not include math utilities to keep the core light, but you can implement it easily:

    ```ts
    function cosineSimilarity(A: number[], B: number[]) {
      const dotProduct = A.reduce((sum, a, i) => sum + a * B[i], 0);
      const magnitudeA = Math.sqrt(A.reduce((sum, a) => sum + a * a, 0));
      const magnitudeB = Math.sqrt(B.reduce((sum, b) => sum + b * b, 0));
      return dotProduct / (magnitudeA * magnitudeB);
    }
    ```

## Error Handling

Wrap calls in try/catch blocks to handle API outages or rate limits.

```ts
try {
  await NodeLLM.embed("Text");
} catch (error) {
  console.error("Embedding failed:", error.message);
}
```


<!-- END FILE: core-features/embeddings.md -->
----------------------------------------

<!-- FILE: core-features/image-generation.md -->

# üìÑ core-features/image-generation.md

---
layout: default
title: Image Generation
nav_order: 7
parent: Core Features
description: Create photorealistic images and digital art from text descriptions using DALL-E, Imagen, and other integrated image models.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Generate images from text descriptions using models like DALL-E, Imagen, and others.

## Basic Usage

The simplest way is using `NodeLLM.paint(prompt)`.

```ts
// Uses default model (e.g. dall-e-3)
const image = await NodeLLM.paint("A red panda coding");

console.log(`Image URL: ${image}`); // Acts as a string URL
```

## Choosing Models & Sizes

Customize the model and dimensions.

```ts
const image = await NodeLLM.paint("A red panda coding", {
  model: "dall-e-3",
  size: "1024x1792", // Portrait
  quality: "hd" // DALL-E 3 specific
});
```

Supported sizes vary by model. Check your provider's documentation.

## Working with the Image Object

The return value is a `GeneratedImage` object which behaves like a URL string but contains rich metadata and helper methods.

```ts
const image = await NodeLLM.paint("A landscape");

// Metadata
console.log(image.url); // "https://..."
console.log(image.revisedPrompt); // "A photorealistic landscape..." (DALL-E 3)
console.log(image.mimeType); // "image/png"

// Check if it's base64 (some providers return data, not URLs)
if (image.isBase64) {
  console.log("Image data received directly.");
}
```

## Saving & Processing

You can easily save the image or get its raw buffer for further processing (e.g., uploading to S3).

```ts
// Save to disk
await image.save("./output.png");

// Get raw buffer (works for both URL and Base64 source)
const buffer = await image.toBuffer();
console.log(`Size: ${buffer.length} bytes`);

// Stream it (e.g. to HTTP response)
const stream = await image.toStream();
stream.pipe(process.stdout);
```


<!-- END FILE: core-features/image-generation.md -->
----------------------------------------

<!-- FILE: core-features/index.md -->

# üìÑ core-features/index.md

---
layout: default
title: Core Features
nav_order: 3
has_children: true
nav_fold: false
permalink: /core-features
description: Deep dive into the primary capabilities of NodeLLM, including chat, tools, vision, and reasoning.
back_to_top: false
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }


<!-- END FILE: core-features/index.md -->
----------------------------------------

<!-- FILE: core-features/models.md -->

# üìÑ core-features/models.md

---
layout: default
title: Models & Registry
parent: Core Features
nav_order: 6
description: Programmatically discover available models, their capabilities, and real-time costs using our built-in registry powered by models.dev.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

\`NodeLLM\` includes a comprehensive, built-in registry of models using data from **models.dev**. This allows you to discover models and their capabilities programmatically.

---

## Inspecting a Model

You can look up any supported model to check its context window, costs, and features.

```ts
import { createLLM } from "@node-llm/core";

const model = NodeLLM.models.find("gpt-4o");

if (model) {
  console.log(`Provider: ${model.provider}`);
  console.log(`Context Window: ${model.context_window} tokens`);
  console.log(`Input Price: $${model.pricing.text_tokens.standard.input_per_million}/1M`);
  console.log(`Output Price: $${model.pricing.text_tokens.standard.output_per_million}/1M`);
}
```

---

## Discovery by Capability

You can filter the registry to find models that match your requirements.

### Finding Vision Models

```ts
const visionModels = NodeLLM.models.list().filter((m) => m.capabilities.includes("vision"));

console.log(`Found ${visionModels.length} vision-capable models.`);
visionModels.forEach((m) => console.log(m.id));
```

### Finding Tool-Use Models

```ts
const toolModels = NodeLLM.models.list().filter((m) => m.capabilities.includes("tools"));
```

### Finding Audio Models

```ts
const audioModels = NodeLLM.models.list().filter((m) => m.capabilities.includes("audio_input"));
```

---

## Supported Providers

The registry includes models from:

- **OpenAI** (GPT-4o, GPT-3.5, DALL-E)
- **Anthropic** (Claude 3.5 Sonnet, Haiku, Opus)
- **Google Gemini** (Gemini 1.5 Pro, Flash)
- **Vertex AI** (via Gemini)

---

## Custom Models & Endpoints

Sometimes you need to use models not in the registry, such as **Azure OpenAI** deployments, **Local Models** (Ollama/LM Studio), or brand new releases.

### Using `assumeModelExists`

This flag tells \`NodeLLM\` to bypass the registry check.

**Important**: You MUST specify the `provider` when using this flag, as the system cannot infer it from the ID.

```ts
const chat = NodeLLM.withProvider("openai").chat("my-custom-deployment", {
  assumeModelExists: true
});

// Note: Capability checks are bypassed (assumed true) for custom models.
await chat.ask("Hello");
```

### Custom Endpoints (e.g. Azure/Local)

To point to a custom URL (like an Azure endpoint or local proxy), configure the base URL globally.

```ts
const llm = createLLM({
  openaiApiBase: "https://my-azure-resource.openai.azure.com",
  openaiApiKey: process.env.AZURE_API_KEY
});

// Now valid for all OpenAI requests
const chat = llm.chat("gpt-4", { provider: "openai" });
```


<!-- END FILE: core-features/models.md -->
----------------------------------------

<!-- FILE: core-features/moderation.md -->

# üìÑ core-features/moderation.md

---
layout: default
title: Moderation
parent: Core Features
nav_order: 4
description: Protect your users and your brand by checking text content against safety policies for violence, hate speech, and harassment.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Check if text content violates safety policies using \`NodeLLM.moderate\`. This is crucial for user-facing applications to prevent abuse.

## Basic Usage

The simplest check returns a flagged boolean and categories.

```ts
const result = await NodeLLM.moderate("I want to help everyone!");

if (result.flagged) {
  console.log(`‚ùå Flagged for: ${result.flaggedCategories.join(", ")}`);
} else {
  console.log("‚úÖ Content appears safe");
}
```

## Understanding Results

The moderation result object provides detailed signals:

- `flagged`: (boolean) Overall safety check. if true, content violates provider policies.
- `categories`: (object) Boolean flags for specific buckets (e.g., `sexual: false`, `violence: true`).
- `category_scores`: (object) Confidence scores (0.0 - 1.0) for each category.

```ts
const result = await NodeLLM.moderate("Some controversial text");

// Check specific categories
if (result.categories.hate) {
  console.log("Hate speech detected");
}

// Check confidence levels
console.log(`Violence Score: ${result.category_scores.violence}`);
```

### Common Categories

- **Sexual**: Sexual content.
- **Hate**: Content promoting hate based on identity.
- **Harassment**: Threatening or bullying content.
- **Self-Harm**: Promoting self-harm or suicide.
- **Violence**: Promoting or depicting violence.

## Integration Patterns

### Pre-Chat Moderation

We recommend validating user input _before_ sending it to a Chat model to save costs and prevent jailbreaks.

```ts
async function safeChat(input: string) {
  const mod = await NodeLLM.moderate(input);

  if (mod.flagged) {
    throw new Error(`Content Unsafe: ${mod.flaggedCategories.join(", ")}`);
  }

  // Only proceed if safe
  return await chat.ask(input);
}
```

### Custom Risk Thresholds

Providers have their own thresholds for "flagging". You can implement stricter (or looser) logic using raw scores.

```ts
const result = await NodeLLM.moderate(userInput);

// Custom strict policy: Flag anything with > 0.1 confidence
const isRisky = Object.entries(result.category_scores).some(([category, score]) => score > 0.1);

if (isRisky) {
  console.warn("Potential risk detected (custom strict mode)");
}
```


<!-- END FILE: core-features/moderation.md -->
----------------------------------------

<!-- FILE: core-features/multimodal.md -->

# üìÑ core-features/multimodal.md

---
layout: default
title: Multi-modal
parent: Core Features
nav_order: 2
description: Go beyond text. Learn how to pass images, audio, video, and documents to modern models using NodeLLM‚Äôs unified file handling system.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Modern LLMs can understand more than just text. `NodeLLM` provides a unified way to pass images, audio, video, and documents to models that support them.

---

## Smart File Handling

You can pass local paths or URLs directly to the `ask` or `stream` method using the `files` (or `images`) option. `NodeLLM` automatically detects the file type and formats it correctly for the specific provider.

**Supported File Types:**

- **Images**: `.jpg`, `.jpeg`, `.png`, `.gif`, `.webp`
- **Videos**: `.mp4`, `.mpeg`, `.mov`, `.avi`, `.webm`
- **Audio**: `.wav`, `.mp3`, `.ogg`, `.flac`
- **Documents**: `.pdf`, `.csv`, `.json`, `.xml`, `.md`, `.txt`
- **Code**: `.js`, `.ts`, `.py`, `.rb`, `.go`, etc.

---

## Working with Images (Vision)

Vision-capable models (like `gpt-4o`, `claude-3-5-sonnet`, `gemini-1.5-pro`) can analyze images.

```ts
const chat = NodeLLM.chat("gpt-4o");

// Analyze a local image
await chat.ask("What's in this image?", {
  files: ["./screenshot.png"]
});

// Analyze an image from a URL
await chat.ask("Describe this logo", {
  files: ["https://example.com/logo.png"]
});

// Compare multiple images
await chat.ask("Compare the design of these two apps", {
  files: ["./v1-screenshot.png", "./v2-screenshot.png"]
});
```

---

## Working with Audio

Audio-capable models (like `gemini-1.5-flash`) can listen to audio files and answer questions about them.

```ts
const chat = NodeLLM.chat("gemini-1.5-flash");

// Summarize a meeting recording
await chat.ask("Summarize the key decisions in this meeting", {
  files: ["./meeting.mp3"]
});

// Transcribe and analyze
await chat.ask("What was the tone of the speaker?", {
  files: ["./voicemail.wav"]
});
```

_Note: For pure transcription without chat, see [Audio Transcription](/core-features/audio-transcription.html)._

---

## Working with Videos

Video analysis is currently supported primarily by Google Gemini and limited OpenAI models. `NodeLLM` handles the upload and reference process seamlessly.

```ts
const chat = NodeLLM.chat("gemini-1.5-pro");

await chat.ask("What happens in this video?", {
  files: ["./demo_video.mp4"]
});
```

---

## Working with Documents (PDFs & Text)

You can provide full documents for analysis.

### Text & Code Files

For text-based files, `NodeLLM` reads the content and passes it as text context to the model.

```ts
const chat = NodeLLM.chat("claude-3-5-sonnet");

// Analyze code
await chat.ask("Explain potential bugs in this code", {
  files: ["./app/auth.ts"]
});
```

### PDFs

For PDFs, providers handled differently:

- **Anthropic**: Supports native PDF blocks (up to 10MB). `NodeLLM` handles the base64 encoding.
- **Gemini**: Supports PDF via File API.
- **OpenAI**: Often requires text extraction first (unless using Assistants API).

```ts
await chat.ask("Summarize this contract", {
  files: ["./contract.pdf"]
});
```

---

## Automatic Type Detection

You don't need to specify the file type; `NodeLLM` infers it from the extension.

```ts
// Mix and match types
await chat.ask("Analyze these project resources", {
  files: [
    "diagram.png", // Image
    "spec.pdf", // Document
    "meeting.mp3", // Audio
    "backend.ts" // Code
  ]
});
```


<!-- END FILE: core-features/multimodal.md -->
----------------------------------------

<!-- FILE: core-features/reasoning.md -->

# üìÑ core-features/reasoning.md

---
layout: default
title: Reasoning
parent: Core Features
nav_order: 10
description: Access the inner thoughts and chain-of-thought process of advanced reasoning models like DeepSeek R1 and OpenAI o1/o3.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

**Added in v1.7.0**
{: .label .label-green }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` provides a unified way to access the "thinking" or "reasoning" process of models like **DeepSeek R1**, **OpenAI o1/o3**, and **Claude 3.7/4**. Many models now expose their internal chain of thought or allow configuring the amount of effort spent on reasoning.

---

## Configuring Thinking

You can control the reasoning behavior using the `.withThinking()` or `.withEffort()` methods. This is particularly useful for models like `o3-mini` or `claude-3-7-sonnet`.

### Setting Effort Level
Effort levels (low, medium, high) allow you to balance between speed/cost and reasoning depth.

```ts
import { NodeLLM } from "@node-llm/core";

const chat = NodeLLM.chat("o3-mini")
  .withEffort("high"); // Options: "low", "medium", "high"

const response = await chat.ask("Solve this complex architecture problem...");
```

### Per-Request Configuration
If you prefer to be stateless or set configuration only for a specific request, you can pass the thinking configuration directly to `ask()` or `stream()`.

```ts
const response = await chat.ask("Solve this puzzle", {
  thinking: { budget: 16000 }
});
```

---

## Accessing Thinking Results

The results of thinking are available via the `.thinking` property on the response object. This unified object contains the text, tokens used, and any cryptographic signatures provided by the model.

```ts
const response = await chat.ask("Prove that the square root of 2 is irrational.");

// High-level access via response.thinking
if (response.thinking) {
  console.log("Thought Process:", response.thinking.text);
  console.log("Tokens Spent:", response.thinking.tokens);
  console.log("Verification Signature:", response.thinking.signature);
}

// Show the final answer
console.log("Answer:", response.content);
```

### Streaming Thinking

When using `.stream()`, thinking content is emitted in chunks. You can capture it by checking `chunk.thinking`.

```ts
const chat = NodeLLM.chat("deepseek-reasoner");

for await (const chunk of chat.stream("Explain quantum entanglement")) {
  if (chunk.thinking?.text) {
    process.stdout.write(`[Thinking] ${chunk.thinking.text}`);
  }
  if (chunk.content) {
    process.stdout.write(chunk.content);
  }
}
```

---

## Backward Compatibility (Deprecated)

Previously, reasoning text was accessed via the `response.reasoning` property. While still supported for backward compatibility, it is recommended to transition to the structured `response.thinking.text` API.

---

## Supported Capabilities

Currently, the following models have enhanced reasoning support in `NodeLLM`:

| Model ID                           | Provider  | Support Level                                     |
| :--------------------------------- | :-------- | :------------------------------------------------ |
| `deepseek-reasoner`                | DeepSeek  | Full text extraction                              |
| `o1-*`, `o3-*`                     | OpenAI    | Effort configuration & token tracking             |
| `claude-3-7-*`, `claude-*-4-*`     | Anthropic | Budget-based thinking & full text extraction      |
| `gemini-2.0-flash-thinking-*`      | Gemini    | Full thinking text extraction                     |


<!-- END FILE: core-features/reasoning.md -->
----------------------------------------

<!-- FILE: core-features/streaming.md -->

# üìÑ core-features/streaming.md

---
layout: default
title: Stream Responses
nav_order: 2
parent: Core Features
description: Implement real-time user experiences with low-latency responses using standard AsyncIterators and seamless tool execution loops.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

For real-time interactions, `NodeLLM` supports streaming responses via standard JavaScript `AsyncIterator`s. This allows you to display text to the user as it's being generated, reducing perceived latency.

---

## Basic Streaming

Use the `stream()` method on a chat instance to get an iterator.

```ts
const chat = NodeLLM.chat("gpt-4o");

process.stdout.write("Assistant: ");

for await (const chunk of chat.stream("Write a haiku about code.")) {
  // Most chunks contain content
  if (chunk.content) {
    process.stdout.write(chunk.content);
  }
}
// => Code flows like water
//    Logic builds a new world now
//    Bugs swim in the stream
```

---

## Understanding Chunks

Each chunk passed to your loop contains partial information about the response.

- `content`: The text fragment for this specific chunk. Can be empty contextually.
- `role`: Usually "assistant".
- `model`: The model ID.
- `usage`: (Optional) Token usage stats. Usually only present in the final chunk (provider dependent).

```ts
for await (const chunk of chat.stream("Hello")) {
  console.log(chunk);
  // { content: "He", role: "assistant", ... }
  // { content: "llo", role: "assistant", ... }
}
```

---

## Streaming with Tools <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">New ‚ú®</span>

Tools now work seamlessly with streaming! When a model decides to call a tool during streaming, `NodeLLM` automatically:

1. **Executes the tool** with the provided arguments
2. **Adds the result** to the conversation history
3. **Continues streaming** the model's final response

This all happens transparently‚Äîyou just iterate over chunks as usual!

```ts
class WeatherTool extends Tool {
  name = "get_weather";
  description = "Get current weather";
  schema = z.object({
    location: z.string().describe("The city e.g. Paris")
  });

  async execute({ location }) {
    return { location, temp: 22, condition: "sunny" };
  }
}

const chat = NodeLLM.chat("gpt-4o").withTool(WeatherTool);

// Tool is automatically executed during streaming!
for await (const chunk of chat.stream("What's the weather in Paris?")) {
  process.stdout.write(chunk.content || "");
}
// Output: "The weather in Paris is currently 22¬∞C and sunny."
```

### Tool Events in Streaming

You can also listen to tool execution events:

```ts
const chat = NodeLLM.chat("gpt-4o")
  .withTool(WeatherTool)
  .onToolCall((call) => {
    console.log(`\n[Tool Called: ${call.function.name}]`);
  })
  .onToolResult((result) => {
    console.log(`[Tool Result: ${JSON.stringify(result)}]\n`);
  });

for await (const chunk of chat.stream("Weather in Tokyo?")) {
  process.stdout.write(chunk.content || "");
}
```

**Supported Providers:** OpenAI, Anthropic, Gemini, DeepSeek

---

## Multimodal & Structured Streaming <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">New ‚ú®</span>

`chat.stream()` now supports the same advanced features as `chat.ask()`.

### Multimodal Streaming

Pass images, audio, or documents just like you would with a standard request.

```ts
const chat = NodeLLM.chat("gpt-4o");

for await (const chunk of chat.stream("What's in this image?", {
  images: ["./analysis.png"]
})) {
  process.stdout.write(chunk.content || "");
}
```

### Structured Streaming (Validated JSON)

Get streaming JSON that is automatically validated against a Zod schema.

```ts
const personSchema = z.object({
  name: z.string(),
  hobbies: z.array(z.string())
});

for await (const chunk of chat.withSchema(personSchema).stream("Generate a person profile")) {
  // Chunks will contain partial content that cumulatively forms valid JSON
  // Once the stream completes, history will contain the validated object
  process.stdout.write(chunk.content || "");
}
```

---

## Error Handling

Stream interruptions (network failure, rate limits) will throw an error within the `for await` loop. Always wrap in a `try/catch` block.

```ts
try {
  for await (const chunk of chat.stream("Generate a long story...")) {
    process.stdout.write(chunk.content);
  }
} catch (error) {
  console.error("\n[Stream Error]", error.message);
}
```

---

## Web Application Integration

Streaming is essential for modern web apps. Here is a simple example using **Express**:

```ts
import express from "express";
import { NodeLLM } from "@node-llm/core";

const app = express();

app.get("/chat", async (req, res) => {
  // Set headers for streaming text
  res.setHeader("Content-Type", "text/plain; charset=utf-8");
  res.setHeader("Transfer-Encoding", "chunked");

  const chat = NodeLLM.chat("gpt-4o-mini");

  try {
    for await (const chunk of chat.stream(req.query.q as string)) {
      if (chunk.content) {
        res.write(chunk.content);
      }
    }
    res.end();
  } catch (error) {
    res.write(`\nError: ${error.message}`);
    res.end();
  }
});
```


<!-- END FILE: core-features/streaming.md -->
----------------------------------------

<!-- FILE: core-features/structured_output.md -->

# üìÑ core-features/structured_output.md

---
layout: default
title: Structured Output
parent: Core Features
nav_order: 3
description: Force models to return strictly validated JSON data using Zod schemas or manual JSON definitions across all supported providers.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Ensure the AI returns data exactly matching a specific structure. `NodeLLM` supports strict schema validation using **Zod** (recommended) or manual JSON schemas.

This feature abstracts the provider-specific implementations (like OpenAI's `json_schema`, Gemini's `responseSchema`, or Anthropic's tool-use workarounds) into a single, unified API.

{: .highlight }

> **See it in action:** The [Brand Perception Checker](https://github.com/node-llm/node-llm/tree/main/examples/applications/brand-perception-checker) demonstrates utilizing rigorous Zod schemas to extract consistent semantic profiles across multiple providers simultaneously.

---

## Using Zod (Recommended)

The easiest way to define schemas is with Zod.

```ts
import { NodeLLM, z } from "@node-llm/core";

// Define a schema using Zod
const personSchema = z.object({
  name: z.string().describe("Person's full name"),
  age: z.number().describe("Person's age in years"),
  hobbies: z.array(z.string()).describe("List of hobbies")
});

const chat = NodeLLM.chat("gpt-4o-mini");

// Use .withSchema() to enforce the structure
const response = await chat
  .withSchema(personSchema)
  .ask("Generate a person named Alice who likes hiking and coding");

// Streaming is also supported!
// for await (const chunk of chat.withSchema(personSchema).stream("...")) { ... }

// The response is strictly validated and parsed
const person = response.parsed;

console.log(person.name); // "Alice"
console.log(person.age); // e.g. 25
console.log(person.hobbies); // ["hiking", "coding"]
```

---

## Manual JSON Schemas

You can also provide a raw JSON schema object if you prefer not to use Zod.

**Note for OpenAI:** You must strictly follow OpenAI's requirements, such as setting `additionalProperties: false`.

```ts
const schema = {
  type: "object",
  properties: {
    name: { type: "string" },
    age: { type: "integer" }
  },
  required: ["name", "age"],
  additionalProperties: false // Required for strict mode in OpenAI
};

const response = await chat.withSchema(schema).ask("Generate a person");

console.log(response.parsed); // { name: "...", age: ... }
```

---

## JSON Mode

If you just need valid JSON but don't want to enforce a rigid schema, you can enable JSON mode. This instructs the model to return valid JSON but gives it more freedom with the structure.

```ts
chat.withRequestOptions({
  responseFormat: { type: "json_object" }
});

const response = await chat.ask("Generate a JSON object with a greeting");
console.log(response.parsed); // { greeting: "..." } or whatever keys it chose
```

---

## Provider Support

| Provider      | Method Used                                | Notes                                                                                                           |
| :------------ | :----------------------------------------- | :-------------------------------------------------------------------------------------------------------------- |
| **OpenAI**    | `response_format: { type: "json_schema" }` | Fully supported with strict adherence.                                                                          |
| **Gemini**    | `responseSchema`                           | Supported natively.                                                                                             |
| **Anthropic** | Tool Use (Mock)                            | `NodeLLM` automatically creates a tool definition and forces the model to use it to simulate structured output. |

---

## Nested Schemas

Complex nested schemas are fully supported via Zod.

```ts
const companySchema = z.object({
  name: z.string(),
  employees: z.array(
    z.object({
      name: z.string(),
      role: z.enum(["developer", "designer", "manager"]),
      skills: z.array(z.string())
    })
  ),
  metadata: z.object({
    founded: z.number(),
    industry: z.string()
  })
});

const response = await chat.withSchema(companySchema).ask("Generate a small tech startup");
```


<!-- END FILE: core-features/structured_output.md -->
----------------------------------------

<!-- FILE: core-features/testing.md -->

# üìÑ core-features/testing.md

---
layout: default
title: Testing
parent: Core Features
nav_order: 10
permalink: /core-features/testing
description: Deterministic testing infrastructure for NodeLLM applications. VCR integration and fluent mocking for reliable AI systems.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Overview

Deterministic testing infrastructure for NodeLLM-powered AI systems. Built for engineers who prioritize **Boring Solutions**, **Security**, and **High-Fidelity Feedback Loops**.

> üí° **What is High-Fidelity?**
> Your tests exercise the same execution path, provider behavior, and tool orchestration as production ‚Äî without live network calls.

**Framework Support**: ‚úÖ Vitest (native) | ‚úÖ Jest (compatible via core APIs) | ‚úÖ Any test framework

---

## The Philosophy: Two-Tier Testing

We believe AI testing should never be flaky or expensive. We provide two distinct strategies:

### 1. VCR (Integration Testing) üìº

**When to use**: To verify your system works with real LLM responses without paying for every test run.

- **High Fidelity**: Captures the **NodeLLM-normalized LLM execution** (model, prompt, tools, retries, and final output), ensuring replay remains stable even if provider APIs change.
- **Security First**: Automatically scrubs API Keys and sensitive PII from "cassettes".
- **CI Safe**: Fails-fast in CI if a cassette is missing, preventing accidental live API calls.
 
 > üö® **CI Safety Guarantee**
 > When `CI=true`, VCR **will never** record new cassettes.
 > If a matching cassette is missing or mismatched, the test fails immediately.

### 2. Mocker (Unit Testing) üé≠
 
 > ‚ö†Ô∏è **Note**
 > The Mocker does **not** attempt to simulate model intelligence or reasoning.
 > It deterministically simulates provider responses to validate application logic, error handling, and control flow.

**When to use**: To test application logic, edge cases (errors, rate limits), and rare tool-calling paths.

- **Declarative**: Fluent, explicit API to define expected prompts and responses.
- **Multimodal**: Native support for `chat`, `embed`, `paint`, `transcribe`, and `moderate`.
- **Streaming**: Simulate token-by-token delivery to test real-time UI logic.

---

## üìº VCR Usage

### Basic Interaction

Wrap your tests in `withVCR` to automatically record interactions the first time they run.

```typescript
import { withVCR } from "@node-llm/testing";

it(
  "calculates sentiment correctly",
  withVCR(async () => {
    const result = await mySentimentAgent.run("I love NodeLLM!");
    expect(result.sentiment).toBe("positive");
  })
);
```

### Hierarchical Organization (Convention-Based Mode) üìÇ

Organize your cassettes into nested subfolders to match your test suite structure.

```typescript
import { describeVCR, withVCR } from "@node-llm/testing";

describeVCR("Authentication", () => {
  describeVCR("Login", () => {
    it(
      "logs in successfully",
      withVCR(async () => {
        // Cassette saved to: test/cassettes/authentication/login/logs-in-successfully.json
      })
    );
  });
});
```

### Security & Scrubbing üõ°Ô∏è

The VCR automatically redacts `api_key`, `authorization`, and other sensitive headers. You can add custom redaction:

```typescript
withVCR({
  // Redact by key name
  sensitiveKeys: ["user_ssn", "stripe_token"],
  
  // Redact by value pattern (Regex)
  sensitivePatterns: [/sk-test-[0-9a-zA-Z]+/g],
  
  // Advanced: Custom function hook
  scrub: (data) => data.replace(/SSN: \d+/g, "[REDACTED_SSN]")
}, async () => { ... });
```
### Global Configuration üåç

Instead of repeating configuration in every test, set global defaults in your test setup file:

```typescript
import { configureVCR } from "@node-llm/testing";

configureVCR({
  cassettesDir: "test/__cassettes__", // Configurable global path
  sensitiveKeys: ["user_ssn", "stripe_token"],
  sensitivePatterns: [/sk-test-[0-9a-zA-Z]+/g]
});
```

### Per-Test Overrides

You can still override defaults on a per-test basis:

```typescript
withVCR({
  // Merged with global config
  sensitiveKeys: ["specific_secret"] 
}, async () => { ... });
```
---

## üé≠ Mocker Usage

### Fluent Mocking

Define lightning-fast, zero-network tests for your agents.

```typescript
import { mockLLM } from "@node-llm/testing";

const mocker = mockLLM();

// Exact match
mocker.chat("Ping").respond("Pong");

// Regex match
mocker.chat(/hello/i).respond("Greetings!");

// Simulate a Tool Call
mocker.chat("What's the weather?").callsTool("get_weather", { city: "London" });
```

### Streaming Mocks üåä

Test your streaming logic by simulating token delivery.

```typescript
mocker.chat("Tell a story").stream(["Once ", "upon ", "a ", "time."]);
```

### Multimodal Mocks üé®

```typescript
mocker.paint(/a cat/i).respond({ url: "https://mock.com/cat.png" });
mocker.embed("text").respond({ vectors: [[0.1, 0.2, 0.3]] });
```

### Call Verification & History üïµÔ∏è‚Äç‚ôÄÔ∏è

Inspect what requests were sent to your mock, enabling "spy" style assertions.

```typescript
// 1. Check full history
const history = mocker.history;
expect(history.length).toBe(1);

// 2. Filter by method
const chats = mocker.getCalls("chat");
expect(chats[0].args[0].messages[0].content).toContain("Hello");

// 3. Get the most recent call
const lastEmbed = mocker.getLastCall("embed");
expect(lastEmbed.args[0].input).toBe("text to embed");

// 4. Reset history (keep mocks)
mocker.resetHistory();
```

---

## üõ£Ô∏è Decision Tree: VCR vs Mocker

Choose the right tool for your test:

```
Does your test need to verify behavior against REAL LLM responses?
‚îú‚îÄ YES ‚Üí Use VCR (integration testing)
‚îÇ   ‚îú‚îÄ Do you need to record the first time and replay afterward?
‚îÇ   ‚îÇ   ‚îî‚îÄ YES ‚Üí Use VCR in "record" or "auto" mode
‚îÇ   ‚îú‚îÄ Are you testing in CI/CD? (No live API calls allowed)
‚îÇ   ‚îÇ   ‚îî‚îÄ YES ‚Üí Set VCR_MODE=replay in CI
‚îÇ   ‚îî‚îÄ Need custom scrubbing for sensitive data?
‚îÇ       ‚îî‚îÄ YES ‚Üí Use withVCR({ scrub: ... })
‚îÇ
‚îî‚îÄ NO ‚Üí Use Mocker (unit testing)
    ‚îú‚îÄ Testing error handling, edge cases, or rare paths?
    ‚îÇ   ‚îî‚îÄ YES ‚Üí Mock the error with mocker.chat(...).respond({ error: ... })
    ‚îú‚îÄ Testing streaming token delivery?
    ‚îÇ   ‚îî‚îÄ YES ‚Üí Use mocker.chat(...).stream([...])
    ‚îî‚îÄ Testing tool-calling paths without real tools?
        ‚îî‚îÄ YES ‚Üí Use mocker.chat(...).callsTool(name, params)
```

**Quick Reference**:
- **VCR**: Database queries, API calls, real provider behavior, network latency
- **Mocker**: Business logic, UI interactions, error scenarios, tool orchestration

### At-a-Glance Comparison

| Use Case | VCR | Mocker |
|----------|-----|--------|
| Real provider behavior | ‚úÖ | ‚ùå |
| CI-safe (no live calls) | ‚úÖ (after record) | ‚úÖ |
| Zero network overhead | ‚ùå (first run) | ‚úÖ |
| Error simulation | ‚ö†Ô∏è (record real) | ‚úÖ |
| Tool orchestration | ‚úÖ | ‚úÖ |
| Streaming tokens | ‚úÖ | ‚úÖ |

---

## ‚öôÔ∏è Configuration Contract

| Env Variable       | Description                                                | Default          |
| ------------------ | ---------------------------------------------------------- | ---------------- |
| `VCR_MODE`         | `record`, `replay`, `auto`, or `passthrough`               | `auto`           |
| `VCR_CASSETTE_DIR` | Base directory for cassettes                               | `test/cassettes` |
| `CI`               | When true, VCR prevents recording and forces exact matches | (Auto-detected)  |

---

## üèõÔ∏è Integration with @node-llm/orm

The testing tools operate at the `providerRegistry` level. This means they **automatically** intercept LLM calls made by the ORM layer.

### Pattern: Testing Database Persistence

When using `@node-llm/orm`, you can verify both the database state and the LLM response in a single test.

```typescript
import { withVCR } from "@node-llm/testing";
import { createChat } from "@node-llm/orm/prisma";

it(
  "saves the LLM response to the database",
  withVCR(async () => {
    // 1. Setup ORM Chat
    const chat = await createChat(prisma, llm, { model: "gpt-4" });

    // 2. Interaction (VCR intercepts the LLM call)
    await chat.ask("Hello ORM!");

    // 3. Verify DB state (standard Prisma/ORM assertions)
    const messages = await prisma.assistantMessage.findMany({
      where: { chatId: chat.id }
    });

    expect(messages).toHaveLength(2); // User + Assistant
    expect(messages[1].content).toBeDefined();
  })
);
```

### Pattern: Mocking Rare Logic

Use the `Mocker` to test how your application handles complex tool results or errors without setting up a real LLM.

```typescript
import { mockLLM } from "@node-llm/testing";

it("handles tool errors in ORM sessions", async () => {
  const mocker = mockLLM();
  mocker.chat("Search docs").respond({ error: new Error("DB Timeout") });

  const chat = await loadChat(prisma, llm, "existing-id");

  await expect(chat.ask("Search docs")).rejects.toThrow("DB Timeout");
});
```

---

## üß™ Framework Integration

### Vitest (Native Support)

Vitest is the primary test framework with optimized helpers:

```typescript
import { it, describe } from "vitest";
import { mockLLM, withVCR, describeVCR } from "@node-llm/testing";

describeVCR("Payments", () => {
  it(
    "processes successfully",
    withVCR(async () => {
      // ‚ú® withVCR auto-detects test name ("processes successfully")
      // ‚ú® describeVCR auto-manages scopes
    })
  );
});
```

### Jest Compatibility

All core APIs work with Jest. The only difference: `withVCR()` can't auto-detect test names, so provide it manually:

```typescript
import { describe, it } from "@jest/globals";
import { mockLLM, setupVCR, describeVCR } from "@node-llm/testing";

describeVCR("Payments", () => {
  it("processes successfully", async () => {
    // ‚úÖ describeVCR works with Jest (framework-agnostic)
    // ‚ö†Ô∏è withVCR doesn't work here (needs Vitest's expect.getState())
    // ‚úÖ Use setupVCR instead:
    const vcr = setupVCR("processes", { mode: "record" });

    const mocker = mockLLM();  // ‚úÖ works with Jest
    mocker.chat("pay").respond("done");

    // Test logic here

    await vcr.stop();
  });
});
```

### Framework Support Matrix

| API | Vitest | Jest | Any Framework |
|-----|--------|------|---------------|
| `mockLLM()` | ‚úÖ | ‚úÖ | ‚úÖ |
| `describeVCR()` | ‚úÖ | ‚úÖ | ‚úÖ |
| `setupVCR()` | ‚úÖ | ‚úÖ | ‚úÖ |
| `withVCR()` | ‚úÖ (auto name) | ‚ö†Ô∏è (manual name) | ‚ö†Ô∏è (manual name) |
| Mocker class | ‚úÖ | ‚úÖ | ‚úÖ |
| VCR class | ‚úÖ | ‚úÖ | ‚úÖ |

**Only `withVCR()` is Vitest-specific** because it auto-detects test names. All other APIs are framework-agnostic.

### Any Test Framework

Using raw classes for maximum portability:

```typescript
import { Mocker, VCR } from "@node-llm/testing";

// Mocker - works everywhere
const mocker = new Mocker();
mocker.chat("hello").respond("hi");

// VCR - works everywhere
const vcr = new VCR("test-name", { mode: "record" });
// ... run test ...
await vcr.stop();
```

---

## üö® Common Error Scenarios

### VCR: Missing Cassette

**Error**: `Error: Cassette file not found`

**Cause**: VCR is in `replay` mode but the cassette doesn't exist yet.

**Solution**:
```bash
# Record it first
VCR_MODE=record npm test

# Or use auto mode (records if missing, replays if exists)
VCR_MODE=auto npm test
```

### VCR: Cassette Mismatch

**Error**: `AssertionError: No interaction matched the request`

**Cause**: Your code is making a request that doesn't match any recorded interaction.

**Solution**:
```bash
# Re-record the cassette
rm -rf test/cassettes/your-test
VCR_MODE=record npm test -- your-test
```

### Mocker: Strict Mode Violation

**Error**: `Error: No mock defined for prompt: "unexpected question"`

**Cause**: Your code asked a question you didn't mock in strict mode.

**Solution**:
```typescript
// Add the missing mock
mocker.chat("unexpected question").respond("mocked response");

// Or disable strict mode
const mocker = mockLLM({ strict: false });
```

### Mocker: Debug Information

Get insight into what mocks are registered:

```typescript
const mocker = mockLLM();
mocker.chat("hello").respond("hi");
mocker.embed("text").respond({ vectors: [[0.1, 0.2]] });

const debug = mocker.getDebugInfo();
console.log(debug);
// Output: { totalMocks: 2, methods: ["chat", "embed"] }
```

---

## üéØ Advanced Patterns

### Pattern: Parametrized Testing with VCR

Test the same logic against multiple scenarios by organizing cassettes hierarchically:

```typescript
describeVCR("Payment Processing", () => {
  ["visa", "mastercard", "amex"].forEach((cardType) => {
    describeVCR(cardType, () => {
      it(
        "processes payment",
        withVCR(async () => {
          const result = await processor.pay({
            amount: 100,
            cardType
          });
          expect(result.status).toBe("success");
        })
      );
    });
  });
});

// Cassettes created at:
// test/cassettes/payment-processing/visa/processes-payment.json
// test/cassettes/payment-processing/mastercard/processes-payment.json
// test/cassettes/payment-processing/amex/processes-payment.json
```

### Pattern: Strict Mode for Safety

Enforce that every expected interaction is mocked:

```typescript
describe("Customer Service Bot", () => {
  it("responds to greeting", async () => {
    const mocker = mockLLM({ strict: true });
    mocker.chat("hello").respond("Hello! How can I help?");
    
    await bot.handle("hello");
    // Pass ‚úÖ
  });

  it("fails if unmocked", async () => {
    const mocker = mockLLM({ strict: true });
    mocker.chat("hello").respond("Hello!");
    
    // This throws because "goodbye" wasn't mocked
    await expect(bot.handle("goodbye")).rejects.toThrow();
  });
});
```

### Pattern: Testing Streaming

Simulate token delivery to verify UI updates correctly:

```typescript
it("displays tokens as they arrive", async () => {
  const mocker = mockLLM();
  mocker.chat("Write a poem").stream([
    "Roses ",
    "are ",
    "red\n",
    "Violets ",
    "are ",
    "blue"
  ]);

  const tokens = [];
  await llm.stream("Write a poem", {
    onToken: (token) => tokens.push(token)
  });

  expect(tokens).toEqual([
    "Roses ",
    "are ",
    "red\n",
    "Violets ",
    "are ",
    "blue"
  ]);
});
```

---

## üèõÔ∏è Architecture Contract

- **No Side Effects**: Mocks and VCR interceptors are automatically cleared after each test turn.
- **Deterministic**: The same input MUST always yield the same output in Replay mode.
- **Explicit > Implicit**: We prefer explicit mock definitions over complex global state.
 
 ---
 
 ## üõë When Not to Use @node-llm/testing
 
 - Do not use **VCR** for rapid prompt iteration ‚Äî use live calls instead.
 - Do not use **Mocker** to validate response quality or correctness.
 - Do not commit **cassettes** for experimental or throwaway prompts.


<!-- END FILE: core-features/testing.md -->
----------------------------------------

<!-- FILE: core-features/tools.md -->

# üìÑ core-features/tools.md

---
layout: default
title: Tool Calling
nav_order: 5
parent: Core Features
permalink: /core-features/tools
description: Give your models the ability to interact with the real world using a clean class-based DSL, automatic execution loops, and built-in safety guards.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` simplifies function calling (tool use) by handling the execution loop automatically. You define the tools, and the library invokes them when the model requests it.

```bash
npm install @node-llm/core
```

{: .highlight }

> **Looking for a real-world example?** Check out the [Brand Perception Checker](https://github.com/node-llm/node-llm/tree/main/examples/applications/brand-perception-checker), which uses the `SerpTool` to perform live Google searches and "read" the results to extract semantic signals.

---

## Class-Based Tools <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">New ‚ú®</span>

The recommended way to define tools is by using the `Tool` class. This provides auto-generated JSON schemas and full type safety using `zod`.

```ts
import { NodeLLM, Tool, z } from "@node-llm/core";

class WeatherTool extends Tool {
  name = "get_weather";
  description = "Get the current weather for a location";

  // Auto-generates JSON Schema
  schema = z.object({
    location: z.string().describe("The city and state, e.g. San Francisco, CA"),
    unit: z.enum(["celsius", "fahrenheit"]).default("celsius")
  });

  async execute({ location, unit }) {
    // Your business logic
    const weather = await fetchWeather(location);
    return { temp: 22, unit, condition: "Sunny" };
  }
}

// Register as a class (instantiated automatically) or instance
const chat = llm.chat().withTool(WeatherTool);
await chat.ask("What is the weather in SF?");
```

### Benefits

- **No Boilerplate**: No need to write manual JSON schemas.
- **Type Safety**: `execute()` arguments are automatically typed from your schema.
- **Self-Documenting**: The Zod `.describe()` calls are automatically pulled into the tool's description for the LLM.

### Defining Parameters with Zod

`NodeLLM` uses `zod-to-json-schema` under the hood. Most standard Zod types work out of the box:

| Zod Type              | Description                                         |
| :-------------------- | :-------------------------------------------------- |
| **All Fields**        | **Required by default**.                            |
| `z.string()`          | Basic text string.                                  |
| `z.number()`          | Number (integer or float).                          |
| `z.boolean()`         | Boolean flag.                                       |
| `z.enum(["a", "b"])`  | String restricted to specific values.               |
| `z.object({...})`     | Nested object.                                      |
| `z.array(z.string())` | Array of items.                                     |
| `.describe("...")`    | **Crucial**: Adds a description for the LLM.        |
| `.optional()`         | Marks the field as not required.                    |
| `.default(val)`       | Sets a default value if the LLM doesn't provide it. |

---

## Using Tools in Chat

Use the fluent `.withTool()` or `.withTools()` API to register tools for a chat session. By default, tools are appended. You can use the `replace` option to clear previous tools.

```ts
// Append tools
const chat = llm.chat("gpt-4o").withTools([WeatherTool, CalculatorTool]);

// Replace all existing tools with a new list
chat.withTools([SearchTool], { replace: true });

const reply = await chat.ask("What is the weather in London?");
```

---

## Tools Work in Streaming Too! <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">New ‚ú®</span>

Tools now work seamlessly with streaming! The same tool execution happens automatically during streaming:

```ts
const chat = llm.chat("gpt-4o").withTool(WeatherTool);

// Tool is automatically executed during streaming
for await (const chunk of chat.stream("What's the weather in Paris?")) {
  process.stdout.write(chunk.content || "");
}
```

See the [Streaming documentation](streaming.html#streaming-with-tools-) for more details.

---

## Parallel Tool Calling

If the provider supports it (like OpenAI and Anthropic), the model can call multiple tools in a single turn. `NodeLLM` handles the concurrent execution of these tools automatically.

See [examples/scripts/openai/chat/parallel-tools.mjs](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/parallel-tools.mjs) for a demo.

---

## Loop Protection (Loop Guard) üõ°Ô∏è

To prevent infinite recursion and runaway costs (where a model keeps calling tools without reaching a conclusion), `NodeLLM` includes a built-in Loop Guard.

By default, `NodeLLM` will throw an error if a model performs more than **5 sequential tool execution turns** in a single request.

### Customizing the Limit

You can configure this limit globally or override it for a specific request:

```ts
// 1. Global Change
const llm = createLLM({ maxToolCalls: 10 });

await chat.ask("Perform a complex deep research task", {
  maxToolCalls: 15
});
```

---

## Tool Execution Policies (Security) <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.0+</span>

For sensitive operations, you can control the "autonomy" of the tool execution loop using `withToolExecution()`.

- **`auto`**: (Default) Tools are executed immediately as proposed by the LLM.
- **`confirm`**: Enables **Human-in-the-loop**. NodeLLM pauses before execution and awaits approval via the `onConfirmToolCall` hook.
- **`dry-run`**: Proposes the tool call structure but **never executes it**. Useful for UI previews or verification-only flows.

```ts
chat.withToolExecution("confirm").onConfirmToolCall(async (call) => {
  // Audit the call or ask the user
  console.log(`LLM wants to call ${call.function.name}`);
  return true; // Return true to execute, false to cancel
});
```

### Inspected Proposals

In `confirm` and `dry-run` modes, the `ChatResponseString` object returned by `.ask()` includes a `.tool_calls` property. This allows you to inspect exactly what the model _wanted_ to do.

```ts
const res = await chat.withToolExecution("dry-run").ask("Delete all users");
console.log(res.tool_calls); // [{ id: '...', function: { name: 'delete_users', ... } }]
```

---

## Advanced Tool Metadata

Some providers support additional metadata in tool definitions, such as Anthropic's **Prompt Caching**. You can include these fields in your tool class, and `NodeLLM` will pass them through.

```ts
class HistoryTool extends Tool {
  name = "get_history";
  description = "Get chat history";
  schema = z.object({ limit: z.number().default(10) });

  // Add provider-specific metadata
  cache_control = { type: 'ephemeral' };

  async execute({ limit }) {
    return [...];
  }

  // Override toLLMTool to include custom metadata if needed
  toLLMTool() {
    const def = super.toLLMTool();
    return {
      ...def,
      cache_control: this.cache_control
    };
  }
}
```

---

## Error Handling & Flow Control <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.1+</span>

`NodeLLM` handles tool errors intelligently to prevent infinite retry loops through a combination of automatic infrastructure protection and manual flow control.

### Zero-Config Safety (Fatal Errors)

By default, the agent loop will **immediately stop and throw** if it encounters an unrecoverable "fatal" error. This prevents wasting tokens on retries that are guaranteed to fail.

Fatal errors include:

- **Authentication Errors**: HTTP 401 or 403 errors from LLM providers or external APIs.
- **Explicit Fatal Errors**: Any error thrown using the `ToolError` class with `fatal: true`.

```ts
import { Tool, ToolError } from "@node-llm/core";

class DatabaseTool extends Tool {
  async execute({ query }) {
    if (isMalicious(query)) {
      // Force the agent to stop immediately
      throw new ToolError("Security Violation", "db_tool", { fatal: true });
    }
  }
}
```

### Hook-Based Flow Control (STOP | CONTINUE)

For granular control, you can use the `onToolCallError` hook to override internal logic. This allows you to differentiate between tools that are "mission-critical" and those that are "optional."

The hook can return one of two directives:

- **`"STOP"`**: Force the agent to crash and bubble the error up to your code.
- **`"CONTINUE"`**: Catch the error, log it, and tell the agent to ignore it and move to the next turn.

```ts
const chat = llm.chat("gpt-4o", {
  onToolCallError: (toolCall, error) => {
    // 1. Critical Tool: Stop everything
    if (toolCall.function.name === "process_payment") {
      return "STOP";
    }

    // 2. Optional Tool: Just ignore if it fails
    if (toolCall.function.name === "fetch_avatar") {
      console.warn("Avatar fetch failed, but continuing...");
      return "CONTINUE";
    }

    // 3. Default: Let NodeLLM decide (e.g. stop on 401/403)
  }
});
```

### Recoverable Errors (AI Self-Correction)

If you want the model to see the error and try to fix its own parameters, simply return a string or object from your handler. NodeLLM will feed this back to the model as a successful tool result containing error details.

```ts
async execute({ date }) {
  if (!isValid(date)) {
    return { error: "Invalid date format. Please use YYYY-MM-DD." };
  }
}
```

---

## Advanced: Raw JSON Schema

If you prefer to define your parameters using standard JSON Schema instead of Zod, you can pass a schema object directly to the `schema` property in your `Tool` class. This is useful for migrating existing tools or when you already have schema definitions.

```ts
class CustomTool extends Tool {
  name = "custom_lookup";
  description = "Lookup items in a legacy system";

  // Use Raw JSON Schema instead of Zod
  schema = {
    type: "object",
    properties: {
      sku: { type: "string", description: "Product SKU" },
      limit: { type: "integer", minimum: 1, maximum: 100 }
    },
    required: ["sku"]
  };

  async execute({ sku, limit }) {
    // Arguments are still passed as a single object
    return { status: "found" };
  }
}
```

---

## Function-Based Tools (Legacy)

For simply wrapping a function without a class, you can define a tool as a plain object with a `handler`.

```ts
const weatherTool = {
  type: "function",
  function: {
    name: "get_weather",
    description: "Get the current weather for a location",
    parameters: {
      type: "object",
      properties: {
        location: { type: "string", description: "City and state" }
      },
      required: ["location"]
    }
  },
  handler: async ({ location }) => {
    return JSON.stringify({ location, temp: 22, unit: "celsius" });
  }
};

chat.withTool(weatherTool);
```

---

## Security Considerations

Treat arguments passed to your `execute` method as **untrusted user input**.

- **Validate**: Always validate parameter types and ranges using libraries like `zod` inside the handler if critical.
- **Sanitize**: Sanitize strings before using them in database queries or shell commands.
- **Avoid Eval**: Never use `eval()` on inputs provided by the model.

---

## Debugging Tools

To see exactly what the model is calling and what your tool is returning, enable debug mode:

```bash
export NODELLM_DEBUG=true
```

You will see logs like:
`[NodeLLM] Tool call: get_weather { location: "Paris" }`
`[NodeLLM] Tool result: { temp: 15 }`


<!-- END FILE: core-features/tools.md -->
----------------------------------------

<!-- FILE: providers/anthropic.md -->

# üìÑ providers/anthropic.md

---
layout: default
title: Anthropic
parent: Providers
nav_order: 3
description: Experience the Claude family of models with native support for PDF document analysis, advanced reasoning, and long-context capabilities.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The Anthropic provider gives access to the Claude family of models, known for high-quality reasoning and coding capabilities.

---

## Configuration

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "anthropic", 
  anthropicApiKey: process.env.ANTHROPIC_API_KEY // Optional if set in env 
});
```

---

## Specific Parameters

You can pass Anthropic-specific parameters or custom headers.

```ts
const chat = llm.chat("claude-3-5-sonnet-20241022").withParams({
  top_k: 50,
  top_p: 0.9,
  // Custom headers if needed
  headers: {
    "anthropic-beta": "max-tokens-3-5-sonnet-2024-07-15"
  }
});
```

---

## Features

- **Models**: `claude-3-7-sonnet`, `claude-3-5-sonnet`, `claude-3-opus`, `claude-3-haiku`.
- **Vision**: Analyzes images.
- **PDF Support**: Can read and analyze PDF documents natively.
- **Tools**: Fully supported.
- **Reasoning**: Support for Extended Thinking and token-based pricing for `claude-3-7`.

---

## PDF Support

Anthropic supports sending PDF files as base64 encoded blocks, which `NodeLLM` handles automatically.

```ts
await chat.ask("Summarize this document", {
  files: ["./report.pdf"]
});
```


<!-- END FILE: providers/anthropic.md -->
----------------------------------------

<!-- FILE: providers/bedrock.md -->

# üìÑ providers/bedrock.md

---
layout: default
title: Amazon Bedrock
parent: Providers
nav_order: 7
description: Access models from Amazon Titan, Anthropic, Meta, and Stability AI through a secure, zero-dependency AWS implementation.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The Amazon Bedrock provider uses a **zero-dependency** implementation of the AWS SigV4 signing process. This means you do **not** need to install the heavy `@aws-sdk/client-bedrock-runtime` package. NodeLLM handles all authentication and request signing natively.

---

## Configuration

NodeLLM automatically attempts to load AWS credentials from standard environment variables matching the AWS CLI.

### 1. Environment Variables (Recommended)

```bash
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="wJalrX..."
export AWS_REGION="us-east-1"
# Optional session token for temporary credentials
export AWS_SESSION_TOKEN="..."
```

### 2. Manual Configuration

You can also pass credentials explicitly when initializing the LLM.

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "bedrock", 
  bedrockRegion: "us-east-1",
  bedrockAccessKeyId: "AKIA...",
  bedrockSecretAccessKey: "..."
});
```

---

## Features

- **Models**: Access to `amazon.titan`, `anthropic.claude`, `meta.llama3`, `mistral`, `cohere`, and `amazon.nova`.
- **Cross-Region Inference**: Natively supports inference profiles (e.g., `us.anthropic.claude-3-5-sonnet...`) for higher throughput.
- **Image Generation**: First-class support for **Titan Image Generator** and **Stable Diffusion**.
- **Prompt Caching**: Save up to 90% on costs with Claude and Nova models.
- **Multimodal**: Send images to Claude and Nova models easily.
- **Extended Thinking (Reasoning)**: Native support for Claude 3.7 and DeepSeek R1 thinking budgets.
- **Guardrail Visibility**: Access raw Guardrail trace assessments via response metadata.

---

## Image Generation

Use the `paint()` method to generate images using Bedrock's specialized models.

```ts
const response = await llm.paint("A futuristic city on Mars, high quality, 4k", {
  model: "amazon.titan-image-generator-v2:0", // or "stability.stable-diffusion-xl-v1:0"
  size: "1024x1024"
});

// Save to disk
await response.save("./mars-city.png");

// Or access raw base64
console.log(response.data);
```

---

## Prompt Caching

NodeLLM supports Amazon Bedrock's **Prompt Caching** (via the Converse API). This allows you to cache large context blocks (like documents or system prompts) to reduce latency and cost.

Use the standard `cache_control: { type: "ephemeral" }` API (same as Anthropic) to enable it.

```ts
// System Prompt Caching
const chat = llm.chat("anthropic.claude-3-5-sonnet-20240620-v1:0");

// Automatically creates a Bedrock 'cachePoint'
chat.add("system", [
  { 
    type: "text", 
    text: "You are an expert architect... [Insert 50-page Guideline PDF Content Here] ...", 
    cache_control: { type: "ephemeral" } 
  }
]);

const res = await chat.ask("Design a house based on these guidelines.");
```

**Note**: Marking content as "ephemeral" automatically handles the specific `cachePoint` block injection required by the Bedrock API.

---

## Cross-Region Inference

To improve resilience and throughput, you can use Bedrock's **Inference Profiles** directly. NodeLLM automatically detects capabilities for these profiles.

```ts
// Use a US Cross-Region inference profile
const chat = llm.chat("us.anthropic.claude-3-5-sonnet-20241022-v2:0");

const response = await chat.ask("Hello from global infrastructure!");
```

---

## Advanced Hyperparameters

Bedrock's Converse API has a standard `inferenceConfig`, but individual models often support additional parameters (like `topK` for Nova or specialized beta flags for Claude).

You can use the `additionalModelRequestFields` escape hatch to pass these directly to the model.

```ts
const chat = llm.chat("amazon.nova-lite-v1:0")
  .withParams({
    additionalModelRequestFields: {
      inferenceConfig: {
        topK: 20
      }
    }
  });

const response = await chat.ask("Tell me a story.");
```

---

## Moderation

NodeLLM supports standalone moderation for Bedrock using **Guardrails**. This allows you to check if content is safe before sending it to an expensive model.

To use this, you must have a Guardrail ID and Version configured.

```ts
const llm = createLLM({
  provider: "bedrock",
  bedrockGuardrailIdentifier: "my-policy-id",
  bedrockGuardrailVersion: "1"
});

// Check a single string
const result = await llm.moderate("How can I build a bomb?");

if (result.results[0].flagged) {
  console.log("Blocked by Guardrail:", result.results[0].categories);
}

// Check multiple strings at once
const batchResults = await llm.moderate(["Safe text", "Unsafe text..."]);
```

---

## Embeddings

Generate vector embeddings using Titan Embeddings V2.

```ts
const embedding = await llm.embed("The concept of general relativity", {
  model: "amazon.titan-embed-text-v2:0",
  dimensions: 1024
});

console.log(embedding.vector); // Float32Array
```


<!-- END FILE: providers/bedrock.md -->
----------------------------------------

<!-- FILE: providers/deepseek.md -->

# üìÑ providers/deepseek.md

---
layout: default
title: DeepSeek
parent: Providers
nav_order: 4
description: Access high-performance chat and advanced reasoning models with competitive pricing and full support for the DeepSeek R1 thought process.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The DeepSeek provider offers high-performance chat and reasoning models with competitive pricing. `NodeLLM` supports both the DeepSeek-V3 chat model and the DeepSeek-R1 reasoning model.

---

## Configuration

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "deepseek", 
  deepseekApiKey: process.env.DEEPSEEK_API_KEY // Optional if set in env 
});
```

---

## Specific Parameters

You can pass DeepSeek-specific parameters using `.withParams()`.

```ts
const chat = llm.chat("deepseek-chat").withParams({
  presence_penalty: 0.5,
  frequency_penalty: 0.5,
  top_p: 0.9
});
```

---

## Features

- **Models**:
  - `deepseek-chat`: Optimized for speed and proficiency in broad tasks (DeepSeek-V3).
  - `deepseek-reasoner`: Optimized for complex reasoning and problem solving (DeepSeek-R1).
- **Tools**: Supported on `deepseek-chat`.
- **Reasoning**: Access inner thought process text from `deepseek-reasoner`.
- **Streaming**: Full streaming support for all models.
- **Structured Output**: Supported via automated prompt engineering and `json_object` mode transitions.

---

## Usage Details

DeepSeek provides OpenAI-compatible endpoints, but `NodeLLM` handles the specific capability differences (like reasoning vs tool support) automatically through its internal registry.


<!-- END FILE: providers/deepseek.md -->
----------------------------------------

<!-- FILE: providers/gemini.md -->

# üìÑ providers/gemini.md

---
layout: default
title: Gemini
parent: Providers
nav_order: 2
description: Leverage Google's powerful multimodal capabilities with native support for image, audio, and video processing alongside long-context reasoning.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Google's Gemini provider offers multimodal capabilities including native video and audio understanding.

---

## Configuration

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "gemini", 
  geminiApiKey: process.env.GEMINI_API_KEY // Optional if set in env 
});
```

---

## Specific Parameters

Gemini uses `generationConfig` and `safetySettings`.

```ts
const chat = llm.chat("gemini-1.5-pro").withParams({
  generationConfig: {
    topP: 0.8,
    topK: 40,
    maxOutputTokens: 8192
  },
  safetySettings: [
    {
      category: "HARM_CATEGORY_HARASSMENT",
      threshold: "BLOCK_LOW_AND_ABOVE"
    }
  ]
});
```

---

## Features

- **Models**: `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-2.0-flash`.
- **Multimodal**: Supports images, audio, and video files directly.
- **Tools**: Supported.
- **System Instructions**: Supported.

---

## Video Support

Gemini is unique in its ability to natively process video files.

```ts
await chat.ask("What happens in this video?", {
  files: ["./video.mp4"]
});
```


<!-- END FILE: providers/gemini.md -->
----------------------------------------

<!-- FILE: providers/index.md -->

# üìÑ providers/index.md

---
layout: default
title: Providers
nav_order: 5
has_children: true
nav_fold: false
permalink: /providers
description: Detailed guides for every supported AI provider, including specific features and authentication configurations.
back_to_top: false
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }


<!-- END FILE: providers/index.md -->
----------------------------------------

<!-- FILE: providers/ollama.md -->

# üìÑ providers/ollama.md

---
layout: default
title: Ollama
parent: Providers
nav_order: 5
description: Run Large Language Models locally on your machine with full support for vision, tools, and embeddings while maintaining total data sovereignty.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Allows you to run large language models locally using [Ollama](https://ollama.com/).

---

## Configuration

Standard configuration for local inference (defaults to `http://localhost:11434/v1`):

```javascript
import { createLLM } from "@node-llm/core";

// Defaults to http://localhost:11434/v1
const llm = createLLM({ provider: "ollama" });
```

### Custom URL

If your Ollama instance is running on a different machine or port:

```javascript
const llm = createLLM({ 
  provider: "ollama", 
  ollamaApiBase: "http://192.168.1.10:11434/v1" // Note the /v1 suffix 
});
```

---

## Specific Parameters

You can pass Ollama/OpenAI-compatible parameters using `.withParams()`.

```javascript
const chat = llm.chat("llama3").withParams({
  temperature: 0.7,
  seed: 42,
  num_ctx: 8192 // Ollama specific context size
});
```

---

## Features

- **Models**: Supports any model pulled via `ollama pull`.
- **Vision**: Use vision-capable models like `llama3.2-vision` or `llava`.
- **Tools**: Fully supported for models with tool-calling capabilities (e.g., `llama3.1`).
- **Embeddings**: High-performance local vector generation.
- **Model Discovery**: Inspect your local library and model metadata via `NodeLLM.listModels()`.

### Multimodal (Vision)

```javascript
const response = await chat.ask("Describe this image", {
  files: ["./image.png"]
});
```

### Model Discovery

List all models currently pulled in your Ollama library to inspect their context windows and features:

```javascript
const models = await NodeLLM.listModels();
console.table(models);
```

---

## Limitations

The following features are **not** supported natively by Ollama's OpenAI-compatible API:

- **Transcription** (Whisper): Not available via the `/v1/audio` endpoint.
- **Image Generation**: Not available via the `/v1/images` endpoint.
- **Moderation**: Not supported.

For full feature parity locally, consider using [LocalAI](https://localai.io/) and connecting via the [OpenAI Provider](/providers/openai.html) with a custom `openaiApiBase`.


<!-- END FILE: providers/ollama.md -->
----------------------------------------

<!-- FILE: providers/openai.md -->

# üìÑ providers/openai.md

---
layout: default
title: OpenAI
parent: Providers
nav_order: 1
description: Full support for the complete range of NodeLLM features including tool calling, vision, image generation, and the advanced Developer role.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The OpenAI provider supports the full range of `NodeLLM` features, including robust tool calling, vision, and structured outputs.

---

## Configuration

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "openai", 
  openaiApiKey: process.env.OPENAI_API_KEY // Optional if set in env 
});
```

---

## Specific Parameters

You can pass OpenAI-specific parameters using `.withParams()`.

```ts
const chat = llm.chat("gpt-4o").withParams({
  seed: 42, // for deterministic output
  user: "user-123", // for user tracking
  presence_penalty: 0.5,
  frequency_penalty: 0.5
});
```

---

## Features

- **Models**: `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, etc.
- **Vision**: Specific models like `gpt-4o` support image analysis.
- **Tools**: Fully supported, including parallel tool execution.
- **Reasoning**: Automatic tracking of reasoning tokens and costs for `o1` and `o3` models.
- **Smart Developer Role**: Modern instructions are automatically mapped to the `developer` role for compatible models when using the official API.
- **Structured Output**: Supports strict schema validation via `json_schema`.

---

## Custom Endpoints

OpenAI's client is also used for compatible services like Ollama, LocalAI, and Azure OpenAI. See [Custom Endpoints](/advanced/custom_endpoints.html) for details.


<!-- END FILE: providers/openai.md -->
----------------------------------------

<!-- FILE: providers/openrouter.md -->

# üìÑ providers/openrouter.md

---
layout: default
title: OpenRouter
parent: Providers
nav_order: 5
description: Access hundreds of open-source and proprietary models through a single gateway with unified tool calling, vision, and reasoning support.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The OpenRouter provider acts as a unified gateway to AI models from multiple providers. `NodeLLM` leverages OpenRouter's standardized API while providing additional capabilities like integrated tool calling and vision.

---

## Configuration

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ 
  provider: "openrouter", 
  openrouterApiKey: process.env.OPENROUTER_API_KEY 
});
```

---

## Features

- **Model Discovery**: Full support for `NodeLLM.listModels()` to explore available models.
- **Unified API**: Switch between models from OpenAI, Anthropic, Google, and Meta using a single configuration.
- **Vision**: Supported for multimodal models.
- **Tools**: Supported for models with function calling capabilities.
- **Reasoning**: Access chain-of-thought for reasoning-capable models (e.g., DeepSeek R1).
- **Streaming**: Native streaming support with the advanced `Stream` utility.

---

## Specific Parameters

OpenRouter supports various unique parameters that can be passed via `.withParams()`:

```ts
const chat = llm.chat("google/gemini-2.0-flash-exp:free").withParams({
  transforms: ["middle-out"], // OpenRouter specific compression
  route: "fallback"
});
```


<!-- END FILE: providers/openrouter.md -->
----------------------------------------

<!-- FILE: orm/index.md -->

# üìÑ orm/index.md

---
layout: default
title: ORM & Persistence
nav_order: 4
has_children: true
permalink: /orm
description: Database persistence layer for NodeLLM. Automatically track chats, messages, tool calls, and API metrics.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

---

## Quick Setup

NodeLLM ORM provides a robust persistence layer that bridges the gap between your application database and LLM providers. It ensures that every turn in a conversation is safely stored, while maintaining high performance for real-time streaming.

Currently, we support **Prisma** with a dedicated adapter.

### installation

```bash
npm install @node-llm/orm @node-llm/core @prisma/client
```

---

## Strategic Design

The ORM is designed to be an **infrastructure-first** layer, much like the core package. It doesn't just store text; it captures the entire execution lifecycle, including:

- **Token Consumption**: Track input/output/thinking tokens per message and per request.
- **Reasoning & Thinking Process**: Capture internal chain-of-thought text and cryptographic signatures for modern reasoning models.
- **Tool Audit Trail**: Record every tool call, its parameters, thought process, and result.
- **Provider status**: Know exactly which model and provider served which message.
- **Request Metadata**: Log latency, status codes, and cost for every API interaction.

[Explore the Prisma Adapter](/orm/prisma.html){: .btn .btn-primary .fs-5 .mb-4 .mb-md-0 .mr-2 }


<!-- END FILE: orm/index.md -->
----------------------------------------

<!-- FILE: orm/migrations.md -->

# üìÑ orm/migrations.md

# Database Migration Guide

Maintaining a production-grade database requires moving away from `npx prisma db push` (which can cause data loss) to **Prisma Migrate** (which tracks incremental changes via SQL files).

This guide explains how to manage schema updates professionally, translating Rails-style migration discipline into the Node.js / Prisma ecosystem.

**Added in v0.2.0**
{: .label .label-green }

---

## The Migration Workflow

NodeLLM's ORM schema will evolve over time (e.g., adding "Extended Thinking" support). To update your application without losing user chat history, follow this workflow.

### 1. Detect Changes (CLI)

The easiest way to check if your existing database is missing columns for new NodeLLM features is to use the **ORM CLI Sync**:

```bash
npx @node-llm/orm sync
```

**What this does:**
- Scans your `prisma/schema.prisma`.
- Identifies missing fields (like `thinkingText` or `thoughtSignature`).
- Provides guidance on the specific columns to add.

### 2. Update the Schema manually
Modify your `prisma/schema.prisma` with the new fields or models (or copy the latest version from `@node-llm/orm/schema.prisma`).

### 3. Generate a Migration
Instead of pushing directly to the DB, generate a versioned migration file:

```bash
npx prisma migrate dev --name add_thinking_support
```

**What this does:**
- Detects the difference between your `schema.prisma` and your actual database.
- Creates a new folder in `prisma/migrations/` containing a `migration.sql` file.
- Applies that SQL to your local database.

### 3. Commit the Migration
**Crucial**: Always commit the `prisma/migrations` folder to your version control. This ensures all environments (staging, production) apply the exact same SQL changes.

---

## Baseline: Moving from `db push` to `migrate`

If you have been using `db push` and now want to start using formal migrations without losing data:

1. **Clear Drift**: Ensure your database and schema are currently in sync via one last `db push`.
2. **Baseline**: Initialize the migration history by marking the current state as the "initial" version:

```bash
mkdir -p prisma/migrations/0_init
npx prisma migrate diff \
  --from-empty \
  --to-schema-datamodel prisma/schema.prisma \
  --script > prisma/migrations/0_init/migration.sql

npx prisma migrate resolve --applied 0_init
```

---

## Deployment to Production

In production, **never** use `migrate dev`. Instead, use the deployment command which applies all pending migrations in the migrations folder:

```bash
npx prisma migrate deploy
```

---

## Common Scenarios

### Renaming a Column
If you rename a column (e.g., `reasoning` to `thinkingText`), Prisma might try to drop the old column and create a new one, causing data loss.

To fix this:
1. Run `npx prisma migrate dev --name rename_reasoning --create-only`.
2. Open the generated `.sql` file.
3. Replace the `DROP` and `ADD` commands with an `ALTER TABLE ... RENAME COLUMN ...` command.
4. Run `npx prisma migrate dev` to apply your edited SQL.

### Adding Required Fields
When adding a required (`non-nullable`) field to a table with existing data:
1. Generate the migration with `--create-only`.
2. Edit the SQL to provide a default value for existing rows or make it nullable temporarily.
3. Apply the migration.


<!-- END FILE: orm/migrations.md -->
----------------------------------------

<!-- FILE: orm/prisma.md -->

# üìÑ orm/prisma.md

---
layout: default
title: Prisma Integration
parent: ORM & Persistence
nav_order: 1
permalink: /orm/prisma
---

# Prisma Integration
{: .no_toc }

NodeLLM + Prisma made simple. Persist chats, messages, and tool calls automatically.
{: .fs-6 .fw-300 }

**Added in v0.2.0**
{: .label .label-green }

1. TOC
{:toc}

---

## Understanding the Persistence Flow

Before diving into setup, it‚Äôs important to understand how NodeLLM handles message persistence. This design ensures that your database remains the source of truth, even during streaming or complex tool execution loops.

### How It Works

When calling `chat.ask("What is the capital of France?")`, the ORM adapter:

1.  **Creates a User Message** in your database with the input content.
2.  **Creates an Empty Assistant Message** immediately. This serves as a "placeholder" for streaming or pending responses.
3.  **Fetches History** automatically from the database to provide full context to the LLM.
4.  **Executes the Request** via the NodeLLM core:
    *   **On Tool Call Start**: Creates a record in the `ToolCall` table.
    *   **On Tool Call End**: Updates the `ToolCall` record with the result.
    *   **On Response**: Logs the full API metric (tokens, latency, cost) to the `Request` table.
5.  **Finalizes the Assistant Message**: Updates the previously created placeholder with the final content and usage metrics.

### Why This Design?

- **Streaming Optimized**: Creates the database record immediately so your UI can target a specific ID for real-time updates.
- **Audit Ready**: Captures partial tool execution data if a process crashes mid-loop.
- **Automated Cleanup**: If the API call fails or is aborted, the ORM automatically cleans up the message records to prevent orphaned "empty" messages.

---

## Setting Up Your Application

### 1. Schema Configuration

The fastest way to get started is to use the **NodeLLM ORM CLI**. Run this command in your project root to generate the required Prisma schema:

```bash
npx @node-llm/orm init
```

This will create a `prisma/schema.prisma` file (or provide instructions if one already exists) populated with the standard models.

Alternatively, you can manually copy the reference models below. You can customize the model names (e.g., using `AssistantChat` instead of `LlmChat`) using the [Custom Table Names](#custom-table-names) option.

```prisma
model LlmChat {
  id           String       @id @default(uuid())
  model        String?
  provider     String?
  instructions String?      
  metadata     Json?         // Use Json for metadata
  createdAt    DateTime     @default(now())
  updatedAt    DateTime     @updatedAt
  messages     LlmMessage[]
  requests     LlmRequest[]
}

model LlmMessage {
  id                String        @id @default(uuid())
  chatId            String
  role              String        // user, assistant, system, tool
  content           String?
  contentRaw        String?       // JSON raw payload
  reasoning         String?       // Chain of thought (deprecated)
  thinkingText      String?       // Extended thinking text
  thinkingSignature String?       // Cryptographic signature
  thinkingTokens    Int?          // Tokens spent on thinking
  inputTokens       Int?
  outputTokens      Int?
  modelId           String?
  provider          String?
  createdAt         DateTime      @default(now())

  chat         LlmChat       @relation(fields: [chatId], references: [id], onDelete: Cascade)
  toolCalls    LlmToolCall[]
  requests     LlmRequest[]
}

model LlmToolCall {
  id               String     @id @default(uuid())
  messageId        String
  toolCallId       String     // ID from the provider
  name             String
  arguments        String     
  thought          String?    
  thoughtSignature String?    
  result           String?    
  createdAt        DateTime   @default(now())

  message      LlmMessage @relation(fields: [messageId], references: [id], onDelete: Cascade)

  @@unique([messageId, toolCallId])
}

model LlmRequest {
  id           String      @id @default(uuid())
  chatId       String
  messageId    String?     
  provider     String
  model        String
  statusCode   Int
  duration     Int         // milliseconds
  inputTokens  Int
  outputTokens Int
  cost         Float?
  createdAt    DateTime    @default(now())

  chat         LlmChat     @relation(fields: [chatId], references: [id], onDelete: Cascade)
  message      LlmMessage? @relation(fields: [messageId], references: [id], onDelete: Cascade)
}
```

### 2. Database Migrations

For production-grade systems, always use **Prisma Migrate** instead of `db push`. This ensures you have a versioned history of changes and prevents accidental data loss.

See the [Database Migration Guide](./migrations.md) for detailed instructions.

### 2. Manual Setup

Initialize the adapter with your `PrismaClient` and `NodeLLMCore` instance.

```typescript
import { PrismaClient } from "@prisma/client";
import { createLLM } from "@node-llm/core";
import { createChat, loadChat } from "@node-llm/orm/prisma";

const prisma = new PrismaClient();
const llm = createLLM();
```

---

## Basic Chat Operations

The ORM Chat implementation provides a fluent API that mirrors the core NodeLLM experience.

### Creating and Loading Chats

```typescript
// Start a new session with reasoning enabled by default
const chat = await createChat(prisma, llm, {
  model: "claude-3-7-sonnet",
  instructions: "You are a helpful assistant.",
  thinking: { budget: 16000 }
});

// Load an existing session from DB (automatically rehydrates history)
const savedChat = await loadChat(prisma, llm, "chat-uuid-123");
```

### Asking Questions

When you use `.ask()`, the persistence flow runs automatically.

```typescript
// This saves the user message, calls the API, and persists the response
const messageRecord = await chat.ask("What is the capital of France?");

// You can also pass thinking configuration directly per request
const advancedResp = await chat.ask("Solve this logical puzzle", {
  thinking: { budget: 32000 }
});

console.log(messageRecord.content); // "The capital of France is Paris."
console.log(messageRecord.inputTokens); // 12
```

---

## Streaming Responses

For real-time user experiences, use `askStream()`. The assistant message record is "finalized" once the stream completes.

```typescript
for await (const token of chat.askStream("Tell me a long story")) {
  process.stdout.write(token);
}

// History is now updated in the DB
const history = await chat.messages();
```

---

## Advanced Usage

### Custom Table Names

If you are integrating with an existing database schema, you can map the ORM to your custom table names:

```typescript
const tableNames = {
  chat: "AssistantChat",
  message: "AssistantMessage",
  toolCall: "AssistantToolCall",
  request: "AssistantRequest"
};

const chat = await createChat(prisma, llm, { 
  model: "gpt-4o",
  tableNames: tableNames 
});
```

### Using Tools

Tools are automatically tracked without additional configuration.

```typescript
import { WeatherTool } from "./tools/weather";

await chat.withTool(WeatherTool).ask("How is the weather in London?");

// Check your database: 
// The 'LlmToolCall' table will contain the 'get_weather' execution details.
```

---

## Error Handling

If an API call fails, NodeLLM follows a "clean rollback" strategy:
1. The pending Assistant message is **deleted**.
2. The initial User message is **deleted** (to prevent orphaned conversation turns).
3. The error is thrown for your application to handle.

This ensures your database doesn't fill up with "broken" chat turns.


<!-- END FILE: orm/prisma.md -->
----------------------------------------

<!-- FILE: advanced/agentic-workflows.md -->

# üìÑ advanced/agentic-workflows.md

---
layout: default
title: Agentic Workflows
nav_order: 2
parent: Advanced
permalink: /advanced/agentic-workflows
description: Compose LLM calls into intelligent workflows that route, research, and collaborate.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

This guide shows you how to compose NodeLLM primitives into more sophisticated patterns. Nothing here is magic‚Äîit's just tools calling other LLMs.

---

## The Core Idea

An "agent" in NodeLLM is just a tool that happens to call another LLM inside its `execute()` method. That's it. No special framework, no orchestration layer‚Äîjust composition.

```typescript
class MyAgent extends Tool {
  async execute(args) {
    // This tool IS an agent because it calls another LLM
    const response = await createLLM({ provider: "openai" })
      .chat("gpt-4o")
      .ask(args.query);
    return response.content;
  }
}
```

Everything below builds on this simple pattern.

---

## Model Routing

Route requests to the best model for the job. Useful when you want GPT-4 for code, Claude for creative writing, and Gemini for factual lookups.

```typescript
import { createLLM, Tool, z } from "@node-llm/core";

class SmartRouter extends Tool {
  name = "smart_router";
  description = "Routes to the best model for the task";
  schema = z.object({
    query: z.string().describe("The user's request")
  });

  async execute({ query }) {
    // Step 1: Classify the task with a fast model
    const taskType = await this.classify(query);

    // Step 2: Route to specialist
    const specialists = {
      code: { provider: "openai", model: "gpt-4o" },
      creative: { provider: "anthropic", model: "claude-sonnet-4-20250514" },
      factual: { provider: "gemini", model: "gemini-2.0-flash" }
    };

    const { provider, model } = specialists[taskType] || specialists.factual;
    const response = await createLLM({ provider }).chat(model).ask(query);
    return response.content;
  }

  private async classify(query: string) {
    const response = await createLLM({ provider: "openai" })
      .chat("gpt-4o-mini")
      .system("Classify as: code, creative, or factual. One word only.")
      .ask(query);
    return response.content.toLowerCase().trim();
  }
}
```

---

## RAG (Retrieval-Augmented Generation)

Combine vector search with LLM generation. NodeLLM provides `llm.embed()` for embeddings; you bring your own vector store.

This pattern is demonstrated in the [HR Chatbot Example](https://github.com/node-llm/node-llm/tree/main/examples/applications/hr-chatbot-rag).

```typescript
import { createLLM, Tool, z } from "@node-llm/core";
import { PrismaClient } from "@prisma/client";

const prisma = new PrismaClient();
const llm = createLLM({ provider: "openai" });

class KnowledgeSearch extends Tool {
  name = "search_knowledge";
  description = "Searches internal documents for relevant context";
  schema = z.object({
    query: z.string().describe("What to search for")
  });

  async execute({ query }) {
    // 1. Embed the query
    const embedding = await llm.embed(query);

    // 2. Vector search with pgvector
    const docs = await prisma.$queryRaw`
      SELECT title, content
      FROM documents
      ORDER BY embedding <-> ${embedding.vector}::vector
      LIMIT 3
    `;

    // 3. Format as context
    return docs.map(d => `[${d.title}]: ${d.content}`).join("\n\n");
  }
}

// Usage
const chat = llm
  .chat("gpt-4o")
  .system("Answer based on the search results. Cite sources.")
  .withTool(new KnowledgeSearch());

await chat.ask("What's our vacation policy?");
```

---

## Multi-Agent Collaboration

Tools can call other tools (via the coordinator), or tools can directly spawn their own LLM calls. Here's the "research then write" pattern:

```typescript
import { createLLM, Tool, z } from "@node-llm/core";

class Researcher extends Tool {
  name = "research";
  description = "Gathers facts about a topic";
  schema = z.object({ topic: z.string() });

  async execute({ topic }) {
    const response = await createLLM({ provider: "gemini" })
      .chat("gemini-2.0-flash")
      .system("List 5 key facts about the topic.")
      .ask(topic);
    return response.content;
  }
}

class Writer extends Tool {
  name = "write";
  description = "Writes content from research notes";
  schema = z.object({ notes: z.string() });

  async execute({ notes }) {
    const response = await createLLM({ provider: "anthropic" })
      .chat("claude-sonnet-4-20250514")
      .system("Write a concise article from these notes.")
      .ask(notes);
    return response.content;
  }
}

// Coordinator orchestrates the flow
const coordinator = createLLM({ provider: "openai" })
  .chat("gpt-4o")
  .system("First research the topic, then write an article.")
  .withTools([Researcher, Writer]);

await coordinator.ask("Write about TypeScript 5.4 features");
```

---

## Parallel Execution

Node.js is async-native. Use `Promise.all()` to run independent LLM calls concurrently.

```typescript
import { createLLM } from "@node-llm/core";

async function analyzeContent(text: string) {
  const llm = createLLM({ provider: "openai" });

  const [sentiment, summary, topics] = await Promise.all([
    llm.chat("gpt-4o-mini").ask(`Sentiment (positive/negative/neutral): ${text}`),
    llm.chat("gpt-4o-mini").ask(`One-sentence summary: ${text}`),
    llm.chat("gpt-4o-mini").ask(`Extract 3 topics: ${text}`)
  ]);

  return {
    sentiment: sentiment.content,
    summary: summary.content,
    topics: topics.content
  };
}
```

---

## Supervisor Pattern

Run specialized reviewers in parallel, then synthesize their findings:

```typescript
import { createLLM } from "@node-llm/core";

async function reviewCode(code: string) {
  // Parallel specialist reviews
  const [security, performance] = await Promise.all([
    createLLM({ provider: "anthropic" })
      .chat("claude-sonnet-4-20250514")
      .system("Security review. List vulnerabilities.")
      .ask(code),
    createLLM({ provider: "openai" })
      .chat("gpt-4o")
      .system("Performance review. List bottlenecks.")
      .ask(code)
  ]);

  // Synthesize
  return createLLM({ provider: "openai" })
    .chat("gpt-4o")
    .system("Combine these reviews into actionable recommendations.")
    .ask(`Security:\n${security.content}\n\nPerformance:\n${performance.content}`);
}
```

---

## Error Handling in Agents

Agents should handle failures gracefully. See the [Tools guide](../core-features/tools.html#error-handling--flow-control-) for details.

```typescript
class RiskyTool extends Tool {
  async execute(args) {
    // Recoverable: return error for LLM to retry
    if (!args.query) {
      return { error: "Query is required" };
    }

    // Fatal: stop the entire agent loop
    if (args.query.includes("DROP TABLE")) {
      throw new ToolError("Blocked dangerous query", this.name, { fatal: true });
    }

    return await this.doWork(args);
  }
}
```

---

## Next Steps

- [HR Chatbot RAG](https://github.com/node-llm/node-llm/tree/main/examples/applications/hr-chatbot-rag) ‚Äî Full RAG implementation with Prisma + pgvector
- [Brand Perception Checker](https://github.com/node-llm/node-llm/tree/main/examples/applications/brand-perception-checker) ‚Äî Multi-tool agent with web search
- [Tool Calling Guide](../core-features/tools.html) ‚Äî Deep dive on tool patterns and safety


<!-- END FILE: advanced/agentic-workflows.md -->
----------------------------------------

<!-- FILE: advanced/custom-providers.md -->

# üìÑ advanced/custom-providers.md

---
layout: default
title: Custom Providers
parent: Advanced
nav_order: 3
description: Extend NodeLLM with support for proprietary models, internal APIs, or legacy systems using our clean BaseProvider architecture.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

One of the core design goals of \`NodeLLM\` is provider-agnosticism. While we ship with support for major providers, you can easily add your own custom provider for internal APIs, proprietary models, or legacy systems.

The **recommended** way to create a custom provider is by extending the `BaseProvider` class.

## Why BaseProvider?

Extending `BaseProvider` instead of implementing the raw `Provider` interface gives you several advantages:

1.  **Safety**: It provides default implementations for features you might not support (like tools, embeddings, or vision), which will throw clean `UnsupportedFeatureError`s instead of failing with undefined errors.
2.  **Consistency**: It ensures your provider follows the project's internal mapping and logging standards.
3.  **Less Boilerplate**: You only need to implement the methods your service actually provides.

## Creating a Provider

To create a new provider, extend `BaseProvider` and implement the abstract methods.

> **Note**: The examples below use TypeScript. If you are using plain JavaScript (`.js` or `.mjs`), remember to remove access modifiers like `public` and `protected`.

```ts
import { NodeLLM, BaseProvider, ChatRequest, ChatResponse } from "@node-llm/core";

class MyCustomProvider extends BaseProvider {
  constructor(config: { apiKey: string; region: string }) {
    super();
    this.apiKey = config.apiKey;
    this.region = config.region;
  }

  // Required: A unique string identifier for your provider
  protected providerName() {
    return "my-custom-service";
  }

  // Required: The base URL for your API
  public apiBase() {
    return `https://api.${this.region}.my-service.com/v1`;
  }

  // Required: Any headers needed for authentication
  public headers() {
    return {
      Authorization: `Bearer ${this.apiKey}`,
      "Content-Type": "application/json"
    };
  }

  // Required: Define the main chat implementation
  async chat(request: ChatRequest): Promise<ChatResponse> {
    return {
      content: "Hello from my custom provider!",
      usage: { input_tokens: 5, output_tokens: 5, total_tokens: 10 }
    };
  }

  // Required: Provide a default model ID
  public defaultModel(feature?: string): string {
    return "my-model-v1";
  }
}
```

## Defining Capabilities

Capabilities tell NodeLLM what your provider is actually capable of. By default, `BaseProvider` assumes most advanced features are disabled. You can override these to opt-in to specific framework behaviors.

```ts
class MyCustomProvider extends BaseProvider {
  // ... rest of implementation

  public capabilities = {
    ...this.defaultCapabilities(), // Start with defaults

    // Enable support for OpenAI-style 'developer' roles
    supportsDeveloperRole: (modelId: string) => true,

    // Declare vision support
    supportsVision: (modelId: string) => modelId.includes("vision"),

    // Declare the context window size
    getContextWindow: (modelId: string) => 128000
  };
}
```

Notably, if `supportsDeveloperRole` is true, NodeLLM will automatically map isolated system instructions to the `developer` role. If false (the default), it will keep them as the standard `system` role.

## Registering Your Provider

Register your provider with `NodeLLM` during your application's initialization.

```ts
// 1. Register the factory function
NodeLLM.registerProvider("my-service", () => new MyCustomProvider());

// 2. Use it globally
const llm = createLLM({ provider: "my-service" });

const response = await llm.chat().ask("Hi!");
```

## Advanced Implementation

### Supporting Streaming

If your provider supports streaming, override the `stream` generator:

```ts
async *stream(request: ChatRequest) {
  // Simulated streaming
  const words = ["This", "is", "a", "stream"];
  for (const word of words) {
    yield { content: word + " " };
  }
}
```

### Handling Scoped Credentials

It's best to pull configuration from environment variables or use the injected configuration when the provider factory is called:

```ts
NodeLLM.registerProvider("internal-llm", (config) => {
  return new MyCustomProvider({
    apiKey: config?.["internalApiKey"] || process.env.INTERNAL_LLM_KEY,
    region: "us-east-1"
  });
});
```

### Handling Extra Fields

End-users might want to pass provider-specific parameters that aren't part of the standard `NodeLLM` API. These can be sent using `.withParams()` and will be available in the `request` object passed to your `chat` method.

```ts
async chat(request) {
  // Destructure to separate standard fields from custom ones
  const { model, messages, ...customParams } = request;

  if (customParams.internal_routing_id) {
    // Handle custom logic...
  }
}
```

### Handling Request Timeouts

NodeLLM passes `requestTimeout` (in milliseconds) through all request interfaces. Your custom provider should respect this timeout to ensure consistent security behavior across all providers.

Use the built-in `fetchWithTimeout` utility:

```ts
import { fetchWithTimeout } from "@node-llm/core/utils/fetch";

async chat(request: ChatRequest): Promise<ChatResponse> {
  const response = await fetchWithTimeout(
    `${this.apiBase()}/chat`,
    {
      method: "POST",
      headers: this.headers(),
      body: JSON.stringify({
        model: request.model,
        messages: request.messages
      })
    },
    request.requestTimeout  // Pass through the timeout
  );

  const json = await response.json();
  return {
    content: json.response,
    usage: json.usage
  };
}
```

**Note**: The `requestTimeout` parameter is available in all provider methods:

- `chat(request)`, `stream(request)`, `paint(request)`, `transcribe(request)`, `moderate(request)`, `embed(request)`

## Custom Pricing

If your custom provider has associated costs, you can register them in the `PricingRegistry`. This allows `NodeLLM` to automatically calculate usage costs for your custom models.

```ts
import { PricingRegistry } from "@node-llm/core";

// Register pricing for your custom service
PricingRegistry.register("my-custom-service", "my-model-v1", {
  text_tokens: {
    standard: {
      input_per_million: 1.5,
      output_per_million: 4.5
    }
  }
});
```

For more details on managing costs, see the [Model Pricing](./pricing.md) guide.

## Deep Dive

- [Building a Custom Provider for Cohere on Oracle Cloud](https://www.eshaiju.com/blog/custom-nodellm-provider-oracle) ‚Äî A real-world example of extending NodeLLM for proprietary cloud gateways.

## Example Implementation

See the [Custom Provider Example](https://github.com/node-llm/node-llm/blob/main/examples/scripts/core/custom-provider.mjs) in the repository for a complete working implementation including error handling, streaming, and extra field support.


<!-- END FILE: advanced/custom-providers.md -->
----------------------------------------

<!-- FILE: advanced/custom_endpoints.md -->

# üìÑ advanced/custom_endpoints.md

---
layout: default
title: Custom Endpoints
parent: Advanced
nav_order: 4
description: Connect NodeLLM to Azure OpenAI, LiteLLM, Ollama, or any OpenAI-compatible API and use custom models outside the standard registry.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

`NodeLLM` is flexible enough to connect to any OpenAI-compatible service and use custom models.

---

## OpenAI-Compatible Endpoints

Connect to services like Azure OpenAI, LiteLLM, or Ollama by configuring the base URL.

### Generic Configuration

Set `OPENAI_API_BASE` to your custom endpoint:

```bash
# LiteLLM
export OPENAI_API_KEY="your-litellm-key"
export OPENAI_API_BASE="https://your-proxy.litellm.ai/v1"

# Ollama (Local)
export OPENAI_API_KEY="not-needed"
export OPENAI_API_BASE="http://localhost:11434/v1"
```

### Azure OpenAI

For Azure, point `OPENAI_API_BASE` to your specific deployment URL. The library correctly handles URL construction even with query parameters.

```bash
export OPENAI_API_KEY="your-azure-key"
# Include the full path to your deployment
export OPENAI_API_BASE="https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT?api-version=2024-08-01-preview"
```

Then, pass the `api-key` header manually when creating the chat instance:

```typescript
import { createLLM } from "@node-llm/core";

const llm = createLLM({ provider: "openai" });

const chat = llm.chat("gpt-4").withRequestOptions({
  headers: { "api-key": process.env.OPENAI_API_KEY }
});

const response = await chat.ask("Hello Azure!");
```

---

## Using Custom Models

If you use a model ID not in the built-in registry (e.g., custom Azure names or new models), use `assumeModelExists: true` to bypass validation.

```typescript
const chat = llm.chat("my-company-gpt-4", {
  assumeModelExists: true,
  // Provider is typically required if not already configured globally
  provider: "openai"
});

await chat.ask("Hello");
```

This flag is available on all major methods:

```typescript
// Embeddings
await NodeLLM.embed("text", {
  model: "custom-embedder",
  assumeModelExists: true
});

// Image Generation
await NodeLLM.paint("prompt", {
  model: "custom-dalle",
  assumeModelExists: true
});
```

**Note:** When using this flag, strict capability checks (e.g., whether a model supports vision) are skipped. You are responsible for ensuring the model supports the requested features.


<!-- END FILE: advanced/custom_endpoints.md -->
----------------------------------------

<!-- FILE: advanced/debugging.md -->

# üìÑ advanced/debugging.md

---
layout: default
title: Debugging & Logging
parent: Advanced
nav_order: 3
description: Peek under the hood and inspect raw API requests, responses, and model alias resolution to troubleshoot your AI workflows.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

When building LLM applications, understanding what's happening "under the hood" is critical. \`NodeLLM\` provides mechanisms to inspect raw requests and responses.

## Debug Mode

You can enable detailed debug logging in two ways:

### Programmatic Configuration (Recommended)

```ts
import { createLLM } from "@node-llm/core";

const llm = createLLM({ debug: true });
```

This will print the raw HTTP requests and responses for **all API calls** across **every feature and provider**.

### Environment Variable

```bash
export NODELLM_DEBUG=true
node my-app.js
```

### Scoped Debug Mode

You can also enable debug mode for specific provider instances:

```ts
const debugAnthropic = NodeLLM.withProvider("anthropic", { debug: true });
```

**Output Example:**

```text
[NodeLLM] [OpenAI] Request: POST https://api.openai.com/v1/chat/completions
{
  "model": "gpt-4o",
  "messages": [...],
  "tools": [...]
}
[NodeLLM] [OpenAI] Response: 200 OK
{
  "id": "chatcmpl-123",
  "choices": [...],
  "usage": {...}
}
```

### Coverage

Debug logging works for:

- **Chat** (regular and streaming)
- **Image Generation** (OpenAI, Gemini)
- **Embeddings** (OpenAI, Gemini, Ollama)
- **Transcription** (OpenAI, Gemini)
- **Moderation** (OpenAI)
- **Model Alias Resolution** (all providers)
- **All Providers** (OpenAI, Anthropic, Gemini, DeepSeek)

The logs include:

- HTTP method and full URL
- Request body (JSON formatted)
- Response status code and status text
- Response body (JSON formatted)
- Model alias resolution (when using aliases)

### Model Alias Resolution

When debug mode is enabled, you'll see logs showing how model aliases are resolved:

```text
[NodeLLM Debug] Resolved model alias 'claude-3-5-haiku' ‚Üí 'claude-3-5-haiku-20241022' for provider 'anthropic'
[NodeLLM Debug] No alias mapping found for 'custom-model' with provider 'anthropic', using as-is
```

This is particularly helpful when debugging 404 errors, as it shows the actual model ID being sent to the API.

## Lifecycle Handlers

For programmatic observability (e.g., sending logs to Datadog or Sentry), use the [Chat Event Handlers](/core-features/chat.html#lifecycle-events).

```ts
chat
  .onNewMessage(() => logger.info("Chat started"))
  .onEndMessage((res) => logger.info("Chat finished", { tokens: res.total_tokens }));
```


<!-- END FILE: advanced/debugging.md -->
----------------------------------------

<!-- FILE: advanced/error-handling.md -->

# üìÑ advanced/error-handling.md

---
layout: default
title: Error Handling
parent: Advanced
nav_order: 3
description: Build resilient AI applications with NodeLLM's descriptive error hierarchy and unified error reporting across all providers.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---
NodeLLM provides a descriptive exception hierarchy to help you handle failures gracefully. All errors inherit from the base `LLMError` class.

## Error Hierarchy

All errors raised by NodeLLM inherit from `LLMError`. Specific errors map to HTTP status codes or library-specific issues:

```
LLMError                        # Base error class
‚îú‚îÄ‚îÄ APIError                    # Base for all provider API issues
‚îÇ   ‚îú‚îÄ‚îÄ BadRequestError         # 400: Invalid request parameters
‚îÇ   ‚îú‚îÄ‚îÄ UnauthorizedError       # 401: Invalid or missing API key
‚îÇ   ‚îú‚îÄ‚îÄ PaymentRequiredError    # 402: Billing issues
‚îÇ   ‚îú‚îÄ‚îÄ ForbiddenError          # 403: Permission denied
‚îÇ   ‚îú‚îÄ‚îÄ RateLimitError          # 429: Rate limit exceeded
‚îÇ   ‚îú‚îÄ‚îÄ ServerError             # 500+: Provider server error
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ServiceUnavailableError  # 502/503/529: Overloaded
‚îÇ   ‚îî‚îÄ‚îÄ AuthenticationError     # 401/403 (deprecated, use specific classes)
‚îú‚îÄ‚îÄ ConfigurationError          # Missing API key or invalid config
‚îú‚îÄ‚îÄ NotFoundError               # Model or provider not found
‚îú‚îÄ‚îÄ CapabilityError             # Model doesn't support feature (e.g. vision)
‚îú‚îÄ‚îÄ ToolError                   # Tool execution failed (has `fatal` property)
‚îú‚îÄ‚îÄ ProviderNotConfiguredError  # No provider set
‚îú‚îÄ‚îÄ UnsupportedFeatureError     # Provider doesn't support feature
‚îî‚îÄ‚îÄ ModelCapabilityError        # Model doesn't support capability
```

---

## Basic Error Handling

Catch the base `LLMError` for generic handling:

```typescript
import { LLMError, ConfigurationError } from "@node-llm/core";

try {
  const response = await chat.ask("Hello");
} catch (error) {
  if (error instanceof ConfigurationError) {
    console.error("Check your API key configuration");
  } else if (error instanceof LLMError) {
    console.error("AI error:", error.message);
  } else {
    throw error;
  }
}
```

---

## Handling Specific Errors

For granular control, catch specific error classes:

```typescript
import {
  UnauthorizedError,
  PaymentRequiredError,
  ForbiddenError,
  RateLimitError,
  ServerError,
  CapabilityError
} from "@node-llm/core";

try {
  await chat.ask("Analyze this image", { files: ["image.png"] });
} catch (error) {
  if (error instanceof UnauthorizedError) {
    console.error("Invalid API key. Check your configuration.");
  } else if (error instanceof PaymentRequiredError) {
    console.error("Billing issue. Check your provider account.");
  } else if (error instanceof ForbiddenError) {
    console.error("Permission denied. Check API key scopes.");
  } else if (error instanceof RateLimitError) {
    console.warn("Rate limited. Waiting before retry...");
    await sleep(5000);
  } else if (error instanceof CapabilityError) {
    console.error("This model doesn't support images. Try gpt-4o.");
  } else if (error instanceof ServerError) {
    console.error("Provider is having issues. Try again later.");
  } else {
    throw error;
  }
}
```

---

## Accessing Response Details

`APIError` instances contain details about the failed request:

```typescript
import { APIError } from "@node-llm/core";

try {
  await chat.ask("Something that fails");
} catch (error) {
  if (error instanceof APIError) {
    console.log(`Status: ${error.status}`);       // e.g. 429
    console.log(`Provider: ${error.provider}`);   // e.g. "openai"
    console.log(`Model: ${error.model}`);         // e.g. "gpt-4o"
    console.log(`Body:`, error.body);             // Raw error response
  }
}
```

---

## Error Handling During Streaming

When streaming, errors can occur after some chunks have been received. NodeLLM will throw after the stream ends or is interrupted:

```typescript
let accumulated = "";

try {
  for await (const chunk of chat.stream("Tell me a long story")) {
    accumulated += chunk.content || "";
    process.stdout.write(chunk.content || "");
  }
} catch (error) {
  console.error("\nStream failed:", error.message);
  console.log("Partial content received:", accumulated);
}
```

Your loop will process chunks received before the error. Always handle partial content when streaming.

---

## Handling Errors Within Tools

When building tools, decide how errors should surface:

### Return Error to LLM (Recoverable)

If the LLM might fix the issue (e.g., bad parameters), return an error object:

```typescript
class WeatherTool extends Tool {
  async execute({ location }) {
    if (!location) {
      return { error: "Location is required. Please provide a city name." };
    }
    // ... call API
  }
}
```

### Throw Error (Fatal)

If the error is unrecoverable, throw it to stop the agent loop:

```typescript
import { ToolError } from "@node-llm/core";

class DatabaseTool extends Tool {
  async execute({ query }) {
    if (query.includes("DROP")) {
      throw new ToolError("Dangerous query blocked", "database", { fatal: true });
    }
    // ...
  }
}
```

See [Tool Error Handling](../core-features/tools.html#error-handling--flow-control-) for more patterns.

---

## Automatic Retries

NodeLLM automatically retries transient errors:

- **Retried**: RateLimitError (429), ServerError (500+), ServiceUnavailableError
- **Not retried**: BadRequestError (400), UnauthorizedError (401), ForbiddenError (403)

Configure retry behavior:

```typescript
const llm = createLLM({
  provider: "openai",
  maxRetries: 3  // Default: 3
});
```

---

## Debugging

Enable debug logging to see detailed request/response information:

```bash
export NODELLM_DEBUG=true
```

This logs API calls, headers, and responses (with sensitive data filtered).

---

## Best Practices

1. **Be Specific**: Catch specific error classes for tailored recovery logic.

2. **Log Context**: Include model, provider, and (safe) input data in logs.

3. **User Feedback**: Show friendly messages, not raw API errors.

4. **Fallbacks**: Consider trying a different model or returning cached data.

5. **Monitor**: Track error frequency in production to identify patterns.

---

## Next Steps

- [Tool Calling](../core-features/tools.html) ‚Äî Build tools with proper error handling
- [Streaming](../core-features/streaming.html) ‚Äî Handle streaming responses
- [Security](security.html) ‚Äî Protect your application with rate limits and guards


<!-- END FILE: advanced/error-handling.md -->
----------------------------------------

<!-- FILE: advanced/index.md -->

# üìÑ advanced/index.md

---
layout: default
title: Advanced
nav_order: 4
has_children: true
nav_fold: false
permalink: /advanced
description: Master NodeLLM with advanced concepts like custom providers, security policies, and parallel model execution.
back_to_top: false
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }


<!-- END FILE: advanced/index.md -->
----------------------------------------

<!-- FILE: advanced/multi_provider_parallel.md -->

# üìÑ advanced/multi_provider_parallel.md

---
layout: default
title: Parallel Execution
parent: Advanced
nav_order: 10
description: Learn how to safely run multiple LLM providers concurrently using NodeLLM‚Äôs scoped context system to avoid global state race conditions.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## The Problem

In previous versions, `NodeLLM` was a mutable singleton. Calling `NodeLLM.configure()` concurrently could lead to race conditions where one request would overwrite the configuration of another.

---

## The Solution

As of v1.6.0, `NodeLLM` is a **frozen, immutable instance**. It cannot be mutated at runtime. For parallel execution with different providers or configurations, you use **context branching** via `.withProvider()` or create independent instances via `createLLM()`.

---

## How To Use It

### Simple Parallel Calls

The most elegant way to run multiple providers is using `.withProvider()`. This creates a scoped, isolated instance for that specific call.

```javascript
import { NodeLLM } from "@node-llm/core";

const [score1, score2, score3] = await Promise.all([
  NodeLLM.withProvider("openai").chat("gpt-4o").ask(prompt),
  NodeLLM.withProvider("anthropic").chat("claude-3-5-sonnet").ask(prompt),
  NodeLLM.withProvider("gemini").chat("gemini-2.0-flash").ask(prompt)
]);
```

---

## Benefits

‚úÖ **Singleton Maintained**: No need to use `new NodeLLM()` unless you want to.  
‚úÖ **Race Condition Solved**: Each `.withProvider()` call creates an isolated context.  
‚úÖ **Clean Syntax**: Chaining `.withProvider().chat().ask()` is intuitive and elegant.  
‚úÖ **Automatic Key Sharing**: Scoped instances inherit the global API keys by default.

---

## Example

Check out the [Parallel Scoring Example](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/core/parallel-scoring.mjs) for a working demonstration.


<!-- END FILE: advanced/multi_provider_parallel.md -->
----------------------------------------

<!-- FILE: advanced/pricing.md -->

# üìÑ advanced/pricing.md

---
layout: default
title: Model Pricing
parent: Advanced
nav_order: 4
description: Learn how to manage, override, and fetch LLM pricing data in NodeLLM.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

NodeLLM comes with built-in pricing data for over 9,000 models, updated weekly from [models.dev](https://models.dev). However, you may need to override these prices for custom contracts, handle new models before they enter our registry, or manage pricing for local/custom providers.

---

## The Pricing Registry

All pricing logic is managed by the `PricingRegistry`. This registry uses a tiered lookup strategy to determine the cost of a model:

1.  **Runtime Overrides**: Manual registrations or remote updates.
2.  **Expert Patterns**: Hardcoded library patterns for specific model families (e.g., Claude 3.7 reasoning).
3.  **Static Registry**: The default values from our weekly-updated `models.ts`.

### Runtime Overrides

You can manually register pricing for any model at runtime. This is particularly useful for custom providers or private deployments.

```ts
import { PricingRegistry } from "@node-llm/core";

PricingRegistry.register("mistral", "mistral-large-latest", {
  text_tokens: {
    standard: {
      input_per_million: 2.0,
      output_per_million: 6.0
    }
  }
});
```

### Remote Updates

For dynamic pricing management without code changes, you can fetch updates from a remote JSON endpoint.

```ts
await PricingRegistry.fetchUpdates("https://api.yourcompany.com/llm-pricing.json");
```

The JSON format should match:
```json
{
  "models": {
    "openai/gpt-5": {
      "text_tokens": {
        "standard": { "input_per_million": 1.0, "output_per_million": 5.0 }
      }
    }
  }
}
```

---

## Custom Providers

Custom providers (e.g., local instances of LLMs or internal proxies) can define their own pricing logic.

### Registering the Model

First, ensure the model exists in the `ModelRegistry` so NodeLLM knows its context window and capabilities.

```ts
import { ModelRegistry } from "@node-llm/core";

ModelRegistry.save({
  id: "local-llama",
  name: "Local Llama 3",
  provider: "local",
  context_window: 8192,
  capabilities: ["chat", "streaming"],
  modalities: { input: ["text"], output: ["text"] }
});
```

### Registering the Price

Then, assign it a price in the `PricingRegistry`.

```ts
import { PricingRegistry } from "@node-llm/core";

PricingRegistry.register("local", "local-llama", {
  text_tokens: {
    standard: {
      input_per_million: 0.0, // Free local model
      output_per_million: 0.0
    }
  }
});
```

---

## Cost Calculation

NodeLLM automatically calculates costs when a `usage` object is returned by a provider. You can also perform manual calculations using the registry:

```ts
import { ModelRegistry } from "@node-llm/core";

const usage = {
  input_tokens: 1000,
  output_tokens: 500,
  total_tokens: 1500
};

const costInfo = ModelRegistry.calculateCost(usage, "gpt-4o", "openai");
console.log(costInfo.cost); // Total cost in USD
```

---

## Advanced: Reasoning & Batch Pricing

For models that support specialized features, you can define more granular pricing:

```ts
PricingRegistry.register("openai", "o1-preview", {
  text_tokens: {
    standard: {
      input_per_million: 15.0,
      output_per_million: 60.0,
      reasoning_output_per_million: 60.0, // Specific reasoning cost
      cached_input_per_million: 7.50     // Discounted cache read
    },
    batch: {
      input_per_million: 7.50,
      output_per_million: 30.0
    }
  }
});
```


<!-- END FILE: advanced/pricing.md -->
----------------------------------------

<!-- FILE: advanced/security.md -->

# üìÑ advanced/security.md

---
layout: default
title: Security & Compliance
parent: Advanced
nav_order: 1
permalink: /advanced/security
description: Learn how NodeLLM acts as an architectural security layer with context isolation, content filtering, human-in-the-loop tool execution, and resource limits.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

NodeLLM is built from the ground up to be an **architectural security layer**. In production AI applications, the LLM is often the most vulnerable component due to prompt injection, instruction drift, and potential PII leakage.

NodeLLM provides several "Zero-Config" and pluggable security features to mitigate these risks.

---

## üß± Smart Context Isolation

The most common vector for LLM vulnerabilities is **Instruction Injection**, where user input tricks the model into ignoring its system instructions.

NodeLLM solves this by maintaining a strict architectural boundary between **System Instructions** and **Conversation History**.

- **Isolation**: Instructions are stored separately from the user message stack. They are never interleaved in a way that allows a user to "close" a system block.
- **Priority**: When sending a payload to a provider, NodeLLM ensures instructions are placed in the most authoritative role available.
- **Drift Protection**: Even in long conversations with many turns, NodeLLM continuously re-asserts the system context as the primary authority.

---

## üõ°Ô∏è Content Policy Hooks <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.0+</span>

NodeLLM allows you to inject security and compliance policies at the **edge** of the request/response cycle using asynchronous hooks.

### `beforeRequest` (Input Guardrail)

Intercept messages before they reach the LLM. Use this for **PII Detection** and **Redaction**.

```ts
chat.beforeRequest(async (messages) => {
  for (const msg of messages) {
    if (typeof msg.content === "string") {
      msg.content = msg.content.replace(/\d{3}-\d{2}-\d{4}/g, "[REDACTED_SSN]");
    }
  }
  return messages;
});
```

### `afterResponse` (Output Guardrail)

Verify the LLM's output before it reaches your application logic. Use this for **Compliance Verification** or **Sensitive Data Masking**.

```ts
chat.afterResponse(async (response) => {
  if (response.content.includes("SECRET_API_KEY")) {
    return response.withContent("Error: Sensitive data detected in output.");
  }
});
```

---

## üîç Observability as Security <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.0+</span>

Security in AI is not just about blocking; it's about **Auditing**. NodeLLM provides high-fidelity hooks for monitoring the entire lifecycle of tool executions, which are often the most sensitive part of an AI agent.

- **`onToolCallStart`**: Audit exactly what parameters the LLM is trying to send to your internal functions.
- **`onToolCallEnd`**: Record the raw data returned from your systems to the LLM.
- **`onToolCallError`**: Track failed attempts or malicious inputs that caused tool crashes.

```ts
chat
  .onToolCallStart((call) => auditLog.info(`Tool ${call.function.name} requested`))
  .onToolCallError((call, err) => incidentResponse.trigger(`Tool failure: ${err.message}`));
```

---

## üö¶ Tool Execution Policies <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.0+</span>

For sensitive operations (like database writes or financial transactions), NodeLLM provides granular control over the tool execution lifecycle via `toolExecution` modes.

- **`auto`**: (Default) Tools are executed immediately as proposed by the LLM.
- **`confirm`**: Enables **Human-in-the-loop**. NodeLLM pauses before execution and awaits approval via the `onConfirmToolCall` hook.
- **`dry-run`**: Proposes the tool call structure but **never executes it**. Useful for UI previews or verification-only flows.

```ts
chat.withToolExecution("confirm").onConfirmToolCall(async (call) => {
  // Return true to execute, false to cancel
  return await userResponse.confirm(`Allow tool: ${call.function.name}?`);
});
```

**Security Benefits:**

- **Prevents Destructive Actions**: Stops the model from accidentally deleting data without oversight.
- **Human-in-the-loop**: Increases trust by ensuring critical business logic remains under human control.

---

## üõ°Ô∏è Loop Protection & Resource Limits <span style="background-color: #0d9488; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.5.0+</span>

NodeLLM provides **defense-in-depth** protection against resource exhaustion, runaway costs, and denial-of-service attacks through configurable execution limits.

### Request Timeout

Prevent hanging requests that could tie up resources or enable DoS attacks. By default, all requests timeout after **30 seconds**.

```ts
// Global configuration
const llm = createLLM({
  requestTimeout: 30000 // 30 seconds (default)
});

// Per-request override for long-running tasks
await chat.ask("Analyze this large dataset", {
  requestTimeout: 120000 // 2 minutes
});
```

**Security Benefits:**

- **DoS Protection**: Prevents malicious or buggy providers from hanging indefinitely
- **Resource Control**: Limits memory, connection, and thread pool consumption
- **Cost Control**: Prevents runaway requests from generating unexpected costs
- **Predictable SLAs**: Ensures applications have predictable response times

### Loop Guard (Tool Execution Limit)

Prevent infinite tool execution loops that could exhaust resources or rack up costs.

```ts
const llm = createLLM({
  maxToolCalls: 5 // Stop after 5 sequential tool execution turns (default)
});

// Override for complex workflows
await chat.ask("Deep research task", { maxToolCalls: 10 });
```

**Security Benefits:**

- **Cost Control**: Prevents infinite loops from generating unbounded API costs
- **Resource Protection**: Stops runaway tool executions from exhausting system resources

### Retry Limit

Prevent retry storms that could cascade through your system during provider outages.

```ts
const llm = createLLM({
  maxRetries: 2 // Retry failed requests twice (default)
});
```

**Security Benefits:**

- **Cascading Failure Prevention**: Stops retry storms during provider outages
- **Resource Protection**: Prevents excessive retries from exhausting connection pools

### Complete Security Configuration

Combine all limits for comprehensive protection:

```ts
const llm = createLLM({
  requestTimeout: 30000, // 30 second timeout
  maxRetries: 2, // Retry failed requests twice
  maxToolCalls: 5, // Limit tool execution loops
  maxTokens: 4096 // Limit output to 4K tokens
});
```

This creates a **defense-in-depth** strategy where multiple layers of protection work together to prevent resource exhaustion, cost overruns, and service disruptions.

**Security Summary:**

- **`requestTimeout`**: DoS protection, resource control, predictable SLAs
- **`maxRetries`**: Prevents cascading failures and retry storms
- **`maxToolCalls`**: Prevents infinite loops and runaway costs
- **`maxTokens`**: Prevents excessive output generation and cost overruns

---

## ‚ö° Smart Developer Role

Modern models (like OpenAI's **o1**, **o3**, and **GPT-4o**) have introduced a specialized `developer` role. This role has higher "Instruction Authority" than the standard `system` role.

NodeLLM **automatically detects** if a model supports this role. If it does, your system instructions are elevated to the `developer` role, making the model significantly more resistant to prompt injection and more likely to follow strict guidelines.

---

## üîê Privacy & Data Strategy

- **Stateless Architecture**: NodeLLM is a library, not a service. We do not store, log, or transmit your data to any third-party servers other than the providers you explicitly configure.
- **Local Sovereignty**: Since NodeLLM supports **Ollama**, you can run the entire stack (including security policies) on-premise without ever sending data over the internet.
- **Encapsulated History**: Conversation history is stored in-memory within the `Chat` instance and is only shared with the provider at the moment of a request.


<!-- END FILE: advanced/security.md -->
----------------------------------------

<!-- FILE: advanced/token_usage.md -->

# üìÑ advanced/token_usage.md

---
layout: default
title: Token Usage
parent: Advanced
nav_order: 5
description: Monitor costs and resource consumption by tracking input/output tokens and estimated spend for individual requests or entire chat sessions.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

Track tokens for individual turns or the entire conversation to monitor costs and usage.

## Per-Response Usage

Every response object contains usage metadata for that specific interaction.

const response = await chat.ask("Hello!");

// Standard Snake Case
console.log(response.input_tokens); 

// Modern Camel Case Alias <span style="background-color: #0d47a1; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.6.0</span>
console.log(response.inputTokens); 

// Full Metadata Object (Perfect for DB storage) <span style="background-color: #0d47a1; color: white; padding: 1px 6px; border-radius: 3px; font-size: 0.65em; font-weight: 600; vertical-align: middle;">v1.6.0</span>
console.log(response.meta); 
// => { usage: {...}, model: "...", provider: "...", reasoning: "..." }
```

## Session Totals

The `Chat` instance maintains a running total of usage for the life of that object.

```ts
// Access aggregated usage for the whole session
console.log(chat.totalUsage.total_tokens);
console.log(chat.totalUsage.cost);
```


<!-- END FILE: advanced/token_usage.md -->
----------------------------------------

<!-- FILE: examples.md -->

# üìÑ examples.md

---
layout: default
title: Examples
nav_order: 7
description: Explore a comprehensive collection of runnable examples demonstrating every feature from basic chat to advanced multi-agent security policies.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

A comprehensive list of runnable examples available in the [examples/](https://github.com/node-llm/node-llm/tree/main/examples) directory of the repository.

## üåü Showcase

| Example                                                                                                                                                 | Description                                                                                                                                                 |
| :------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`examples/applications/brand-perception-checker/`](https://github.com/node-llm/node-llm/tree/main/examples/applications/brand-perception-checker)      | **Brand Perception Auditor** ‚Äî A full-stack (Node+React) app demonstrating multi-provider orchestration, tool calling (Google SERP), and structured output. |
| [`examples/applications/hr-chatbot-rag/`](https://github.com/node-llm/node-llm/tree/main/examples/applications/hr-chatbot-rag)                        | **HR Chatbot RAG** ‚Äî A production Next.js chatbot featuring `@node-llm/orm`, streaming, and persistence.                                                    |
| [`examples/scripts/openai/core/support-agent.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/core/support-agent.mjs)       | **Real-world Travel Support AI Agent** using Context Isolation, Auto-executing Tools, and Structured Output.                                                |
| [`examples/scripts/openai/security/content-policy-hooks.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/security/content-policy-hooks.mjs) | **Content Policy & Security** using `beforeRequest` and `afterResponse` hooks for PII redaction.                                                            |
| [`examples/scripts/openai/security/tool-policies.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/security/tool-policies.mjs) | **Advanced Tool Security** using `confirm` and `dry-run` modes for human-in-the-loop auditing.                                                              |

## OpenAI Examples

| Example                                                                                                                                   | Description                         |
| :---------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------- |
| [`examples/scripts/openai/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/basic.mjs)                         | Basic chat with streaming           |
| [`examples/scripts/openai/chat/events.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/events.mjs)                       | Lifecycle hooks (onNewMessage, etc) |
| [`examples/scripts/openai/chat/tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/tools.mjs)                         | Automatic tool execution            |
| [`examples/scripts/openai/chat/tool-dsl.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/tool-dsl.mjs)                   | Class-based Tool DSL                |
| [`examples/scripts/openai/chat/structured.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/structured.mjs)               | Zod schema validation               |
| [`examples/scripts/openai/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/multimodal/vision.mjs)           | Image analysis via URL              |
| [`examples/scripts/openai/multimodal/files.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/multimodal/files.mjs)             | Analyzing local files               |
| [`examples/scripts/openai/images/generate.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/images/generate.mjs)               | DALL-E 3 Generation                 |
| [`examples/scripts/openai/safety/moderation.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/safety/moderation.mjs)           | Custom safety thresholds            |
| [`examples/scripts/openai/embeddings/create.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/embeddings/create.mjs)           | Creating text embeddings            |
| [`examples/scripts/openai/chat/usage.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/usage.mjs)                         | Token usage tracking                |
| [`examples/scripts/openai/chat/parallel-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/parallel-tools.mjs)       | Parallel tool execution             |
| [`examples/scripts/openai/chat/max-tokens.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/max-tokens.mjs)               | Controlling output length           |
| [`examples/scripts/openai/chat/streaming-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/streaming-tools.mjs)     | Tool use with streaming             |
| [`examples/scripts/openai/chat/instructions.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/instructions.mjs)           | System prompt instructions          |
| [`examples/scripts/openai/chat/reasoning.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/reasoning.mjs)                 | Reasoning capabilities (o1)         |
| [`examples/scripts/openai/chat/params.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/params.mjs)                       | Custom model parameters             |
| [`examples/scripts/openai/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/chat/streaming.mjs)                 | Advanced streaming examples         |
| [`examples/scripts/openai/discovery/models.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/discovery/models.mjs)             | Listing available models            |
| [`examples/scripts/openai/multimodal/transcribe.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/multimodal/transcribe.mjs)   | Audio transcription                 |
| [`examples/scripts/openai/multimodal/multi-image.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openai/multimodal/multi-image.mjs) | Multiple image analysis             |

### Gemini

| Example | Description |
| :--- | :--- |
| [`examples/scripts/gemini/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/basic.mjs) | Streaming chat with Gemini 1.5 |
| [`examples/scripts/gemini/chat/json_mode.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/json_mode.mjs) | Native JSON mode |
| [`examples/scripts/gemini/multimodal/video.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/video.mjs) | Analyzing video files |
| [`examples/scripts/gemini/multimodal/audio.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/audio.mjs) | Native audio understanding |
| [`examples/scripts/gemini/multimodal/files.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/files.mjs) | Multi-file context |
| [`examples/scripts/gemini/embeddings/create.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/embeddings/create.mjs) | Creating text embeddings |
| [`examples/scripts/gemini/chat/structured.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/structured.mjs) | Structured output with Zod |
| [`examples/scripts/gemini/chat/usage.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/usage.mjs) | Token usage tracking |
| [`examples/scripts/gemini/chat/parallel-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/parallel-tools.mjs) | Parallel tool execution |
| [`examples/scripts/gemini/chat/max-tokens.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/max-tokens.mjs) | Controlling output length |
| [`examples/scripts/gemini/chat/streaming-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/streaming-tools.mjs) | Tool use with streaming |
| [`examples/scripts/gemini/chat/instructions.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/instructions.mjs) | System prompt instructions |
| [`examples/scripts/gemini/chat/params.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/params.mjs) | Custom model parameters |
| [`examples/scripts/gemini/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/streaming.mjs) | Advanced streaming |
| [`examples/scripts/gemini/chat/tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/tools.mjs) | Tool execution |
| [`examples/scripts/gemini/chat/events.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/chat/events.mjs) | Chat lifecycle events |
| [`examples/scripts/gemini/images/generate.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/images/generate.mjs) | Imagen 3 Generation |
| [`examples/scripts/gemini/discovery/models.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/discovery/models.mjs) | Listing available models |
| [`examples/scripts/gemini/safety/moderation.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/safety/moderation.mjs) | Content safety settings |
| [`examples/scripts/gemini/multimodal/transcribe.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/transcribe.mjs) | Audio transcription |
| [`examples/scripts/gemini/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/vision.mjs) | Image analysis |
| [`examples/scripts/gemini/multimodal/multi-image.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/gemini/multimodal/multi-image.mjs) | Multiple image analysis |

### Anthropic

| Example | Description |
| :--- | :--- |
| [`examples/scripts/anthropic/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/basic.mjs) | Claude 3.5 Sonnet Chat |
| [`examples/scripts/anthropic/chat/tool_use.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/tool_use.mjs) | Tool calling with Claude |
| [`examples/scripts/anthropic/multimodal/pdf.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/multimodal/pdf.mjs) | Native PDF analysis |
| [`examples/scripts/anthropic/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/multimodal/vision.mjs) | Image understanding |
| [`examples/scripts/anthropic/embeddings/create.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/embeddings/create.mjs) | Creating embeddings (Voyage AI) |
| [`examples/scripts/anthropic/chat/structured.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/structured.mjs) | Structured output (Tool use) |
| [`examples/scripts/anthropic/chat/usage.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/usage.mjs) | Token usage tracking |
| [`examples/scripts/anthropic/chat/parallel-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/parallel-tools.mjs) | Parallel tool execution |
| [`examples/scripts/anthropic/chat/max-tokens.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/max-tokens.mjs) | Controlling output length |
| [`examples/scripts/anthropic/chat/streaming-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/streaming-tools.mjs) | Tool use with streaming |
| [`examples/scripts/anthropic/chat/instructions.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/instructions.mjs) | System instructions |
| [`examples/scripts/anthropic/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/streaming.mjs) | Streaming chat |
| [`examples/scripts/anthropic/chat/events.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/chat/events.mjs) | Lifecycle events |
| [`examples/scripts/anthropic/images/generate.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/images/generate.mjs) | Image generation |
| [`examples/scripts/anthropic/discovery/models.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/discovery/models.mjs) | Listing models |
| [`examples/scripts/anthropic/safety/moderation.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/safety/moderation.mjs) | Content moderation |
| [`examples/scripts/anthropic/multimodal/transcribe.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/multimodal/transcribe.mjs) | Audio transcription |
| [`examples/scripts/anthropic/multimodal/multi-image.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/multimodal/multi-image.mjs) | Multiple image analysis |
| [`examples/scripts/anthropic/multimodal/files.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/anthropic/multimodal/files.mjs) | Multi-file context |

### Ollama Examples

| Example | Description |
| :--- | :--- |
| [`examples/scripts/ollama/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/chat/basic.mjs) | Local model chat |
| [`examples/scripts/ollama/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/chat/streaming.mjs) | Streaming local inference |
| [`examples/scripts/ollama/chat/tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/chat/tools.mjs) | Function calling with Llama 3.1 |
| [`examples/scripts/ollama/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/multimodal/vision.mjs) | Multi-modal local analysis |
| [`examples/scripts/ollama/embeddings/similarity.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/embeddings/similarity.mjs) | Vector similarity search |
| [`examples/scripts/ollama/discovery/list.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/ollama/discovery/list.mjs) | Inspecting local model library |

### DeepSeek Examples

| Example | Description |
| :--- | :--- |
| [`examples/scripts/deepseek/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/basic.mjs) | Basic chat with DeepSeek |
| [`examples/scripts/deepseek/chat/reasoning.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/reasoning.mjs) | DeepSeek-R1 reasoning tracking |
| [`examples/scripts/deepseek/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/streaming.mjs) | Streaming chat responses |
| [`examples/scripts/deepseek/chat/tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/tools.mjs) | Function calling with DeepSeek |
| [`examples/scripts/deepseek/chat/structured.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/structured.mjs) | Structured JSON output |
| [`examples/scripts/deepseek/embeddings/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/embeddings/basic.mjs) | Generating embeddings |
| [`examples/scripts/deepseek/chat/usage.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/usage.mjs) | Token usage tracking |
| [`examples/scripts/deepseek/chat/max-tokens.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/max-tokens.mjs) | Controlling output length |
| [`examples/scripts/deepseek/chat/streaming-tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/streaming-tools.mjs) | Tool use with streaming |
| [`examples/scripts/deepseek/chat/instructions.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/instructions.mjs) | System prompt instructions |
| [`examples/scripts/deepseek/chat/params.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/params.mjs) | Custom model parameters |
| [`examples/scripts/deepseek/chat/events.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/chat/events.mjs) | Lifecycle hooks |
| [`examples/scripts/deepseek/images/generate.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/images/generate.mjs) | Image generation |
| [`examples/scripts/deepseek/discovery/models.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/discovery/models.mjs) | Listing models |
| [`examples/scripts/deepseek/safety/moderation.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/safety/moderation.mjs) | Content moderation |
| [`examples/scripts/deepseek/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/deepseek/multimodal/vision.mjs) | Vision (V3) |

### OpenRouter Examples

| Example | Description |
| :--- | :--- |
| [`examples/scripts/openrouter/chat/basic.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/chat/basic.mjs) | Multi-model chat gateway |
| [`examples/scripts/openrouter/chat/streaming.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/chat/streaming.mjs) | Unified streaming across 300+ models |
| [`examples/scripts/openrouter/chat/tools.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/chat/tools.mjs) | Cross-provider function calling |
| [`examples/scripts/openrouter/chat/reasoning.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/chat/reasoning.mjs) | Accessing DeepSeek & o1 reasoning |
| [`examples/scripts/openrouter/discovery/models.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/discovery/models.mjs) | Exploring the global model library |
| [`examples/scripts/openrouter/multimodal/vision.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/multimodal/vision.mjs) | Unified vision API for all models |
| [`examples/scripts/openrouter/embeddings/create.mjs`](https://github.com/node-llm/node-llm/blob/main/examples/scripts/openrouter/embeddings/create.mjs) | Aggregated embedding services |


<!-- END FILE: examples.md -->
----------------------------------------

<!-- FILE: index.md -->

# üìÑ index.md

---
layout: landing
title: Home
nav_exclude: true
permalink: /
---


<!-- END FILE: index.md -->
----------------------------------------

<!-- FILE: models/available_models.md -->

# üìÑ models/available_models.md

---
layout: default
title: Available Models
nav_order: 5
has_children: false
permalink: /available-models
description: Browse AI models from every major provider. Always up-to-date, automatically generated.
---

# {{ page.title }}
{: .no_toc }

{{ page.description }}
{: .fs-6 .fw-300 }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

_Model information enriched by [models.dev](https://models.dev)._

## Last Updated
{: .d-inline-block }

2026-01-24
{: .label .label-green }

---

## Models by Provider

### OpenAI (159)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `gpt-4.1` | 1.0M | 32.768k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `gpt-4.1` | 1.0M | 32.768k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `gpt-4.1-2025-04-14` | 1.0M | 32.768k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `gpt-4.1-mini` | 1.0M | 32.768k | In: $0.40, Out: $1.60, Cache: $0.10 |
| `gpt-4.1-mini` | 1.0M | 32.768k | In: $0.40, Out: $1.60, Cache: $0.10 |
| `gpt-4.1-mini-2025-04-14` | 1.0M | 32.768k | In: $0.40, Out: $1.60, Cache: $0.10 |
| `gpt-4.1-nano` | 1.0M | 32.768k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gpt-4.1-nano` | 1.0M | 32.768k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gpt-4.1-nano-2025-04-14` | 1.0M | 32.768k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gpt-5` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-2025-08-07` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-chat-latest` | 400k | 128k | In: $1.25, Out: $10.00 |
| `gpt-5-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-mini` | 400k | 128k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `gpt-5-mini` | 400k | 128k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `gpt-5-mini-2025-08-07` | 400k | 128k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `gpt-5-nano` | 400k | 128k | In: $0.05, Out: $0.40, Cache: $0.01 |
| `gpt-5-nano` | 400k | 128k | In: $0.05, Out: $0.40, Cache: $0.01 |
| `gpt-5-nano-2025-08-07` | 400k | 128k | In: $0.05, Out: $0.40, Cache: $0.01 |
| `gpt-5-pro` | 400k | 272k | In: $15.00, Out: $120.00 |
| `gpt-5-pro` | 400k | 272k | In: $15.00, Out: $120.00 |
| `gpt-5-pro-2025-10-06` | 400k | 272k | In: $15.00, Out: $120.00 |
| `gpt-5.1` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-2025-11-13` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-codex-max` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-codex-mini` | 400k | 128k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `gpt-5.1-codex-mini` | 400k | 128k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `gpt-5.2` | 400k | 128k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `gpt-5.2-codex` | 400k | 128k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `gpt-5.2-pro` | 400k | 128k | In: $21.00, Out: $168.00 |
| `codex-mini-latest` | 200k | 100k | In: $1.50, Out: $6.00, Cache: $0.38 |
| `codex-mini-latest` | 200k | 100k | In: $1.50, Out: $6.00, Cache: $0.38 |
| `o1` | 200k | 100k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `o1` | 200k | 100k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `o1-2024-12-17` | 200k | 100k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `o1-pro` | 200k | 100k | In: $150.00, Out: $600.00 |
| `o1-pro` | 200k | 100k | In: $150.00, Out: $600.00 |
| `o1-pro-2025-03-19` | 200k | 100k | In: $150.00, Out: $600.00 |
| `o3` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `o3` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `o3-2025-04-16` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `o3-deep-research` | 200k | 100k | In: $10.00, Out: $40.00, Cache: $2.50 |
| `o3-deep-research` | 200k | 100k | In: $10.00, Out: $40.00, Cache: $2.50 |
| `o3-deep-research-2025-06-26` | 200k | 100k | In: $10.00, Out: $40.00, Cache: $2.50 |
| `o3-mini` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o3-mini` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o3-mini-2025-01-31` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o3-pro` | 200k | 100k | In: $20.00, Out: $80.00 |
| `o3-pro` | 200k | 100k | In: $20.00, Out: $80.00 |
| `o3-pro-2025-06-10` | 200k | 100k | In: $20.00, Out: $80.00 |
| `o4-mini` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.28 |
| `o4-mini` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.28 |
| `o4-mini-2025-04-16` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.28 |
| `o4-mini-deep-research` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `o4-mini-deep-research` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `o4-mini-deep-research-2025-06-26` | 200k | 100k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `gpt-oss-120b` | 131.072k | 131.072k | - |
| `gpt-oss-20b` | 131.072k | 131.072k | - |
| `chatgpt-4o-latest` | 128k | 16.384k | In: $5.00, Out: $15.00 |
| `gpt-4-turbo` | 128k | 4.096k | In: $10.00, Out: $30.00 |
| `gpt-4-turbo` | 128k | 4.096k | In: $10.00, Out: $30.00 |
| `gpt-4-turbo-2024-04-09` | 128k | 4.096k | In: $10.00, Out: $30.00 |
| `gpt-4-turbo-preview` | 128k | 4.096k | In: $10.00, Out: $30.00 |
| `gpt-4.5-preview` | 128k | 16.384k | In: $75.00, Out: $150.00, Cache: $37.50 |
| `gpt-4.5-preview-2025-02-27` | 128k | 16.384k | In: $75.00, Out: $150.00, Cache: $37.50 |
| `gpt-4o` | 128k | 16.384k | In: $2.50, Out: $10.00, Cache: $1.25 |
| `gpt-4o` | 128k | 16.384k | In: $2.50, Out: $10.00, Cache: $1.25 |
| `gpt-4o-2023-01-01` | 128k | 16.384k | In: $2.50, Out: $10.00, Cache: $1.25 |
| `gpt-4o-2024-05-13` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-2024-05-13` | 128k | 4.096k | In: $5.00, Out: $15.00 |
| `gpt-4o-2024-08-06` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-2024-08-06` | 128k | 16.384k | In: $2.50, Out: $10.00, Cache: $1.25 |
| `gpt-4o-2024-11-20` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-2024-11-20` | 128k | 16.384k | In: $2.50, Out: $10.00, Cache: $1.25 |
| `gpt-4o-audio-preview` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-audio-preview-2024-10-01` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-audio-preview-2024-12-17` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-audio-preview-2025-06-03` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-mini` | 128k | 16.384k | In: $0.15, Out: $0.60, Cache: $0.07 |
| `gpt-4o-mini` | 128k | 16.384k | In: $0.15, Out: $0.60, Cache: $0.08 |
| `gpt-4o-mini-2024-07-18` | 128k | 16.384k | In: $0.15, Out: $0.60, Cache: $0.07 |
| `gpt-4o-mini-audio-preview` | 128k | 16.384k | In: $0.15, Out: $0.60 |
| `gpt-4o-mini-audio-preview-2024-12-17` | 128k | 16.384k | In: $0.15, Out: $0.60 |
| `gpt-4o-mini-realtime-preview-2024-12-17` | 128k | 4.096k | In: $0.60, Out: $2.40 |
| `gpt-4o-mini-search-preview` | 128k | 16.384k | In: $0.15, Out: $0.60 |
| `gpt-4o-mini-search-preview-2025-03-11` | 128k | 16.384k | In: $0.15, Out: $0.60 |
| `gpt-4o-realtime-preview-2024-10-01` | 128k | 4.096k | In: $5.00, Out: $20.00 |
| `gpt-4o-realtime-preview-2024-12-17` | 128k | 4.096k | In: $5.00, Out: $20.00 |
| `gpt-4o-realtime-preview-2025-06-03` | 128k | 4.096k | In: $5.00, Out: $20.00 |
| `gpt-4o-search-preview` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-4o-search-preview-2025-03-11` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-5-chat-latest` | 128k | 16.384k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-search-api` | 128k | 400k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5-search-api-2025-10-14` | 128k | 400k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-chat-latest` | 128k | 16.384k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.1-chat-latest` | 128k | 16.384k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gpt-5.2-chat-latest` | 128k | 16.384k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `gpt-audio` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-audio-2025-08-28` | 128k | 16.384k | In: $2.50, Out: $10.00 |
| `gpt-audio-mini` | 128k | 16.384k | In: $0.60, Out: $2.40 |
| `gpt-audio-mini-2025-10-06` | 128k | 16.384k | In: $0.60, Out: $2.40 |
| `o1-mini` | 128k | 65.536k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o1-mini` | 128k | 65.536k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o1-mini-2024-09-12` | 128k | 65.536k | In: $1.10, Out: $4.40, Cache: $0.55 |
| `o1-preview` | 128k | 32.768k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `o1-preview` | 128k | 32.768k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `o1-preview-2024-09-12` | 128k | 32.768k | In: $15.00, Out: $60.00, Cache: $7.50 |
| `gpt-4o-realtime-preview` | 32k | 4.096k | In: $5.00, Out: $20.00, Cache: $2.50 |
| `gpt-realtime` | 32k | 4.096k | In: $4.00, Out: $16.00, Cache: $0.50 |
| `gpt-realtime-2025-08-28` | 32k | 4.096k | In: $4.00, Out: $16.00, Cache: $0.50 |
| `gpt-realtime-mini` | 32k | 4.096k | In: $0.60, Out: $2.40, Cache: $0.06 |
| `gpt-realtime-mini-2025-10-06` | 32k | 4.096k | In: $0.60, Out: $2.40, Cache: $0.06 |
| `gpt-3.5-turbo` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-3.5-turbo` | 16.385k | 4.096k | In: $0.50, Out: $1.50, Cache: $1.25 |
| `gpt-3.5-turbo-0125` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-3.5-turbo-1106` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-3.5-turbo-16k` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-3.5-turbo-instruct` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-3.5-turbo-instruct-0914` | 16.385k | 4.096k | In: $0.50, Out: $1.50 |
| `gpt-4o-mini-realtime-preview` | 16k | 4.096k | In: $0.60, Out: $2.40, Cache: $0.30 |
| `gpt-4o-mini-transcribe` | 16k | 2k | In: $1.25, Out: $5.00 |
| `gpt-4o-transcribe` | 16k | 2k | In: $2.50, Out: $10.00 |
| `gpt-4o-transcribe-diarize` | 16k | 2k | In: $2.50, Out: $10.00 |
| `computer-use-preview` | 8.192k | 1.024k | In: $3.00, Out: $12.00 |
| `computer-use-preview-2025-03-11` | 8.192k | 1.024k | In: $3.00, Out: $12.00 |
| `gpt-4` | 8.192k | 8.192k | In: $30.00, Out: $60.00 |
| `gpt-4` | 8.192k | 8.192k | In: $30.00, Out: $60.00 |
| `gpt-4-0613` | 8.192k | 8.192k | In: $30.00, Out: $60.00 |
| `text-embedding-ada-002` | 8.192k | 1.536k | In: $0.10 |
| `text-embedding-3-large` | 8.191k | 3.072k | In: $0.13 |
| `text-embedding-3-small` | 8.191k | 1.536k | In: $0.02 |
| `gpt-4-0125-preview` | 4.096k | 16.384k | In: $0.50, Out: $1.50 |
| `gpt-4-1106-preview` | 4.096k | 16.384k | In: $0.50, Out: $1.50 |
| `gpt-4o-mini-tts` | 2k | - | In: $0.60, Out: $12.00 |
| `babbage-002` | - | 16.384k | In: $0.40, Out: $0.40 |
| `dall-e-2` | - | - | - |
| `dall-e-3` | - | - | - |
| `davinci-002` | - | 16.384k | In: $2.00, Out: $2.00 |
| `gpt-image-1` | - | - | In: $5.00, Out: $40.00, Cache: $1.25 |
| `gpt-image-1-mini` | - | - | In: $2.00, Out: $8.00, Cache: $0.20 |
| `omni-moderation-2024-09-26` | - | - | - |
| `omni-moderation-latest` | - | - | - |
| `sora-2` | - | - | In: $0.10 |
| `sora-2-pro` | - | - | - |
| `text-embedding-3-large` | - | - | In: $0.13 |
| `text-embedding-3-small` | - | - | In: $0.02 |
| `text-embedding-ada-002` | - | - | In: $0.10 |
| `text-moderation-latest` | - | 32.768k | - |
| `text-moderation-stable` | - | 32.768k | - |
| `tts-1` | - | - | Out: $15.00 |
| `tts-1-1106` | - | - | In: $15.00, Out: $15.00 |
| `tts-1-hd` | - | - | Out: $30.00 |
| `tts-1-hd-1106` | - | - | In: $30.00, Out: $30.00 |
| `whisper-1` | - | - | In: $0.01 |

### Anthropic (33)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `claude-3-5-haiku-20241022` | 200k | 8.192k | In: $0.80, Out: $4.00 |
| `claude-3-5-haiku-20241022` | 200k | 8.192k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `claude-3-5-haiku-latest` | 200k | 8.192k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `claude-3-5-sonnet-20240620` | 200k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-5-sonnet-20241022` | 200k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-7-sonnet-20250219` | 200k | 8.192k | In: $3.00, Out: $15.00 |
| `claude-3-7-sonnet-20250219` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-7-sonnet-latest` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-haiku-20240307` | 200k | 4.096k | In: $0.25, Out: $1.25 |
| `claude-3-haiku-20240307` | 200k | 4.096k | In: $0.25, Out: $1.25, Cache: $0.03 |
| `claude-3-opus-20240229` | 200k | 4.096k | In: $15.00, Out: $75.00 |
| `claude-3-opus-20240229` | 200k | 4.096k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `claude-3-sonnet-20240229` | 200k | 4.096k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-haiku-4-5` | 200k | 64k | In: $1.00, Out: $5.00 |
| `claude-haiku-4-5` | 200k | 64k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `claude-haiku-4-5-20251001` | 200k | 64k | In: $1.00, Out: $5.00 |
| `claude-haiku-4-5-20251001` | 200k | 64k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `claude-opus-4-0` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `claude-opus-4-1` | 200k | 32k | In: $15.00, Out: $75.00 |
| `claude-opus-4-1` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `claude-opus-4-1-20250805` | 200k | 32k | In: $15.00, Out: $75.00 |
| `claude-opus-4-1-20250805` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `claude-opus-4-20250514` | 200k | 4.096k | In: $3.00, Out: $15.00 |
| `claude-opus-4-20250514` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `claude-opus-4-5` | 200k | 64k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `claude-opus-4-5-20251101` | 200k | 64k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `claude-sonnet-4-0` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-sonnet-4-20250514` | 200k | 4.096k | In: $3.00, Out: $15.00 |
| `claude-sonnet-4-20250514` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-sonnet-4-5` | 200k | 64k | In: $3.00, Out: $15.00 |
| `claude-sonnet-4-5` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-sonnet-4-5-20250929` | 200k | 64k | In: $3.00, Out: $15.00 |
| `claude-sonnet-4-5-20250929` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |

### Gemini (96)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `gemini-2.0-flash` | 1.0M | 8.192k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash` | 1.0M | 8.192k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash` | 1.0M | 8.192k | In: $0.15, Out: $0.60, Cache: $0.03 |
| `gemini-2.0-flash-001` | 1.0M | 8.192k | In: $0.10, Out: $0.40 |
| `gemini-2.0-flash-exp` | 1.0M | 8.192k | In: $0.10, Out: $0.40 |
| `gemini-2.0-flash-exp-image-generation` | 1.0M | 8.192k | In: $0.10, Out: $0.40 |
| `gemini-2.0-flash-lite` | 1.0M | 8.192k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash-lite` | 1.0M | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-lite` | 1.0M | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-lite-001` | 1.0M | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-lite-preview` | 1.0M | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-lite-preview-02-05` | 1.0M | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-thinking-exp` | 1.0M | 65.536k | In: $0.10, Out: $0.40 |
| `gemini-2.0-flash-thinking-exp-01-21` | 1.0M | 65.536k | In: $0.10, Out: $0.40 |
| `gemini-2.0-flash-thinking-exp-1219` | 1.0M | 65.536k | In: $0.10, Out: $0.40 |
| `gemini-2.0-pro-exp` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.0-pro-exp-02-05` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.03 |
| `gemini-2.5-flash` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-flash` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-flash-lite` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.03 |
| `gemini-2.5-flash-lite` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-lite` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-lite-preview-06-17` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-lite-preview-09-2025` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-lite-preview-09-2025` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-lite-preview-09-2025` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-preview-04-17` | 1.0M | 65.536k | In: $0.15, Out: $0.60, Cache: $0.04 |
| `gemini-2.5-flash-preview-04-17` | 1.0M | 65.536k | In: $0.15, Out: $0.60, Cache: $0.04 |
| `gemini-2.5-flash-preview-05-20` | 1.0M | 65.536k | In: $0.15, Out: $0.60, Cache: $0.04 |
| `gemini-2.5-flash-preview-05-20` | 1.0M | 65.536k | In: $0.15, Out: $0.60, Cache: $0.04 |
| `gemini-2.5-flash-preview-09-2025` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-preview-09-2025` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-flash-preview-09-2025` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-pro` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `gemini-2.5-pro` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-2.5-pro` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-2.5-pro-preview-03-25` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-pro-preview-05-06` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-pro-preview-05-06` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-2.5-pro-preview-05-06` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-2.5-pro-preview-06-05` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-2.5-pro-preview-06-05` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-2.5-pro-preview-06-05` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `gemini-3-flash-preview` | 1.0M | 65.536k | In: $0.50, Out: $3.00, Cache: $0.05 |
| `gemini-3-flash-preview` | 1.0M | 65.536k | In: $0.50, Out: $3.00, Cache: $0.05 |
| `gemini-3-pro-preview` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-3-pro-preview` | 1.0M | 65.536k | In: $2.00, Out: $12.00, Cache: $0.20 |
| `gemini-exp-1206` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-flash-latest` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-flash-latest` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-flash-latest` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-flash-lite-latest` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-flash-lite-latest` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-flash-lite-latest` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-pro-latest` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-robotics-er-1.5-preview` | 1.0M | 65.536k | In: $0.07, Out: $0.30 |
| `learnlm-2.0-flash-experimental` | 1.0M | 32.768k | In: $0.07, Out: $0.30 |
| `gemini-1.5-flash` | 1.0M | 8.192k | In: $0.07, Out: $0.30, Cache: $0.02 |
| `gemini-1.5-flash-8b` | 1.0M | 8.192k | In: $0.04, Out: $0.15, Cache: $0.01 |
| `gemini-1.5-pro` | 1.0M | 8.192k | In: $1.25, Out: $5.00, Cache: $0.31 |
| `gemini-3-pro-preview` | 1.0M | 64k | In: $2.00, Out: $12.00, Cache: $0.20 |
| `zai-org/glm-4.7-maas` | 204.8k | 131.072k | In: $0.60, Out: $2.20 |
| `gemini-2.5-computer-use-preview-10-2025` | 131.072k | 65.536k | In: $0.07, Out: $0.30 |
| `gemini-live-2.5-flash-preview-native-audio` | 131.072k | 65.536k | In: $0.50, Out: $2.00 |
| `gemma-3-27b-it` | 131.072k | 8.192k | In: $0.07, Out: $0.30 |
| `openai/gpt-oss-120b-maas` | 131.072k | 32.768k | In: $0.09, Out: $0.36 |
| `openai/gpt-oss-20b-maas` | 131.072k | 32.768k | In: $0.07, Out: $0.25 |
| `gemini-live-2.5-flash` | 128k | 8k | In: $0.50, Out: $2.00 |
| `gemini-2.5-flash-lite-preview-06-17` | 65.536k | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-image` | 32.768k | 32.768k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-image` | 32.768k | 32.768k | In: $0.30, Out: $30.00, Cache: $0.07 |
| `gemini-2.5-flash-image-preview` | 32.768k | 32.768k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-image-preview` | 32.768k | 32.768k | In: $0.30, Out: $30.00, Cache: $0.07 |
| `gemma-3-12b-it` | 32.768k | 8.192k | In: $0.07, Out: $0.30 |
| `gemma-3-1b-it` | 32.768k | 8.192k | In: $0.07, Out: $0.30 |
| `gemma-3-4b-it` | 32.768k | 8.192k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-preview-tts` | 8.192k | 16.384k | In: $0.07, Out: $0.30 |
| `gemini-2.5-pro-preview-tts` | 8.192k | 16.384k | In: $0.07, Out: $0.30 |
| `gemini-embedding-exp` | 8.192k | 1 | In: $0.00, Out: $0.00 |
| `gemini-embedding-exp-03-07` | 8.192k | 1 | In: $0.00, Out: $0.00 |
| `gemma-3n-e2b-it` | 8.192k | 2.048k | In: $0.07, Out: $0.30 |
| `gemma-3n-e4b-it` | 8.192k | 2.048k | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash-preview-tts` | 8k | 16k | In: $0.50, Out: $10.00 |
| `gemini-2.5-pro-preview-tts` | 8k | 16k | In: $1.00, Out: $20.00 |
| `aqa` | 7.168k | 1.024k | - |
| `embedding-001` | 2.048k | 1 | - |
| `gemini-embedding-001` | 2.048k | 1 | - |
| `gemini-embedding-001` | 2.048k | 3.072k | In: $0.15 |
| `gemini-embedding-001` | 2.048k | 3.072k | In: $0.15 |
| `text-embedding-004` | 2.048k | 1 | - |
| `embedding-gecko-001` | 1.024k | 1 | - |
| `imagen-4.0-generate-001` | 480 | 8.192k | - |
| `imagen-4.0-generate-preview-06-06` | 480 | 8.192k | - |
| `imagen-4.0-ultra-generate-001` | 480 | 8.192k | - |
| `imagen-4.0-ultra-generate-preview-06-06` | 480 | 8.192k | - |

### DeepSeek (2)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `deepseek-chat` | 128k | 8.192k | In: $0.28, Out: $0.42, Cache: $0.03 |
| `deepseek-reasoner` | 128k | 128k | In: $0.28, Out: $0.42, Cache: $0.03 |

### OpenRouter (140)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `x-ai/grok-4-fast` | 2.0M | 30k | In: $0.20, Out: $0.50, Cache: $0.05 |
| `x-ai/grok-4.1-fast` | 2.0M | 30k | In: $0.20, Out: $0.50, Cache: $0.05 |
| `openrouter/sherlock-dash-alpha` | 1.8M | - | - |
| `openrouter/sherlock-think-alpha` | 1.8M | - | - |
| `google/gemini-3-pro-preview` | 1.1M | 66k | In: $2.00, Out: $12.00 |
| `google/gemini-2.0-flash-001` | 1.0M | 8.192k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `google/gemini-2.0-flash-exp:free` | 1.0M | 1.0M | - |
| `google/gemini-2.5-flash` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.04 |
| `google/gemini-2.5-flash-lite` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `google/gemini-2.5-flash-lite-preview-09-2025` | 1.0M | 65.536k | In: $0.10, Out: $0.40, Cache: $0.03 |
| `google/gemini-2.5-flash-preview-09-2025` | 1.0M | 65.536k | In: $0.30, Out: $2.50, Cache: $0.03 |
| `google/gemini-2.5-pro` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `google/gemini-2.5-pro-preview-05-06` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `google/gemini-2.5-pro-preview-06-05` | 1.0M | 65.536k | In: $1.25, Out: $10.00, Cache: $0.31 |
| `google/gemini-3-flash-preview` | 1.0M | 65.536k | In: $0.50, Out: $3.00, Cache: $0.05 |
| `openai/gpt-4.1` | 1.0M | 32.768k | In: $2.00, Out: $8.00, Cache: $0.50 |
| `openai/gpt-4.1-mini` | 1.0M | 32.768k | In: $0.40, Out: $1.60, Cache: $0.10 |
| `anthropic/claude-sonnet-4.5` | 1.0M | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `minimax/minimax-01` | 1.0M | 1.0M | In: $0.20, Out: $1.10 |
| `minimax/minimax-m1` | 1.0M | 40k | In: $0.40, Out: $2.20 |
| `openai/gpt-5` | 400k | 128k | In: $1.25, Out: $10.00 |
| `openai/gpt-5-chat` | 400k | 128k | In: $1.25, Out: $10.00 |
| `openai/gpt-5-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `openai/gpt-5-image` | 400k | 128k | In: $5.00, Out: $10.00, Cache: $1.25 |
| `openai/gpt-5-mini` | 400k | 128k | In: $0.25, Out: $2.00 |
| `openai/gpt-5-nano` | 400k | 128k | In: $0.05, Out: $0.40 |
| `openai/gpt-5-pro` | 400k | 272k | In: $15.00, Out: $120.00 |
| `openai/gpt-5.1` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `openai/gpt-5.1-codex` | 400k | 128k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `openai/gpt-5.1-codex-max` | 400k | 128k | In: $1.10, Out: $9.00, Cache: $0.11 |
| `openai/gpt-5.1-codex-mini` | 400k | 100k | In: $0.25, Out: $2.00, Cache: $0.03 |
| `openai/gpt-5.2` | 400k | 128k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `openai/gpt-5.2-codex` | 400k | 128k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `openai/gpt-5.2-pro` | 400k | 128k | In: $21.00, Out: $168.00 |
| `mistralai/devstral-2512` | 262.144k | 262.144k | In: $0.15, Out: $0.60 |
| `mistralai/devstral-2512:free` | 262.144k | 262.144k | - |
| `mistralai/mistral-medium-3.1` | 262.144k | 262.144k | In: $0.40, Out: $2.00 |
| `moonshotai/kimi-k2-0905` | 262.144k | 16.384k | In: $0.60, Out: $2.50 |
| `moonshotai/kimi-k2-0905:exacto` | 262.144k | 16.384k | In: $0.60, Out: $2.50 |
| `moonshotai/kimi-k2-thinking` | 262.144k | 262.144k | In: $0.60, Out: $2.50, Cache: $0.15 |
| `qwen/qwen3-235b-a22b-07-25` | 262.144k | 131.072k | In: $0.15, Out: $0.85 |
| `qwen/qwen3-235b-a22b-07-25:free` | 262.144k | 131.072k | - |
| `qwen/qwen3-235b-a22b-thinking-2507` | 262.144k | 81.92k | In: $0.08, Out: $0.31 |
| `qwen/qwen3-coder` | 262.144k | 66.536k | In: $0.30, Out: $1.20 |
| `qwen/qwen3-coder:free` | 262.144k | 66.536k | - |
| `qwen/qwen3-max` | 262.144k | 32.768k | In: $1.20, Out: $6.00 |
| `qwen/qwen3-next-80b-a3b-instruct` | 262.144k | 262.144k | In: $0.14, Out: $1.40 |
| `qwen/qwen3-next-80b-a3b-thinking` | 262.144k | 262.144k | In: $0.14, Out: $1.40 |
| `qwen/qwen3-30b-a3b-instruct-2507` | 262k | 262k | In: $0.20, Out: $0.80 |
| `qwen/qwen3-30b-a3b-thinking-2507` | 262k | 262k | In: $0.20, Out: $0.80 |
| `kwaipilot/kat-coder-pro:free` | 256k | 65.536k | - |
| `mistralai/codestral-2508` | 256k | 256k | In: $0.30, Out: $0.90 |
| `x-ai/grok-4` | 256k | 64k | In: $3.00, Out: $15.00, Cache: $0.75 |
| `x-ai/grok-code-fast-1` | 256k | 10k | In: $0.20, Out: $1.50, Cache: $0.02 |
| `minimax/minimax-m2.1` | 204.8k | 131.072k | In: $0.30, Out: $1.20 |
| `z-ai/glm-4.7` | 204.8k | 131.072k | In: $0.60, Out: $2.20, Cache: $0.11 |
| `anthropic/claude-3.5-haiku` | 200k | 8.192k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic/claude-3.7-sonnet` | 200k | 128k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-haiku-4.5` | 200k | 64k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic/claude-opus-4` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-opus-4.1` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-opus-4.5` | 200k | 32k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `anthropic/claude-sonnet-4` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `openai/o4-mini` | 200k | 100k | In: $1.10, Out: $4.40, Cache: $0.28 |
| `z-ai/glm-4.6` | 200k | 128k | In: $0.60, Out: $2.20, Cache: $0.11 |
| `z-ai/glm-4.6:exacto` | 200k | 128k | In: $0.60, Out: $1.90, Cache: $0.11 |
| `minimax/minimax-m2` | 196.6k | 118k | In: $0.28, Out: $1.15, Cache: $0.28 |
| `deepseek/deepseek-chat-v3.1` | 163.84k | 163.84k | In: $0.20, Out: $0.80 |
| `deepseek/deepseek-r1-0528:free` | 163.84k | 163.84k | - |
| `deepseek/deepseek-r1:free` | 163.84k | 163.84k | - |
| `deepseek/deepseek-v3-base:free` | 163.84k | 163.84k | - |
| `deepseek/deepseek-v3.2` | 163.84k | 65.536k | In: $0.28, Out: $0.40 |
| `deepseek/deepseek-v3.2-speciale` | 163.84k | 65.536k | In: $0.27, Out: $0.41 |
| `microsoft/mai-ds-r1:free` | 163.84k | 163.84k | - |
| `tngtech/deepseek-r1t2-chimera:free` | 163.84k | 163.84k | - |
| `qwen/qwen3-coder-30b-a3b-instruct` | 160k | 65.536k | In: $0.07, Out: $0.27 |
| `deepseek/deepseek-r1-0528-qwen3-8b:free` | 131.072k | 131.072k | - |
| `deepseek/deepseek-v3.1-terminus` | 131.072k | 65.536k | In: $0.27, Out: $1.00 |
| `deepseek/deepseek-v3.1-terminus:exacto` | 131.072k | 65.536k | In: $0.27, Out: $1.00 |
| `meta-llama/llama-3.2-11b-vision-instruct` | 131.072k | 8.192k | - |
| `mistralai/devstral-medium-2507` | 131.072k | 131.072k | In: $0.40, Out: $2.00 |
| `mistralai/devstral-small-2507` | 131.072k | 131.072k | In: $0.10, Out: $0.30 |
| `mistralai/mistral-medium-3` | 131.072k | 131.072k | In: $0.40, Out: $2.00 |
| `mistralai/mistral-nemo:free` | 131.072k | 131.072k | - |
| `moonshotai/kimi-dev-72b:free` | 131.072k | 131.072k | - |
| `moonshotai/kimi-k2` | 131.072k | 32.768k | In: $0.55, Out: $2.20 |
| `nousresearch/deephermes-3-llama-3-8b-preview` | 131.072k | 8.192k | - |
| `nousresearch/hermes-4-405b` | 131.072k | 131.072k | In: $1.00, Out: $3.00 |
| `nousresearch/hermes-4-70b` | 131.072k | 131.072k | In: $0.13, Out: $0.40 |
| `nvidia/nemotron-nano-9b-v2` | 131.072k | 131.072k | In: $0.04, Out: $0.16 |
| `openai/gpt-oss-120b` | 131.072k | 32.768k | In: $0.07, Out: $0.28 |
| `openai/gpt-oss-120b:exacto` | 131.072k | 32.768k | In: $0.05, Out: $0.24 |
| `openai/gpt-oss-20b` | 131.072k | 32.768k | In: $0.05, Out: $0.20 |
| `openai/gpt-oss-safeguard-20b` | 131.072k | 65.536k | In: $0.07, Out: $0.30 |
| `qwen/qwen3-235b-a22b:free` | 131.072k | 131.072k | - |
| `qwen/qwen3-coder:exacto` | 131.072k | 32.768k | In: $0.38, Out: $1.53 |
| `x-ai/grok-3` | 131.072k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.75 |
| `x-ai/grok-3-beta` | 131.072k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.75 |
| `x-ai/grok-3-mini` | 131.072k | 8.192k | In: $0.30, Out: $0.50, Cache: $0.07 |
| `x-ai/grok-3-mini-beta` | 131.072k | 8.192k | In: $0.30, Out: $0.50, Cache: $0.07 |
| `mistralai/devstral-small-2505` | 128k | 128k | In: $0.06, Out: $0.12 |
| `mistralai/mistral-small-3.1-24b-instruct` | 128k | 8.192k | - |
| `openai/gpt-4o-mini` | 128k | 16.384k | In: $0.15, Out: $0.60, Cache: $0.08 |
| `openai/gpt-5.1-chat` | 128k | 16.384k | In: $1.25, Out: $10.00, Cache: $0.13 |
| `openai/gpt-5.2-chat-latest` | 128k | 16.384k | In: $1.75, Out: $14.00, Cache: $0.17 |
| `qwen/qwen3-coder-flash` | 128k | 66.536k | In: $0.30, Out: $1.50 |
| `z-ai/glm-4.5` | 128k | 96k | In: $0.60, Out: $2.20 |
| `z-ai/glm-4.5-air` | 128k | 96k | In: $0.20, Out: $1.10 |
| `z-ai/glm-4.5-air:free` | 128k | 96k | - |
| `google/gemma-3-12b-it` | 96k | 8.192k | - |
| `google/gemma-3-27b-it` | 96k | 8.192k | - |
| `mistralai/mistral-small-3.2-24b-instruct` | 96k | 8.192k | - |
| `mistralai/mistral-small-3.2-24b-instruct:free` | 96k | 96k | - |
| `meta-llama/llama-3.3-70b-instruct:free` | 65.536k | 65.536k | - |
| `deepseek/deepseek-r1-distill-qwen-14b` | 64k | 8.192k | - |
| `meta-llama/llama-4-scout:free` | 64k | 64k | - |
| `z-ai/glm-4.5v` | 64k | 16.384k | In: $0.60, Out: $1.80 |
| `qwen/qwen3-14b:free` | 40.96k | 40.96k | - |
| `qwen/qwen3-30b-a3b:free` | 40.96k | 40.96k | - |
| `qwen/qwen3-32b:free` | 40.96k | 40.96k | - |
| `qwen/qwen3-8b:free` | 40.96k | 40.96k | - |
| `moonshotai/kimi-k2:free` | 32.8k | 32.8k | - |
| `cognitivecomputations/dolphin3.0-mistral-24b` | 32.768k | 8.192k | - |
| `cognitivecomputations/dolphin3.0-r1-mistral-24b` | 32.768k | 8.192k | - |
| `featherless/qwerky-72b` | 32.768k | 8.192k | - |
| `mistralai/devstral-small-2505:free` | 32.768k | 32.768k | - |
| `mistralai/mistral-7b-instruct:free` | 32.768k | 32.768k | - |
| `qwen/qwen-2.5-coder-32b-instruct` | 32.768k | 8.192k | - |
| `qwen/qwen2.5-vl-72b-instruct` | 32.768k | 8.192k | - |
| `qwen/qwen2.5-vl-72b-instruct:free` | 32.768k | 32.768k | - |
| `qwen/qwq-32b:free` | 32.768k | 32.768k | - |
| `rekaai/reka-flash-3` | 32.768k | 8.192k | - |
| `sarvamai/sarvam-m:free` | 32.768k | 32.768k | - |
| `thudm/glm-z1-32b:free` | 32.768k | 32.768k | - |
| `deepseek/deepseek-chat-v3-0324` | 16.384k | 8.192k | - |
| `deepseek/deepseek-r1-distill-llama-70b` | 8.192k | 8.192k | - |
| `google/gemma-2-9b-it:free` | 8.192k | 8.192k | - |
| `google/gemma-3n-e4b-it` | 8.192k | 8.192k | - |
| `google/gemma-3n-e4b-it:free` | 8.192k | 8.192k | - |
| `qwen/qwen2.5-vl-32b-instruct:free` | 8.192k | 8.192k | - |

### Amazon Bedrock (67)

| Model | Context | Max Output | Pricing (per 1M tokens) |
| :--- | ---: | ---: | :--- |
| `meta.llama4-scout-17b-instruct-v1:0` | 3.5M | 16.384k | In: $0.17, Out: $0.66 |
| `amazon.nova-premier-v1:0` | 1.0M | 16.384k | In: $2.50, Out: $12.50 |
| `meta.llama4-maverick-17b-instruct-v1:0` | 1.0M | 16.384k | In: $0.24, Out: $0.97 |
| `amazon.nova-lite-v1:0` | 300k | 8.192k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-pro-v1:0` | 300k | 8.192k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `qwen.qwen3-235b-a22b-2507-v1:0` | 262.144k | 131.072k | In: $0.22, Out: $0.88 |
| `qwen.qwen3-coder-30b-a3b-v1:0` | 262.144k | 131.072k | In: $0.15, Out: $0.60 |
| `qwen.qwen3-next-80b-a3b` | 262k | 262k | In: $0.14, Out: $1.40 |
| `qwen.qwen3-vl-235b-a22b` | 262k | 262k | In: $0.30, Out: $1.50 |
| `ai21.jamba-1-5-large-v1:0` | 256k | 4.096k | In: $2.00, Out: $8.00 |
| `ai21.jamba-1-5-mini-v1:0` | 256k | 4.096k | In: $0.20, Out: $0.40 |
| `moonshot.kimi-k2-thinking` | 256k | 256k | In: $0.60, Out: $2.50 |
| `minimax.minimax-m2` | 204.608k | 128k | In: $0.30, Out: $1.20 |
| `google.gemma-3-27b-it` | 202.752k | 8.192k | In: $0.12, Out: $0.20 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | 200k | 8.192k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | 200k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | 200k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | 200k | 8.192k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | 200k | 4.096k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | 200k | 4.096k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | 200k | 4.096k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | 200k | 64k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | 200k | 32k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | 200k | 64k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `anthropic.claude-sonnet-4-20250514-v1:0` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-sonnet-4-5-20250929-v1:0` | 200k | 64k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-v2:1` | 200k | 4.096k | In: $8.00, Out: $24.00 |
| `global.anthropic.claude-opus-4-5-20251101-v1:0` | 200k | 64k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `deepseek.v3-v1:0` | 163.84k | 81.92k | In: $0.58, Out: $1.68 |
| `google.gemma-3-12b-it` | 131.072k | 8.192k | In: $0.05, Out: $0.10 |
| `qwen.qwen3-coder-480b-a35b-v1:0` | 131.072k | 65.536k | In: $0.22, Out: $1.80 |
| `meta.llama3-2-1b-instruct-v1:0` | 131k | 4.096k | In: $0.10, Out: $0.10 |
| `meta.llama3-2-3b-instruct-v1:0` | 131k | 4.096k | In: $0.15, Out: $0.15 |
| `amazon.nova-2-lite-v1:0` | 128k | 4.096k | In: $0.33, Out: $2.75 |
| `amazon.nova-micro-v1:0` | 128k | 8.192k | In: $0.04, Out: $0.14, Cache: $0.01 |
| `amazon.titan-text-express-v1` | 128k | 4.096k | In: $0.20, Out: $0.60 |
| `amazon.titan-text-express-v1:0:8k` | 128k | 4.096k | In: $0.20, Out: $0.60 |
| `cohere.command-r-plus-v1:0` | 128k | 4.096k | In: $3.00, Out: $15.00 |
| `cohere.command-r-v1:0` | 128k | 4.096k | In: $0.50, Out: $1.50 |
| `deepseek.r1-v1:0` | 128k | 32.768k | In: $1.35, Out: $5.40 |
| `google.gemma-3-4b-it` | 128k | 4.096k | In: $0.04, Out: $0.08 |
| `meta.llama3-1-70b-instruct-v1:0` | 128k | 4.096k | In: $0.72, Out: $0.72 |
| `meta.llama3-1-8b-instruct-v1:0` | 128k | 4.096k | In: $0.22, Out: $0.22 |
| `meta.llama3-2-11b-instruct-v1:0` | 128k | 4.096k | In: $0.16, Out: $0.16 |
| `meta.llama3-2-90b-instruct-v1:0` | 128k | 4.096k | In: $0.72, Out: $0.72 |
| `meta.llama3-3-70b-instruct-v1:0` | 128k | 4.096k | In: $0.72, Out: $0.72 |
| `mistral.ministral-3-14b-instruct` | 128k | 4.096k | In: $0.20, Out: $0.20 |
| `mistral.ministral-3-8b-instruct` | 128k | 4.096k | In: $0.15, Out: $0.15 |
| `mistral.mistral-large-2402-v1:0` | 128k | 4.096k | In: $0.50, Out: $1.50 |
| `mistral.voxtral-mini-3b-2507` | 128k | 4.096k | In: $0.04, Out: $0.04 |
| `nvidia.nemotron-nano-12b-v2` | 128k | 4.096k | In: $0.20, Out: $0.60 |
| `nvidia.nemotron-nano-9b-v2` | 128k | 4.096k | In: $0.06, Out: $0.23 |
| `openai.gpt-oss-120b-1:0` | 128k | 4.096k | In: $0.15, Out: $0.60 |
| `openai.gpt-oss-20b-1:0` | 128k | 4.096k | In: $0.07, Out: $0.30 |
| `openai.gpt-oss-safeguard-120b` | 128k | 4.096k | In: $0.15, Out: $0.60 |
| `openai.gpt-oss-safeguard-20b` | 128k | 4.096k | In: $0.07, Out: $0.20 |
| `mistral.mistral-7b-instruct-v0:2` | 127k | 127k | In: $0.11, Out: $0.11 |
| `anthropic.claude-instant-v1` | 100k | 4.096k | In: $0.80, Out: $2.40 |
| `anthropic.claude-v2` | 100k | 4.096k | In: $8.00, Out: $24.00 |
| `mistral.mixtral-8x7b-instruct-v0:1` | 32k | 32k | In: $0.70, Out: $0.70 |
| `mistral.voxtral-small-24b-2507` | 32k | 8.192k | In: $0.15, Out: $0.35 |
| `qwen.qwen3-32b-v1:0` | 16.384k | 16.384k | In: $0.15, Out: $0.60 |
| `meta.llama3-70b-instruct-v1:0` | 8.192k | 2.048k | In: $2.65, Out: $3.50 |
| `meta.llama3-8b-instruct-v1:0` | 8.192k | 2.048k | In: $0.30, Out: $0.60 |
| `cohere.command-light-text-v14` | 4.096k | 4.096k | In: $0.30, Out: $0.60 |
| `cohere.command-text-v14` | 4.096k | 4.096k | In: $1.50, Out: $2.00 |

---

## Models by Capability

### Function Calling (386)

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `ai21.jamba-1-5-large-v1:0` | bedrock | 256k | In: $2.00, Out: $8.00 |
| `ai21.jamba-1-5-mini-v1:0` | bedrock | 256k | In: $0.20, Out: $0.40 |
| `amazon.nova-2-lite-v1:0` | bedrock | 128k | In: $0.33, Out: $2.75 |
| `amazon.nova-lite-v1:0` | bedrock | 300k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-micro-v1:0` | bedrock | 128k | In: $0.04, Out: $0.14, Cache: $0.01 |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `amazon.nova-pro-v1:0` | bedrock | 300k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `amazon.titan-text-express-v1` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `amazon.titan-text-express-v1:0:8k` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | bedrock | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | bedrock | 200k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | bedrock | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |

### Vision (180)

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `amazon.nova-2-lite-v1:0` | bedrock | 128k | In: $0.33, Out: $2.75 |
| `amazon.nova-lite-v1:0` | bedrock | 300k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `amazon.nova-pro-v1:0` | bedrock | 300k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | bedrock | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | bedrock | 200k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | bedrock | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `anthropic.claude-sonnet-4-20250514-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-sonnet-4-5-20250929-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic/claude-3.5-haiku` | openrouter | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic/claude-3.7-sonnet` | openrouter | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-haiku-4.5` | openrouter | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |

### Reasoning (180)

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | bedrock | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `anthropic.claude-sonnet-4-20250514-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-sonnet-4-5-20250929-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic/claude-3.7-sonnet` | openrouter | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-haiku-4.5` | openrouter | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic/claude-opus-4` | openrouter | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-opus-4.1` | openrouter | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic/claude-opus-4.5` | openrouter | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |
| `anthropic/claude-sonnet-4` | openrouter | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic/claude-sonnet-4.5` | openrouter | 1.0M | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-7-sonnet-20250219` | anthropic | 200k | In: $3.00, Out: $15.00 |
| `claude-3-7-sonnet-20250219` | anthropic | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-3-7-sonnet-latest` | anthropic | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `claude-haiku-4-5` | anthropic | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `claude-haiku-4-5-20251001` | anthropic | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `claude-opus-4-0` | anthropic | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |

### Streaming (389)

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `ai21.jamba-1-5-large-v1:0` | bedrock | 256k | In: $2.00, Out: $8.00 |
| `ai21.jamba-1-5-mini-v1:0` | bedrock | 256k | In: $0.20, Out: $0.40 |
| `amazon.nova-2-lite-v1:0` | bedrock | 128k | In: $0.33, Out: $2.75 |
| `amazon.nova-lite-v1:0` | bedrock | 300k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-micro-v1:0` | bedrock | 128k | In: $0.04, Out: $0.14, Cache: $0.01 |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `amazon.nova-pro-v1:0` | bedrock | 300k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `amazon.titan-text-express-v1` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `amazon.titan-text-express-v1:0:8k` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | bedrock | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | bedrock | 200k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-instant-v1` | bedrock | 100k | In: $0.80, Out: $2.40 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |

### Structured Output (380)

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `ai21.jamba-1-5-large-v1:0` | bedrock | 256k | In: $2.00, Out: $8.00 |
| `ai21.jamba-1-5-mini-v1:0` | bedrock | 256k | In: $0.20, Out: $0.40 |
| `amazon.nova-2-lite-v1:0` | bedrock | 128k | In: $0.33, Out: $2.75 |
| `amazon.nova-lite-v1:0` | bedrock | 300k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-micro-v1:0` | bedrock | 128k | In: $0.04, Out: $0.14, Cache: $0.01 |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `amazon.nova-pro-v1:0` | bedrock | 300k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `amazon.titan-text-express-v1` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `amazon.titan-text-express-v1:0:8k` | bedrock | 128k | In: $0.20, Out: $0.60 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | bedrock | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | bedrock | 200k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | bedrock | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |

---

## Models by Modality

### Vision Models (287)

Models that can process images:

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `amazon.nova-2-lite-v1:0` | bedrock | 128k | In: $0.33, Out: $2.75 |
| `amazon.nova-lite-v1:0` | bedrock | 300k | In: $0.06, Out: $0.24, Cache: $0.01 |
| `amazon.nova-premier-v1:0` | bedrock | 1.0M | In: $2.50, Out: $12.50 |
| `amazon.nova-pro-v1:0` | bedrock | 300k | In: $0.80, Out: $3.20, Cache: $0.20 |
| `anthropic.claude-3-5-haiku-20241022-v1:0` | bedrock | 200k | In: $0.80, Out: $4.00, Cache: $0.08 |
| `anthropic.claude-3-5-sonnet-20240620-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-5-sonnet-20241022-v2:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-7-sonnet-20250219-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00, Cache: $0.30 |
| `anthropic.claude-3-haiku-20240307-v1:0` | bedrock | 200k | In: $0.25, Out: $1.25 |
| `anthropic.claude-3-opus-20240229-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00 |
| `anthropic.claude-3-sonnet-20240229-v1:0` | bedrock | 200k | In: $3.00, Out: $15.00 |
| `anthropic.claude-haiku-4-5-20251001-v1:0` | bedrock | 200k | In: $1.00, Out: $5.00, Cache: $0.10 |
| `anthropic.claude-opus-4-1-20250805-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-20250514-v1:0` | bedrock | 200k | In: $15.00, Out: $75.00, Cache: $1.50 |
| `anthropic.claude-opus-4-5-20251101-v1:0` | bedrock | 200k | In: $5.00, Out: $25.00, Cache: $0.50 |

### Audio Input Models (78)

Models that can process audio:

| Model | Provider | Context | Pricing |
| :--- | :--- | ---: | :--- |
| `gemini-1.5-flash` | gemini | 1.0M | In: $0.07, Out: $0.30, Cache: $0.02 |
| `gemini-1.5-flash-8b` | gemini | 1.0M | In: $0.04, Out: $0.15, Cache: $0.01 |
| `gemini-1.5-pro` | gemini | 1.0M | In: $1.25, Out: $5.00, Cache: $0.31 |
| `gemini-2.0-flash` | gemini | 1.0M | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash` | gemini | 1.0M | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash` | gemini | 1.0M | In: $0.15, Out: $0.60, Cache: $0.03 |
| `gemini-2.0-flash-lite` | gemini | 1.0M | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.0-flash-lite` | gemini | 1.0M | In: $0.07, Out: $0.30 |
| `gemini-2.0-flash-lite` | gemini | 1.0M | In: $0.07, Out: $0.30 |
| `gemini-2.5-flash` | gemini | 1.0M | In: $0.30, Out: $2.50, Cache: $0.03 |
| `gemini-2.5-flash` | gemini | 1.0M | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-flash` | gemini | 1.0M | In: $0.30, Out: $2.50, Cache: $0.07 |
| `gemini-2.5-flash-lite` | gemini | 1.0M | In: $0.30, Out: $2.50, Cache: $0.03 |
| `gemini-2.5-flash-lite` | gemini | 1.0M | In: $0.10, Out: $0.40, Cache: $0.03 |
| `gemini-2.5-flash-lite` | gemini | 1.0M | In: $0.10, Out: $0.40, Cache: $0.03 |

### Embedding Models (102)

Models that generate embeddings:

| Model | Provider | Dimensions | Pricing |
| :--- | :--- | ---: | :--- |
| `babbage-002` | openai | - | In: $0.40, Out: $0.40 |
| `chatgpt-4o-latest` | openai | - | In: $5.00, Out: $15.00 |
| `codex-mini-latest` | openai | - | In: $1.50, Out: $6.00, Cache: $0.38 |
| `computer-use-preview` | openai | - | In: $3.00, Out: $12.00 |
| `computer-use-preview-2025-03-11` | openai | - | In: $3.00, Out: $12.00 |
| `dall-e-2` | openai | - | - |
| `dall-e-3` | openai | - | - |
| `davinci-002` | openai | - | In: $2.00, Out: $2.00 |
| `embedding-001` | gemini | - | - |
| `embedding-gecko-001` | gemini | - | - |
| `gemini-embedding-001` | gemini | - | - |
| `gemini-embedding-exp` | gemini | - | In: $0.00, Out: $0.00 |
| `gemini-embedding-exp-03-07` | gemini | - | In: $0.00, Out: $0.00 |
| `gpt-3.5-turbo` | openai | - | In: $0.50, Out: $1.50 |
| `gpt-4` | openai | - | In: $30.00, Out: $60.00 |

---

## Programmatic Access

You can access this data programmatically using the registry:

```ts
import { NodeLLM } from "@node-llm/core";

// Get metadata for a specific model
const model = await NodeLLM.model("gpt-4o");

console.log(model.context_window); // 128000
console.log(model.pricing.text_tokens.standard.input_per_million); // 2.5
console.log(model.capabilities); // ["vision", "function_calling", ...]

// Get all models in the registry
const allModels = await NodeLLM.listModels();
```

---

## Finding Models

Use the registry to find models dynamically based on capabilities:

```ts
const allModels = await NodeLLM.listModels();

// Find a model that supports vision and tools
const visionModel = allModels.find(m => 
  m.capabilities.includes("vision") && m.capabilities.includes("function_calling")
);
```

---

## Model Aliases

`NodeLLM` uses aliases (defined strictly in `packages/core/src/aliases.ts`) for convenience, mapping common names to specific provider-specific versions. This allows you to use a generic name like `"gpt-4o"` or `"claude-3-5-sonnet"` and have it resolve to the correct ID for your configured provider.

### How It Works

Aliases abstract away the specific model ID strings required by different providers. For example, `claude-3-5-sonnet` might map to:

- **Anthropic**: `claude-3-5-sonnet-20241022`
- **OpenRouter**: `anthropic/claude-3.5-sonnet`

When you call a method like `NodeLLM.chat("claude-3-5-sonnet")`, `NodeLLM` checks the configured provider and automatically resolves the alias.

```ts
// Using Anthropic provider
const llm = createLLM({ provider: "anthropic" });
const chat = llm.chat("claude-3-5-sonnet"); 
// Resolves internally to "claude-3-5-sonnet-20241022" (or latest stable version)
```

### Provider-Specific Resolution

If an alias exists for multiple providers, the resolution depends entirely on the `provider` you have currently configured/passed.

```json
// Example aliases.ts structure
{
  "gemini-flash": {
    "gemini": "gemini-1.5-flash-001",
    "openrouter": "google/gemini-1.5-flash-001"
  }
}
```

This ensures your code remains portable across providers without changing the model string manually.

### Prioritization

`NodeLLM` prioritizes exact ID matches first (if you pass a specific ID like `"gpt-4-0613"`, it uses it). If no exact match or known ID is found, it attempts to resolve it as an alias.

### Programmatic Access

You can access the alias mappings programmatically for validation or UI purposes:

```ts
import { MODEL_ALIASES, resolveModelAlias } from "@node-llm/core";

// Check if an alias exists
const isValidAlias = "claude-3-5-haiku" in MODEL_ALIASES;

// Get all providers supporting an alias
const providers = Object.keys(MODEL_ALIASES["claude-3-5-haiku"]);
// => ["anthropic", "openrouter"]

// Resolve alias for a specific provider
const resolved = resolveModelAlias("claude-3-5-haiku", "anthropic");
// => "claude-3-5-haiku-20241022"

// List all available aliases
const allAliases = Object.keys(MODEL_ALIASES);

// Validate user input
function validateModel(input, provider) {
  if (input in MODEL_ALIASES) {
    if (MODEL_ALIASES[input][provider]) {
      return { valid: true, resolved: MODEL_ALIASES[input][provider] };
    }
    return { valid: false, reason: `Alias not supported for ${provider}` };
  }
  return { valid: true, resolved: input };
}
```

This is useful for:
- Building model selection UIs
- Validating user input before API calls
- Checking provider compatibility
- Debugging 404 errors

---

**Auto-generated by `npm run sync-models`** ‚Ä¢ Last updated: 2026-01-24


<!-- END FILE: models/available_models.md -->
----------------------------------------

<!-- FILE: roadmap.md -->

# üìÑ roadmap.md

---
layout: default
title: Roadmap
nav_order: 99
---

# üó∫Ô∏è Project Roadmap

NodeLLM is evolving to support more complex AI-native Node.js applications.

---

### ‚úÖ RECENTLY RELEASED
{: .no_toc }

- **[Extended Thinking](/core-features/reasoning)**: Unified interface for Claude 3.7, DeepSeek R1, and OpenAI o1/o3.
- **[Professional ORM Support](/orm/prisma)**: Database persistence for Prisma with automated history management and professional migration workflows.
- **Context Isolation 2.0**: Strict separation of system instructions and conversation turns for enterprise-grade safety.

---

## üöÄ Future Priorities

### üß† High-Level Orchestration
**Managed Chain-of-Thought Patterns.**

Beyond simple chat loops, we are building structured orchestration patterns for complex multi-step reasoning:
- **Planner/Executor Loops**: Automated sub-task decomposition.
- **Self-Correction Patterns**: Native support for LLM-based output validation and retry loops.

### üß™ Evaluation Framework
**Integration Testing for AI.**

Measuring the quality of non-deterministic LLM outputs is hard. We are exploring a lightweight evaluation toolkit to help developers:
- **Snapshot Testing**: Lock down expected behaviors.
- **Prompt Regression Detection**: Ensure new model versions don't break your specialized instructions.

### üìÇ Expanded Example Library

We learn by doing. We will double down on high-quality, full-stack reference implementations covering:

- **RAG Knowledge Base**: A verified pattern for "Chat with your Docs".
- **Voice Interface**: Real-time audio-in/audio-out.
- **Local-First Agent**: Zero-latency offline agents using Ollama + Llama 3.

---

## üõ°Ô∏è Ongoing

- **Security First**: Continued investment in Context Isolation, PII hooks, and adversarial defense.
- **Zero-Dependency Core**: Keeping the core library lightweight while moving heavy integrations to separate packages (e.g. `@node-llm/tools`).


<!-- END FILE: roadmap.md -->
----------------------------------------