{"0": {
    "doc": "Anthropic",
    "title": "Anthropic Provider",
    "content": "The Anthropic provider gives access to the Claude family of models, known for high-quality reasoning and coding capabilities. ",
    "url": "/providers/anthropic.html#anthropic-provider",
    
    "relUrl": "/providers/anthropic.html#anthropic-provider"
  },"1": {
    "doc": "Anthropic",
    "title": "Configuration",
    "content": "import { LLM } from \"@node-llm/core\"; LLM.configure({ provider: \"anthropic\", apiKey: process.env.ANTHROPIC_API_KEY, // Optional if set in env }); . ",
    "url": "/providers/anthropic.html#configuration",
    
    "relUrl": "/providers/anthropic.html#configuration"
  },"2": {
    "doc": "Anthropic",
    "title": "Specific Parameters",
    "content": "You can pass Anthropic-specific parameters or custom headers. const chat = LLM.chat(\"claude-3-5-sonnet-20241022\") .withParams({ top_k: 50, top_p: 0.9, // Custom headers if needed headers: { \"anthropic-beta\": \"max-tokens-3-5-sonnet-2024-07-15\" } }); . ",
    "url": "/providers/anthropic.html#specific-parameters",
    
    "relUrl": "/providers/anthropic.html#specific-parameters"
  },"3": {
    "doc": "Anthropic",
    "title": "Features",
    "content": ". | Models: claude-3-5-sonnet, claude-3-opus, claude-3-haiku. | Vision: Analyzes images. | PDF Support: Can read and analyze PDF documents natively. | Tools: Fully supported. | . ",
    "url": "/providers/anthropic.html#features",
    
    "relUrl": "/providers/anthropic.html#features"
  },"4": {
    "doc": "Anthropic",
    "title": "PDF Support",
    "content": "Anthropic supports sending PDF files as base64 encoded blocks, which node-llm handles automatically. await chat.ask(\"Summarize this document\", { files: [\"./report.pdf\"] }); . ",
    "url": "/providers/anthropic.html#pdf-support",
    
    "relUrl": "/providers/anthropic.html#pdf-support"
  },"5": {
    "doc": "Anthropic",
    "title": "Anthropic",
    "content": " ",
    "url": "/providers/anthropic.html",
    
    "relUrl": "/providers/anthropic.html"
  },"6": {
    "doc": "Audio Transcription",
    "title": "Audio Transcription",
    "content": "Convert audio files to text using models like OpenAI‚Äôs Whisper or Google‚Äôs Gemini. node-llm supports both raw transcription and multimodal chat analysis. ",
    "url": "/core_features/audio-transcription.html",
    
    "relUrl": "/core_features/audio-transcription.html"
  },"7": {
    "doc": "Audio Transcription",
    "title": "Basic Transcription",
    "content": "Use LLM.transcribe() for direct speech-to-text conversion. const text = await LLM.transcribe(\"meeting.mp3\", { model: \"whisper-1\" }); console.log(text.toString()); . ",
    "url": "/core_features/audio-transcription.html#basic-transcription",
    
    "relUrl": "/core_features/audio-transcription.html#basic-transcription"
  },"8": {
    "doc": "Audio Transcription",
    "title": "Advanced Options",
    "content": "Speed vs Accuracy . You can choose different models or parameters depending on your needs. await LLM.transcribe(\"audio.mp3\", { model: \"whisper-1\", language: \"en\", // ISO-639-1 code hint to improve accuracy prompt: \"ZyntriQix, API\" // Guide the model with domain-specific terms }); . Accessing Segments &amp; Timestamps . The transcribe method returns a Transcription object that contains more than just text. You can access detailed timing information if supported by the provider (e.g., using response_format: 'verbose_json' with OpenAI). const response = await LLM.transcribe(\"interview.mp3\", { params: { response_format: \"verbose_json\" } }); console.log(`Duration: ${response.duration}s`); for (const segment of response.segments) { console.log(`[${segment.start}s - ${segment.end}s]: ${segment.text}`); } . ",
    "url": "/core_features/audio-transcription.html#advanced-options",
    
    "relUrl": "/core_features/audio-transcription.html#advanced-options"
  },"9": {
    "doc": "Audio Transcription",
    "title": "Multimodal Chat vs. Transcription",
    "content": "There are two ways to work with audio: . | Transcription (LLM.transcribe): Best when you need the verbatim text. | Result: ‚ÄúHello everyone today we are‚Ä¶‚Äù | . | Multimodal Chat (chat.ask): Best when you need to analyze or summarize the audio directly, without seeing the raw text first. Supported by models like gemini-1.5-pro and gpt-4o. | . // Multimodal Chat Example const chat = LLM.chat(\"gemini-1.5-pro\"); await chat.ask(\"What is the main topic of this podcast?\", { files: [\"podcast.mp3\"] }); . ",
    "url": "/core_features/audio-transcription.html#multimodal-chat-vs-transcription",
    
    "relUrl": "/core_features/audio-transcription.html#multimodal-chat-vs-transcription"
  },"10": {
    "doc": "Audio Transcription",
    "title": "Error Handling",
    "content": "Audio files can be large and prone to timeouts. try { await LLM.transcribe(\"large-file.mp3\"); } catch (error) { console.error(\"Transcription failed:\", error.message); } . ",
    "url": "/core_features/audio-transcription.html#error-handling",
    
    "relUrl": "/core_features/audio-transcription.html#error-handling"
  },"11": {
    "doc": "Available Models",
    "title": "Available Models",
    "content": "node-llm includes a comprehensive registry of models with metadata for pricing, context windows, and capabilities. Below are the most popular models supported out of the box. ",
    "url": "/available-models",
    
    "relUrl": "/available-models"
  },"12": {
    "doc": "Available Models",
    "title": "OpenAI",
    "content": "| Model ID | Context | Cost (In/Out) | Notes | . | Flagship | ¬† | ¬† | ¬† | . | gpt-4o | 128k | $5 / $15 | High intelligence, native multimodal | . | gpt-4o-mini | 128k | $0.15 / $0.60 | Fast, cheap replacement for GPT-3.5 | . | Reasoning | ¬† | ¬† | ¬† | . | o1-preview | 128k | $15 / $60 | Advanced reasoning, slower | . | o1-mini | 128k | $3 / $12 | Efficient reasoning | . | Standard | ¬† | ¬† | ¬† | . | gpt-4-turbo | 128k | $10 / $30 | Previous flagship | . | gpt-3.5-turbo | 16k | $0.50 / $1.50 | Legacy standard | . | Audio / Other | ¬† | ¬† | ¬† | . | whisper-1 | - | $0.006 / min | Speech to Text | . | text-embedding-3-small | 8k | $0.02 | Embeddings | . | text-embedding-3-large | 8k | $0.13 | Embeddings | . ",
    "url": "/available-models#openai",
    
    "relUrl": "/available-models#openai"
  },"13": {
    "doc": "Available Models",
    "title": "Anthropic",
    "content": "| Model ID | Context | Cost (In/Out) | Notes | . | Claude 3.5 | ¬† | ¬† | ¬† | . | claude-3-5-sonnet-20241022 | 200k | $3 / $15 | Best balance of speed/intelligence | . | claude-3-5-haiku-20241022 | 200k | $0.80 / $4 | Extremely fast | . | Claude 3 | ¬† | ¬† | ¬† | . | claude-3-opus-20240229 | 200k | $15 / $75 | Highest reasoning (legacy) | . | claude-3-sonnet-20240229 | 200k | $3 / $15 | Balanced | . | claude-3-haiku-20240307 | 200k | $0.25 / $1.25 | Economy | . ",
    "url": "/available-models#anthropic",
    
    "relUrl": "/available-models#anthropic"
  },"14": {
    "doc": "Available Models",
    "title": "Gemini",
    "content": "| Model ID | Context | Cost (In/Out) | Notes | . | Gemini 2.0 | ¬† | ¬† | ¬† | . | gemini-2.0-flash | 1M | $0.10 / $0.40 | Next-gen speed &amp; multimodal | . | Gemini 1.5 | ¬† | ¬† | ¬† | . | gemini-1.5-pro | 2M | $1.25 / $5 | Massive context window | . | gemini-1.5-flash | 1M | $0.075 / $0.30 | High volume tasks | . | Embedding | ¬† | ¬† | ¬† | . | text-embedding-004 | - | - | Semantic search | . ",
    "url": "/available-models#gemini",
    
    "relUrl": "/available-models#gemini"
  },"15": {
    "doc": "Available Models",
    "title": "Programmatic Access",
    "content": "You can access this data programmatically using the registry: . import { LLM } from \"@node-llm/core\"; const model = LLM.models.find(\"gpt-4o\"); console.log(model.context_window); // 128000 console.log(model.pricing.text_tokens.standard.input_per_million); // 5 console.log(model.capabilities); // [\"vision\", \"function_calling\", ...] . ",
    "url": "/available-models#programmatic-access",
    
    "relUrl": "/available-models#programmatic-access"
  },"16": {
    "doc": "Available Models",
    "title": "Finding Models",
    "content": "Use the registry to find models dynamically based on capabilities: . // Find a model that supports vision and tools const visionModel = LLM.models.list().find(m =&gt; m.capabilities.includes(\"vision\") &amp;&amp; m.capabilities.includes(\"function_calling\") ); . ",
    "url": "/available-models#finding-models",
    
    "relUrl": "/available-models#finding-models"
  },"17": {
    "doc": "Chat",
    "title": "Chat",
    "content": "node-llm provides a unified chat interface across all providers (OpenAI, Gemini, Anthropic). It normalizes the differences in their APIs, allowing you to use a single set of methods for interacting with them. ",
    "url": "/core_features/chat.html",
    
    "relUrl": "/core_features/chat.html"
  },"18": {
    "doc": "Chat",
    "title": "Starting a Conversation",
    "content": "The core entry point is LLM.chat(model_id, options?). import { LLM } from \"@node-llm/core\"; // Create a chat instance const chat = LLM.chat(\"gpt-4o-mini\"); // Ask a question const response = await chat.ask(\"What is the capital of France?\"); console.log(response.content); // \"The capital of France is Paris.\" . Continuing the Conversation . The chat object maintains a history of the conversation, so you can ask follow-up questions naturally. await chat.ask(\"What is the capital of France?\"); // =&gt; \"Paris\" await chat.ask(\"What is the population there?\"); // =&gt; \"The population of Paris is approximately...\" . ",
    "url": "/core_features/chat.html#starting-a-conversation",
    
    "relUrl": "/core_features/chat.html#starting-a-conversation"
  },"19": {
    "doc": "Chat",
    "title": "System Prompts (Instructions)",
    "content": "Guide the AI‚Äôs behavior, personality, or constraints using system prompts. You can set this when creating the chat or update it later. // Option 1: Set at initialization const chat = LLM.chat(\"gpt-4o\", { systemPrompt: \"You are a helpful assistant that answers in rhyming couplets.\" }); // Option 2: Set or update later chat.withInstructions(\"Now speak like a pirate.\"); await chat.ask(\"Hello\"); // =&gt; \"Ahoy matey! The seas are calm today.\" . ",
    "url": "/core_features/chat.html#system-prompts-instructions",
    
    "relUrl": "/core_features/chat.html#system-prompts-instructions"
  },"20": {
    "doc": "Chat",
    "title": "Custom HTTP Headers",
    "content": "Some providers offer beta features or require specific headers (like for observability proxies). // Enable Anthropic's beta features const chat = LLM.chat(\"claude-3-5-sonnet\") .withRequestOptions({ headers: { \"anthropic-beta\": \"max-tokens-3-5-sonnet-2024-07-15\" } }); await chat.ask(\"Tell me about the weather\"); . ",
    "url": "/core_features/chat.html#custom-http-headers",
    
    "relUrl": "/core_features/chat.html#custom-http-headers"
  },"21": {
    "doc": "Chat",
    "title": "Raw Content Blocks (Advanced)",
    "content": "For advanced use cases like Anthropic Prompt Caching, you can pass provider-specific content blocks directly. node-llm attempts to pass array content through to the provider. // Example: Anthropic Prompt Caching const systemBlock = { type: \"text\", text: \"You are a coding assistant. (Cached context...)\", cache_control: { type: \"ephemeral\" } }; const chat = LLM.chat(\"claude-3-5-sonnet\", { systemPrompt: systemBlock as any // Cast if strict types complain }); . ",
    "url": "/core_features/chat.html#raw-content-blocks-advanced",
    
    "relUrl": "/core_features/chat.html#raw-content-blocks-advanced"
  },"22": {
    "doc": "Chat",
    "title": "Working with Different Models",
    "content": "Switching providers is as simple as changing the model ID strings. node-llm automatically detects which provider to use based on the model name. // OpenAI const gpt = LLM.chat(\"gpt-4o\"); // Anthropic const claude = LLM.chat(\"claude-3-5-sonnet-20241022\"); // Gemini const gemini = LLM.chat(\"gemini-1.5-pro\"); . ",
    "url": "/core_features/chat.html#working-with-different-models",
    
    "relUrl": "/core_features/chat.html#working-with-different-models"
  },"23": {
    "doc": "Chat",
    "title": "Temperature &amp; Creativity",
    "content": "Adjust the randomness of the model‚Äôs responses using .withTemperature(0.0 - 1.0). // Deterministic / Factual (Low Temperature) const factual = LLM.chat(\"gpt-4o\").withTemperature(0.0); // Creative / Random (High Temperature) const creative = LLM.chat(\"gpt-4o\").withTemperature(0.9); . ",
    "url": "/core_features/chat.html#temperature--creativity",
    
    "relUrl": "/core_features/chat.html#temperature--creativity"
  },"24": {
    "doc": "Chat",
    "title": "Lifecycle Events",
    "content": "Hook into the chat lifecycle for logging, UI updates, audit trails, or debugging. chat .onNewMessage(() =&gt; console.log(\"AI started typing...\")) .onToolCall((tool) =&gt; console.log(`Calling tool: ${tool.function.name}`)) .onToolResult((result) =&gt; console.log(`Tool result: ${result}`)) .onEndMessage((response) =&gt; { console.log(`Finished. Total tokens: ${response.total_tokens}`); }); await chat.ask(\"What's the weather?\"); . ",
    "url": "/core_features/chat.html#lifecycle-events",
    
    "relUrl": "/core_features/chat.html#lifecycle-events"
  },"25": {
    "doc": "Chat",
    "title": "Next Steps",
    "content": ". | Multi-modal Capabilities (Images, Audio, Files) | Structured Output (JSON Schemas, Zod) | Tool Calling | . ",
    "url": "/core_features/chat.html#next-steps",
    
    "relUrl": "/core_features/chat.html#next-steps"
  },"26": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": "Before using the library, you need to configure the provider. You can do this globally using LLM.configure. import { LLM } from \"@node-llm/core\"; import \"dotenv/config\"; LLM.configure({ provider: \"openai\", // or \"anthropic\", \"gemini\" apiKey: process.env.OPENAI_API_KEY, // Optional if set in env vars retry: { attempts: 3, delayMs: 500 }, defaultModerationModel: \"text-moderation-latest\", defaultTranscriptionModel: \"whisper-1\", // Provider specific defaults defaultEmbeddingModel: \"text-embedding-3-small\" }); . See Custom Endpoints for advanced configuration. ",
    "url": "/getting_started/configuration.html",
    
    "relUrl": "/getting_started/configuration.html"
  },"27": {
    "doc": "Custom Endpoints",
    "title": "Custom Endpoints &amp; Models",
    "content": "node-llm is flexible enough to connect to any OpenAI-compatible service and use custom models. ",
    "url": "/advanced/custom_endpoints.html#custom-endpoints--models",
    
    "relUrl": "/advanced/custom_endpoints.html#custom-endpoints--models"
  },"28": {
    "doc": "Custom Endpoints",
    "title": "OpenAI-Compatible Endpoints",
    "content": "Connect to services like Azure OpenAI, LiteLLM, or Ollama by configuring the base URL. Generic Configuration . Set OPENAI_API_BASE to your custom endpoint: . # LiteLLM export OPENAI_API_KEY=\"your-litellm-key\" export OPENAI_API_BASE=\"https://your-proxy.litellm.ai/v1\" # Ollama (Local) export OPENAI_API_KEY=\"not-needed\" export OPENAI_API_BASE=\"http://localhost:11434/v1\" . Azure OpenAI . For Azure, point OPENAI_API_BASE to your specific deployment URL. The library correctly handles URL construction even with query parameters. export OPENAI_API_KEY=\"your-azure-key\" # Include the full path to your deployment export OPENAI_API_BASE=\"https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT?api-version=2024-08-01-preview\" . Then, pass the api-key header manually when creating the chat instance: . import { LLM } from '@node-llm/core'; LLM.configure({ provider: 'openai' }); const chat = LLM.chat('gpt-4').withRequestOptions({ headers: { 'api-key': process.env.OPENAI_API_KEY } }); const response = await chat.ask('Hello Azure!'); . ",
    "url": "/advanced/custom_endpoints.html#openai-compatible-endpoints",
    
    "relUrl": "/advanced/custom_endpoints.html#openai-compatible-endpoints"
  },"29": {
    "doc": "Custom Endpoints",
    "title": "Using Custom Models (assumeModelExists)",
    "content": "If you use a model ID not in the built-in registry (e.g., custom Azure names or new models), use assumeModelExists: true to bypass validation. const chat = LLM.chat('my-company-gpt-4', { assumeModelExists: true, // Provider is typically required if not already configured globally provider: 'openai' }); await chat.ask(\"Hello\"); . This flag is available on all major methods: . // Embeddings await LLM.embed('text', { model: 'custom-embedder', assumeModelExists: true }); // Image Generation await LLM.paint('prompt', { model: 'custom-dalle', assumeModelExists: true }); . Note: When using this flag, strict capability checks (e.g., whether a model supports vision) are skipped. You are responsible for ensuring the model supports the requested features. ",
    "url": "/advanced/custom_endpoints.html#using-custom-models-assumemodelexists",
    
    "relUrl": "/advanced/custom_endpoints.html#using-custom-models-assumemodelexists"
  },"30": {
    "doc": "Custom Endpoints",
    "title": "Custom Endpoints",
    "content": " ",
    "url": "/advanced/custom_endpoints.html",
    
    "relUrl": "/advanced/custom_endpoints.html"
  },"31": {
    "doc": "Debugging & Logging",
    "title": "Debugging &amp; Logging",
    "content": "When building LLM applications, understanding what‚Äôs happening ‚Äúunder the hood‚Äù is critical. node-llm provides mechanisms to inspect raw requests and responses. ",
    "url": "/advanced/debugging.html#debugging--logging",
    
    "relUrl": "/advanced/debugging.html#debugging--logging"
  },"32": {
    "doc": "Debugging & Logging",
    "title": "Debug Mode",
    "content": "You can enable detailed debug logging by setting the NODELLM_DEBUG environment variable. This will print the raw payloads sent to and received from the providers. export NODELLM_DEBUG=true node my-app.js . Output Example: . [NodeLLM] [OpenAI] Request: POST https://api.openai.com/v1/chat/completions { \"model\": \"gpt-4o\", \"messages\": [...] } [NodeLLM] [OpenAI] Response: 200 OK { \"id\": \"chatcmpl-123\", ... } . ",
    "url": "/advanced/debugging.html#debug-mode",
    
    "relUrl": "/advanced/debugging.html#debug-mode"
  },"33": {
    "doc": "Debugging & Logging",
    "title": "Lifecycle Handlers",
    "content": "For programmatic observability (e.g., sending logs to Datadog or Sentry), use the Chat Event Handlers. chat .onNewMessage(() =&gt; logger.info(\"Chat started\")) .onEndMessage((res) =&gt; logger.info(\"Chat finished\", { tokens: res.total_tokens })); . ",
    "url": "/advanced/debugging.html#lifecycle-handlers",
    
    "relUrl": "/advanced/debugging.html#lifecycle-handlers"
  },"34": {
    "doc": "Debugging & Logging",
    "title": "Debugging & Logging",
    "content": " ",
    "url": "/advanced/debugging.html",
    
    "relUrl": "/advanced/debugging.html"
  },"35": {
    "doc": "Embeddings",
    "title": "Embeddings",
    "content": "Embeddings are vector representations of text used for semantic search, clustering, and similarity comparisons. node-llm provides a unified interface for generating embeddings across different providers. ",
    "url": "/core_features/embeddings.html",
    
    "relUrl": "/core_features/embeddings.html"
  },"36": {
    "doc": "Embeddings",
    "title": "Basic Usage",
    "content": "Single Text . import { LLM } from \"@node-llm/core\"; const embedding = await LLM.embed(\"Ruby is a programmer's best friend\"); console.log(embedding.vector); // Float32Array[] (e.g., 1536 dimensions) console.log(embedding.dimensions); // 1536 console.log(embedding.model); // \"text-embedding-3-small\" (default) console.log(embedding.usage.total_tokens); // Token count . Batch Embeddings . Always batch multiple texts in a single call when possible. This is much more efficient than calling embed in a loop. const embeddings = await LLM.embed([ \"First text\", \"Second text\", \"Third text\" ]); console.log(embeddings.vectors.length); // 3 console.log(embeddings.vectors[0]); // Vector for \"First text\" . ",
    "url": "/core_features/embeddings.html#basic-usage",
    
    "relUrl": "/core_features/embeddings.html#basic-usage"
  },"37": {
    "doc": "Embeddings",
    "title": "Configuring Models",
    "content": "By default, node-llm uses text-embedding-3-small. You can change this globally or per request. Global Configuration . LLM.configure({ defaultEmbeddingModel: \"text-embedding-3-large\" }); . Per-Request . const embedding = await LLM.embed(\"Text\", { model: \"text-embedding-004\" // Google Gemini model }); . Custom Models . For models not in the registry (e.g., Azure deployments or new releases), use assumeModelExists. const embedding = await LLM.embed(\"Text\", { model: \"new-embedding-v2\", provider: \"openai\", assumeModelExists: true }); . ",
    "url": "/core_features/embeddings.html#configuring-models",
    
    "relUrl": "/core_features/embeddings.html#configuring-models"
  },"38": {
    "doc": "Embeddings",
    "title": "Reducing Dimensions",
    "content": "Some models (like text-embedding-3-large) allow you to reduce the output dimensions to save on storage and compute, with minimal loss in accuracy. const embedding = await LLM.embed(\"Text\", { model: \"text-embedding-3-large\", dimensions: 256 }); console.log(embedding.vector.length); // 256 . ",
    "url": "/core_features/embeddings.html#reducing-dimensions",
    
    "relUrl": "/core_features/embeddings.html#reducing-dimensions"
  },"39": {
    "doc": "Embeddings",
    "title": "Best Practices",
    "content": ". | Batching: Use LLM.embed([\"text1\", \"text2\"]) instead of serial calls. | Caching: Embeddings are deterministic for a given model and text. Cache them in your database to save costs. | COSINE SIMILARITY: To compare two vectors, calculate the cosine similarity. node-llm does not include math utilities to keep the core light, but you can implement it easily: . function cosineSimilarity(A: number[], B: number[]) { const dotProduct = A.reduce((sum, a, i) =&gt; sum + a * B[i], 0); const magnitudeA = Math.sqrt(A.reduce((sum, a) =&gt; sum + a * a, 0)); const magnitudeB = Math.sqrt(B.reduce((sum, b) =&gt; sum + b * b, 0)); return dotProduct / (magnitudeA * magnitudeB); } . | . ",
    "url": "/core_features/embeddings.html#best-practices",
    
    "relUrl": "/core_features/embeddings.html#best-practices"
  },"40": {
    "doc": "Embeddings",
    "title": "Error Handling",
    "content": "Wrap calls in try/catch blocks to handle API outages or rate limits. try { await LLM.embed(\"Text\"); } catch (error) { console.error(\"Embedding failed:\", error.message); } . ",
    "url": "/core_features/embeddings.html#error-handling",
    
    "relUrl": "/core_features/embeddings.html#error-handling"
  },"41": {
    "doc": "Error Handling",
    "title": "Error Handling",
    "content": "node-llm provides a rich exception hierarchy to help you handle failures gracefully. All errors inherit from the base LLMError class. ",
    "url": "/advanced/error-handling.html",
    
    "relUrl": "/advanced/error-handling.html"
  },"42": {
    "doc": "Error Handling",
    "title": "Error Hierarchy",
    "content": ". | LLMError: Base class for all library errors. | ConfigurationError: Missing API keys or invalid config. | NotFoundError: Model or provider not found in registry. | CapabilityError: Model does not support the requested feature (e.g. vision). | APIError: Base for all upstream provider API issues. | BadRequestError (400): Invalid parameters. | AuthenticationError (401/403): Invalid API key or permissions. | RateLimitError (429): You are hitting limits. | ServerError (500+): Provider internal issues. | ServiceUnavailableError: Temporary outages or overloads. | . | . | . | . ",
    "url": "/advanced/error-handling.html#error-hierarchy",
    
    "relUrl": "/advanced/error-handling.html#error-hierarchy"
  },"43": {
    "doc": "Error Handling",
    "title": "Handling Specific Errors",
    "content": "You can catch specific errors to implement custom logic. import { LLM, RateLimitError, CapabilityError } from \"@node-llm/core\"; try { await LLM.chat(\"text-only-model\").ask(\"Analyze\", { files: [\"image.png\"] }); } catch (error) { if (error instanceof CapabilityError) { console.error(\"This model can't see images. Try gpt-4o.\"); } else if (error instanceof RateLimitError) { console.warn(\"Slowing down...\"); await sleep(5000); } else { // Re-throw unknown errors throw error; } } . ",
    "url": "/advanced/error-handling.html#handling-specific-errors",
    
    "relUrl": "/advanced/error-handling.html#handling-specific-errors"
  },"44": {
    "doc": "Error Handling",
    "title": "Accessing Response Details",
    "content": "APIError instances contain details about the failed request. try { await chat.ask(\"Create a...\"); } catch (error) { if (error instanceof APIError) { console.log(`Status: ${error.status}`); // e.g. 500 console.log(`Provider: ${error.provider}`); // e.g. \"openai\" console.log(`Raw Body:`, error.body); // { error: { message: \"...\" } } } } . ",
    "url": "/advanced/error-handling.html#accessing-response-details",
    
    "relUrl": "/advanced/error-handling.html#accessing-response-details"
  },"45": {
    "doc": "Error Handling",
    "title": "Automatic Retries",
    "content": "node-llm automatically retries transient errors (Rate Limits, 5xx Server Errors) using an exponential backoff strategy. You can configure this globally. LLM.configure({ retry: { attempts: 3, // Max retries (default: 3) delayMs: 1000, // Initial delay (default: 1000ms) multiplier: 2 // Exponential factor } }); . The library will not retry non-transient errors like BadRequestError (400) or AuthenticationError (401). ",
    "url": "/advanced/error-handling.html#automatic-retries",
    
    "relUrl": "/advanced/error-handling.html#automatic-retries"
  },"46": {
    "doc": "Error Handling",
    "title": "Debugging",
    "content": "If you are stuck, enable debug logs to see the exact request and response payloads associated with an error. export NODELLM_DEBUG=true . ",
    "url": "/advanced/error-handling.html#debugging",
    
    "relUrl": "/advanced/error-handling.html#debugging"
  },"47": {
    "doc": "Examples",
    "title": "Examples",
    "content": "A comprehensive list of runnable examples available in the examples/ directory of the repository. ",
    "url": "/examples.html",
    
    "relUrl": "/examples.html"
  },"48": {
    "doc": "Examples",
    "title": "OpenAI Examples",
    "content": "| Example | Description | . | examples/openai/chat/basic.mjs | Basic chat with streaming | . | examples/openai/chat/events.mjs | Lifecycle hooks (onNewMessage, etc) | . | examples/openai/chat/tools.mjs | Automatic tool execution | . | examples/openai/chat/structured.mjs | Zod schema validation | . | examples/openai/multimodal/vision.mjs | Image analysis via URL | . | examples/openai/multimodal/files.mjs | Analyzing local files | . | examples/openai/images/generate.mjs | DALL-E 3 Generation | . | examples/openai/safety/moderation.mjs | Custom safety thresholds | . ",
    "url": "/examples.html#openai-examples",
    
    "relUrl": "/examples.html#openai-examples"
  },"49": {
    "doc": "Examples",
    "title": "Gemini Examples",
    "content": "| Example | Description | . | examples/gemini/chat/basic.mjs | Streaming chat with Gemini 1.5 | . | examples/gemini/chat/json_mode.mjs | Native JSON mode | . | examples/gemini/multimodal/video.mjs | Analyzing video files | . | examples/gemini/multimodal/audio.mjs | Native audio understanding | . | examples/gemini/multimodal/files.mjs | Multi-file context | . ",
    "url": "/examples.html#gemini-examples",
    
    "relUrl": "/examples.html#gemini-examples"
  },"50": {
    "doc": "Examples",
    "title": "Anthropic Examples",
    "content": "| Example | Description | . | examples/anthropic/chat/basic.mjs | Claude 3.5 Sonnet Chat | . | examples/anthropic/chat/tool_use.mjs | Tool calling with Claude | . | examples/anthropic/multimodal/pdf.mjs | Native PDF analysis | . | examples/anthropic/multimodal/vision.mjs | Image understanding | . ",
    "url": "/examples.html#anthropic-examples",
    
    "relUrl": "/examples.html#anthropic-examples"
  },"51": {
    "doc": "Gemini",
    "title": "Gemini Provider",
    "content": "Google‚Äôs Gemini provider offers multimodal capabilities including native video and audio understanding. ",
    "url": "/providers/gemini.html#gemini-provider",
    
    "relUrl": "/providers/gemini.html#gemini-provider"
  },"52": {
    "doc": "Gemini",
    "title": "Configuration",
    "content": "import { LLM } from \"@node-llm/core\"; LLM.configure({ provider: \"gemini\", apiKey: process.env.GEMINI_API_KEY, // Optional if set in env }); . ",
    "url": "/providers/gemini.html#configuration",
    
    "relUrl": "/providers/gemini.html#configuration"
  },"53": {
    "doc": "Gemini",
    "title": "Specific Parameters",
    "content": "Gemini uses generationConfig and safetySettings. const chat = LLM.chat(\"gemini-1.5-pro\") .withParams({ generationConfig: { topP: 0.8, topK: 40, maxOutputTokens: 8192 }, safetySettings: [ { category: \"HARM_CATEGORY_HARASSMENT\", threshold: \"BLOCK_LOW_AND_ABOVE\" } ] }); . ",
    "url": "/providers/gemini.html#specific-parameters",
    
    "relUrl": "/providers/gemini.html#specific-parameters"
  },"54": {
    "doc": "Gemini",
    "title": "Features",
    "content": ". | Models: gemini-1.5-pro, gemini-1.5-flash, gemini-1.0-pro. | Multimodal: Supports Images, Audio, and Video files directly. | Tools: Supported. | System Instructions: Supported. | . ",
    "url": "/providers/gemini.html#features",
    
    "relUrl": "/providers/gemini.html#features"
  },"55": {
    "doc": "Gemini",
    "title": "Video Support",
    "content": "Gemini is unique in its ability to natively process video files. await chat.ask(\"What happens in this video?\", { files: [\"./video.mp4\"] }); . ",
    "url": "/providers/gemini.html#video-support",
    
    "relUrl": "/providers/gemini.html#video-support"
  },"56": {
    "doc": "Gemini",
    "title": "Gemini",
    "content": " ",
    "url": "/providers/gemini.html",
    
    "relUrl": "/providers/gemini.html"
  },"57": {
    "doc": "Image Generation",
    "title": "Image Generation",
    "content": "Generate images from text descriptions using models like DALL-E, Imagen, and others. ",
    "url": "/core_features/image-generation.html",
    
    "relUrl": "/core_features/image-generation.html"
  },"58": {
    "doc": "Image Generation",
    "title": "Basic Usage",
    "content": "The simplest way is using LLM.paint(prompt). // Uses default model (e.g. dall-e-3) const image = await LLM.paint(\"A red panda coding\"); console.log(`Image URL: ${image}`); // Acts as a string URL . ",
    "url": "/core_features/image-generation.html#basic-usage",
    
    "relUrl": "/core_features/image-generation.html#basic-usage"
  },"59": {
    "doc": "Image Generation",
    "title": "Choosing Models &amp; Sizes",
    "content": "Customize the model and dimensions. const image = await LLM.paint(\"A red panda coding\", { model: \"dall-e-3\", size: \"1024x1792\", // Portrait quality: \"hd\" // DALL-E 3 specific }); . Supported sizes vary by model. Check your provider‚Äôs documentation. ",
    "url": "/core_features/image-generation.html#choosing-models--sizes",
    
    "relUrl": "/core_features/image-generation.html#choosing-models--sizes"
  },"60": {
    "doc": "Image Generation",
    "title": "Working with the Image Object",
    "content": "The return value is a GeneratedImage object which behaves like a URL string but contains rich metadata and helper methods. const image = await LLM.paint(\"A landscape\"); // Metadata console.log(image.url); // \"https://...\" console.log(image.revisedPrompt); // \"A photorealistic landscape...\" (DALL-E 3) console.log(image.mimeType); // \"image/png\" // Check if it's base64 (some providers return data, not URLs) if (image.isBase64) { console.log(\"Image data received directly.\"); } . ",
    "url": "/core_features/image-generation.html#working-with-the-image-object",
    
    "relUrl": "/core_features/image-generation.html#working-with-the-image-object"
  },"61": {
    "doc": "Image Generation",
    "title": "Saving &amp; Processing",
    "content": "You can easily save the image or get its raw buffer for further processing (e.g., uploading to S3). // Save to disk await image.save(\"./output.png\"); // Get raw buffer (works for both URL and Base64 source) const buffer = await image.toBuffer(); console.log(`Size: ${buffer.length} bytes`); // Stream it (e.g. to HTTP response) const stream = await image.toStream(); stream.pipe(process.stdout); . ",
    "url": "/core_features/image-generation.html#saving--processing",
    
    "relUrl": "/core_features/image-generation.html#saving--processing"
  },"62": {
    "doc": "Home",
    "title": "@node-llm/core",
    "content": "One unified interface for OpenAI, Anthropic, Gemini, and local models. node-llm abstracts away the chaos of vendor-specific SDKs. It gives you a clean, streaming-first API with built-in support for Vision, Tools, and Structured Outputs. Get Started View on GitHub . ",
    "url": "/#node-llmcore",
    
    "relUrl": "/#node-llmcore"
  },"63": {
    "doc": "Home",
    "title": "‚ö° Quick Example",
    "content": "import { LLM } from \"@node-llm/core\"; // 1. Configure once LLM.configure({ provider: \"openai\" }); // 2. Chat with streaming const chat = LLM.chat(\"gpt-4o\"); for await (const chunk of chat.stream(\"Explain Node.js\")) { process.stdout.write(chunk.content); } . ",
    "url": "/#-quick-example",
    
    "relUrl": "/#-quick-example"
  },"64": {
    "doc": "Home",
    "title": "üîÆ Capabilities",
    "content": "üí¨ Unified Chat . Stop rewriting code for every provider. node-llm normalizes inputs and outputs. const chat = LLM.chat(); // Defaults to GPT-4o await chat.ask(\"Hello world\"); . üëÅÔ∏è Smart Vision &amp; Files . Pass images, PDFs, or audio files directly. We handle the base64 encoding and MIME types. await chat.ask(\"Analyze this interface\", { files: [\"./screenshot.png\", \"./specs.pdf\"] }); . üõ†Ô∏è Auto-Executing Tools . Define tools once, and the library manages the execution loop for you. const tools = [{ type: 'function', function: { name: 'get_weather', ... }, handler: async ({ loc }) =&gt; `Sunny in ${loc}` }]; await chat.withTools(tools).ask(\"Weather in Tokyo?\"); . ‚ú® Structured Output . Get type-safe JSON back using Zod schemas. import { z } from \"zod\"; const Product = z.object({ name: z.string(), price: z.number() }); const res = await chat.withSchema(Product).ask(\"Generate a gadget\"); console.log(res.parsed.name); // Type-safe access . üé® Image Generation . await LLM.paint(\"A cyberpunk city in rain\"); . üé§ Audio Transcription . await LLM.transcribe(\"meeting-recording.wav\"); . ",
    "url": "/#-capabilities",
    
    "relUrl": "/#-capabilities"
  },"65": {
    "doc": "Home",
    "title": "üöÄ Why use this over official SDKs?",
    "content": "| Feature | node-llm | Official SDKs | . | API Style | Consistent across all providers | Different for everyone | . | Streaming | Standard AsyncIterator | Callbacks/Events/Streams mixed | . | Tools | Automatic Execution Loop | Manual parsing &amp; recursion | . | Files | Path string or URL | Base64 buffers / distinct types | . | Retries | Built-in &amp; Configurable | Varies by SDK | . ",
    "url": "/#-why-use-this-over-official-sdks",
    
    "relUrl": "/#-why-use-this-over-official-sdks"
  },"66": {
    "doc": "Home",
    "title": "üìã Supported Providers",
    "content": "| Provider | Supported Features | . | OpenAI | Chat, Streaming, Tools, Vision, Audio, Images, Transcription | . | Gemini | Chat, Streaming, Tools, Vision, Audio, Video, Embeddings | . | Anthropic | Chat, Streaming, Tools, Vision, PDF Support, Structured Output | . ",
    "url": "/#-supported-providers",
    
    "relUrl": "/#-supported-providers"
  },"67": {
    "doc": "Home",
    "title": "ü´∂ Credits",
    "content": "Heavily inspired by the elegant design of RubyLLM. ",
    "url": "/#-credits",
    
    "relUrl": "/#-credits"
  },"68": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"69": {
    "doc": "Advanced",
    "title": "Advanced",
    "content": " ",
    "url": "/advanced",
    
    "relUrl": "/advanced"
  },"70": {
    "doc": "Providers",
    "title": "Providers",
    "content": " ",
    "url": "/providers",
    
    "relUrl": "/providers"
  },"71": {
    "doc": "Core Features",
    "title": "Core Features",
    "content": " ",
    "url": "/core-features",
    
    "relUrl": "/core-features"
  },"72": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": " ",
    "url": "/getting-started",
    
    "relUrl": "/getting-started"
  },"73": {
    "doc": "Models & Registry",
    "title": "Models &amp; Registry",
    "content": "node-llm includes a comprehensive, built-in registry of hundreds of models using data from Parsera. This allows you to discover models and their capabilities programmatically. ",
    "url": "/core_features/models.html#models--registry",
    
    "relUrl": "/core_features/models.html#models--registry"
  },"74": {
    "doc": "Models & Registry",
    "title": "Inspecting a Model",
    "content": "You can look up any supported model to check its context window, costs, and features. import { LLM } from \"@node-llm/core\"; const model = LLM.models.find(\"gpt-4o\"); if (model) { console.log(`Provider: ${model.provider}`); console.log(`Context Window: ${model.context_window} tokens`); console.log(`Input Price: $${model.pricing.text_tokens.standard.input_per_million}/1M`); console.log(`Output Price: $${model.pricing.text_tokens.standard.output_per_million}/1M`); } . ",
    "url": "/core_features/models.html#inspecting-a-model",
    
    "relUrl": "/core_features/models.html#inspecting-a-model"
  },"75": {
    "doc": "Models & Registry",
    "title": "Discovery by Capability",
    "content": "You can filter the registry to find models that match your requirements. Finding Vision Models . const visionModels = LLM.models.list().filter(m =&gt; m.capabilities.includes(\"vision\") ); console.log(`Found ${visionModels.length} vision-capable models.`); visionModels.forEach(m =&gt; console.log(m.id)); . Finding Tool-Use Models . const toolModels = LLM.models.list().filter(m =&gt; m.capabilities.includes(\"tools\") ); . Finding Audio Models . const audioModels = LLM.models.list().filter(m =&gt; m.capabilities.includes(\"audio_input\") ); . ",
    "url": "/core_features/models.html#discovery-by-capability",
    
    "relUrl": "/core_features/models.html#discovery-by-capability"
  },"76": {
    "doc": "Models & Registry",
    "title": "Supported Providers",
    "content": "The registry includes models from: . | OpenAI (GPT-4o, GPT-3.5, DALL-E) | Anthropic (Claude 3.5 Sonnet, Haiku, Opus) | Google Gemini (Gemini 1.5 Pro, Flash) | Vertex AI (via Gemini) | . ",
    "url": "/core_features/models.html#supported-providers",
    
    "relUrl": "/core_features/models.html#supported-providers"
  },"77": {
    "doc": "Models & Registry",
    "title": "Custom Models &amp; Endpoints",
    "content": "Sometimes you need to use models not in the registry, such as Azure OpenAI deployments, Local Models (Ollama/LM Studio), or brand new releases. Using assumeModelExists . This flag tells node-llm to bypass the registry check. Important: You MUST specify the provider when using this flag, as the system cannot infer it from the ID. const chat = LLM.chat(\"my-custom-deployment\", { provider: \"openai\", // Mandatory assumeModelExists: true }); // Note: Capability checks are bypassed (assumed true) for custom models. await chat.ask(\"Hello\"); . Custom Endpoints (e.g. Azure/Local) . To point to a custom URL (like an Azure endpoint or local proxy), configure the base URL globally. LLM.configure({ openaiApiBase: \"https://my-azure-resource.openai.azure.com\", openaiApiKey: process.env.AZURE_API_KEY }); // Now valid for all OpenAI requests const chat = LLM.chat(\"gpt-4\", { provider: \"openai\" }); . ",
    "url": "/core_features/models.html#custom-models--endpoints",
    
    "relUrl": "/core_features/models.html#custom-models--endpoints"
  },"78": {
    "doc": "Models & Registry",
    "title": "Models & Registry",
    "content": " ",
    "url": "/core_features/models.html",
    
    "relUrl": "/core_features/models.html"
  },"79": {
    "doc": "Moderation",
    "title": "Content Moderation",
    "content": "Check if text content violates safety policies using LLM.moderate. This is crucial for user-facing applications to prevent abuse. ",
    "url": "/core_features/moderation.html#content-moderation",
    
    "relUrl": "/core_features/moderation.html#content-moderation"
  },"80": {
    "doc": "Moderation",
    "title": "Basic Usage",
    "content": "The simplest check returns a flagged boolean and categories. const result = await LLM.moderate(\"I want to help everyone!\"); if (result.flagged) { console.log(`‚ùå Flagged for: ${result.flaggedCategories.join(\", \")}`); } else { console.log(\"‚úÖ Content appears safe\"); } . ",
    "url": "/core_features/moderation.html#basic-usage",
    
    "relUrl": "/core_features/moderation.html#basic-usage"
  },"81": {
    "doc": "Moderation",
    "title": "Understanding Results",
    "content": "The moderation result object provides detailed signals: . | flagged: (boolean) Overall safety check. if true, content violates provider policies. | categories: (object) Boolean flags for specific buckets (e.g., sexual: false, violence: true). | category_scores: (object) Confidence scores (0.0 - 1.0) for each category. | . const result = await LLM.moderate(\"Some controversial text\"); // Check specific categories if (result.categories.hate) { console.log(\"Hate speech detected\"); } // Check confidence levels console.log(`Violence Score: ${result.category_scores.violence}`); . Common Categories . | Sexual: Sexual content. | Hate: Content promoting hate based on identity. | Harassment: Threatening or bullying content. | Self-Harm: Promoting self-harm or suicide. | Violence: Promoting or depicting violence. | . ",
    "url": "/core_features/moderation.html#understanding-results",
    
    "relUrl": "/core_features/moderation.html#understanding-results"
  },"82": {
    "doc": "Moderation",
    "title": "Integration Patterns",
    "content": "Pre-Chat Moderation . We recommend validating user input before sending it to a Chat model to save costs and prevent jailbreaks. async function safeChat(input: string) { const mod = await LLM.moderate(input); if (mod.flagged) { throw new Error(`Content Unsafe: ${mod.flaggedCategories.join(', ')}`); } // Only proceed if safe return await chat.ask(input); } . Custom Risk Thresholds . Providers have their own thresholds for ‚Äúflagging‚Äù. You can implement stricter (or looser) logic using raw scores. const result = await LLM.moderate(userInput); // Custom strict policy: Flag anything with &gt; 0.1 confidence const isRisky = Object.entries(result.category_scores) .some(([category, score]) =&gt; score &gt; 0.1); if (isRisky) { console.warn(\"Potential risk detected (custom strict mode)\"); } . ",
    "url": "/core_features/moderation.html#integration-patterns",
    
    "relUrl": "/core_features/moderation.html#integration-patterns"
  },"83": {
    "doc": "Moderation",
    "title": "Moderation",
    "content": " ",
    "url": "/core_features/moderation.html",
    
    "relUrl": "/core_features/moderation.html"
  },"84": {
    "doc": "Multi-modal",
    "title": "Multi-modal Capabilities",
    "content": "Modern LLMs can understand more than just text. node-llm provides a unified way to pass images, audio, video, and documents to models that support them. ",
    "url": "/core_features/multimodal.html#multi-modal-capabilities",
    
    "relUrl": "/core_features/multimodal.html#multi-modal-capabilities"
  },"85": {
    "doc": "Multi-modal",
    "title": "Smart File Handling",
    "content": "You can pass local paths or URLs directly to the ask method using the files option. node-llm automatically detects the file type and formats it correctly for the specific provider. Supported File Types: . | Images: .jpg, .jpeg, .png, .gif, .webp | Videos: .mp4, .mpeg, .mov, .avi, .webm | Audio: .wav, .mp3, .ogg, .flac | Documents: .pdf, .csv, .json, .xml, .md, .txt | Code: .js, .ts, .py, .rb, .go, etc. | . ",
    "url": "/core_features/multimodal.html#smart-file-handling",
    
    "relUrl": "/core_features/multimodal.html#smart-file-handling"
  },"86": {
    "doc": "Multi-modal",
    "title": "Working with Images (Vision)",
    "content": "Vision-capable models (like gpt-4o, claude-3-5-sonnet, gemini-1.5-pro) can analyze images. const chat = LLM.chat(\"gpt-4o\"); // Analyze a local image await chat.ask(\"What's in this image?\", { files: [\"./screenshot.png\"] }); // Analyze an image from a URL await chat.ask(\"Describe this logo\", { files: [\"https://example.com/logo.png\"] }); // Compare multiple images await chat.ask(\"Compare the design of these two apps\", { files: [\"./v1-screenshot.png\", \"./v2-screenshot.png\"] }); . ",
    "url": "/core_features/multimodal.html#working-with-images-vision",
    
    "relUrl": "/core_features/multimodal.html#working-with-images-vision"
  },"87": {
    "doc": "Multi-modal",
    "title": "Working with Audio",
    "content": "Audio-capable models (like gemini-1.5-flash) can listen to audio files and answer questions about them. const chat = LLM.chat(\"gemini-1.5-flash\"); // Summarize a meeting recording await chat.ask(\"Summarize the key decisions in this meeting\", { files: [\"./meeting.mp3\"] }); // Transcribe and analyze await chat.ask(\"What was the tone of the speaker?\", { files: [\"./voicemail.wav\"] }); . Note: For pure transcription without chat, see Audio Transcription. ",
    "url": "/core_features/multimodal.html#working-with-audio",
    
    "relUrl": "/core_features/multimodal.html#working-with-audio"
  },"88": {
    "doc": "Multi-modal",
    "title": "Working with Videos",
    "content": "Video analysis is currently supported primarily by Google Gemini and limited OpenAI models. node-llm handles the upload and reference process seamlessly. const chat = LLM.chat(\"gemini-1.5-pro\"); await chat.ask(\"What happens in this video?\", { files: [\"./demo_video.mp4\"] }); . ",
    "url": "/core_features/multimodal.html#working-with-videos",
    
    "relUrl": "/core_features/multimodal.html#working-with-videos"
  },"89": {
    "doc": "Multi-modal",
    "title": "Working with Documents (PDFs &amp; Text)",
    "content": "You can provide full documents for analysis. Text &amp; Code Files . For text-based files, node-llm reads the content and passes it as text context to the model. const chat = LLM.chat(\"claude-3-5-sonnet\"); // Analyze code await chat.ask(\"Explain potential bugs in this code\", { files: [\"./app/auth.ts\"] }); . PDFs . For PDFs, providers handled differently: . | Anthropic: Supports native PDF blocks (up to 10MB). node-llm handles the base64 encoding. | Gemini: Supports PDF via File API. | OpenAI: Often requires text extraction first (unless using Assistants API, which node-llm core interacts with as Chat). Note: Ensure your provider supports the PDF modality directly or use a text extractor. | . await chat.ask(\"Summarize this contract\", { files: [\"./contract.pdf\"] }); . ",
    "url": "/core_features/multimodal.html#working-with-documents-pdfs--text",
    
    "relUrl": "/core_features/multimodal.html#working-with-documents-pdfs--text"
  },"90": {
    "doc": "Multi-modal",
    "title": "Automatic Type Detection",
    "content": "You don‚Äôt need to specify the file type; node-llm infers it from the extension. // Mix and match types await chat.ask(\"Analyze these project resources\", { files: [ \"diagram.png\", // Image \"spec.pdf\", // Document \"meeting.mp3\", // Audio \"backend.ts\" // Code ] }); . ",
    "url": "/core_features/multimodal.html#automatic-type-detection",
    
    "relUrl": "/core_features/multimodal.html#automatic-type-detection"
  },"91": {
    "doc": "Multi-modal",
    "title": "Multi-modal",
    "content": " ",
    "url": "/core_features/multimodal.html",
    
    "relUrl": "/core_features/multimodal.html"
  },"92": {
    "doc": "OpenAI",
    "title": "OpenAI Provider",
    "content": "The OpenAI provider supports the full range of node-llm features, including robust tool calling, vision, and structured outputs. ",
    "url": "/providers/openai.html#openai-provider",
    
    "relUrl": "/providers/openai.html#openai-provider"
  },"93": {
    "doc": "OpenAI",
    "title": "Configuration",
    "content": "import { LLM } from \"@node-llm/core\"; LLM.configure({ provider: \"openai\", apiKey: process.env.OPENAI_API_KEY, // Optional if set in env }); . ",
    "url": "/providers/openai.html#configuration",
    
    "relUrl": "/providers/openai.html#configuration"
  },"94": {
    "doc": "OpenAI",
    "title": "Specific Parameters",
    "content": "You can pass OpenAI-specific parameters using .withParams(). const chat = LLM.chat(\"gpt-4o\") .withParams({ seed: 42, // for deterministic output user: \"user-123\", // for user tracking presence_penalty: 0.5, frequency_penalty: 0.5 }); . ",
    "url": "/providers/openai.html#specific-parameters",
    
    "relUrl": "/providers/openai.html#specific-parameters"
  },"95": {
    "doc": "OpenAI",
    "title": "Features",
    "content": ". | Models: gpt-4o, gpt-4o-mini, gpt-4-turbo, etc. | Vision: specific models like gpt-4o support image analysis. | Tools: Fully supported, including parallel tool execution. | Structured Output: Supports strict schema validation via json_schema. | . ",
    "url": "/providers/openai.html#features",
    
    "relUrl": "/providers/openai.html#features"
  },"96": {
    "doc": "OpenAI",
    "title": "Custom Endpoints",
    "content": "OpenAI‚Äôs client is also used for compatible services like Ollama, LocalAI, and Azure OpenAI. See Custom Endpoints for details. ",
    "url": "/providers/openai.html#custom-endpoints",
    
    "relUrl": "/providers/openai.html#custom-endpoints"
  },"97": {
    "doc": "OpenAI",
    "title": "OpenAI",
    "content": " ",
    "url": "/providers/openai.html",
    
    "relUrl": "/providers/openai.html"
  },"98": {
    "doc": "Overview",
    "title": "Overview",
    "content": "node-llm provides a seamless, unified interface for interacting with multiple Large Language Model (LLM) providers. Whether you are building a simple chat bot or a complex multi-modal agentic workflow, node-llm abstracts away the provider-specific complexities. ",
    "url": "/getting_started/overview.html",
    
    "relUrl": "/getting_started/overview.html"
  },"99": {
    "doc": "Overview",
    "title": "Core Components",
    "content": "Understanding these components will help you use the framework effectively. 1. Chat . The primary interface for conversational AI. LLM.chat() creates a stateful object that manages conversation history. const chat = LLM.chat(\"gpt-4o\"); . 2. Providers . Adapters that translate the unified node-llm format into provider-specific API calls (OpenAI, Anthropic, Gemini). You rarely interact with them directly; the library handles this based on the model ID you choose. 3. Tools . Functions that the AI can execute. You define the schema and the handler, and node-llm manages the execution loop automatically. 4. Configuration . Global settings for API keys and defaults. LLM.configure({ openaiApiKey: \"sk-...\", defaultModel: \"gpt-4o\" }); . ",
    "url": "/getting_started/overview.html#core-components",
    
    "relUrl": "/getting_started/overview.html#core-components"
  },"100": {
    "doc": "Overview",
    "title": "Design Principles",
    "content": "Unified Interface . Every provider works differently. node-llm normalizes inputs (messages, images) and outputs (content, usage stats) so your code doesn‚Äôt change when you switch models. Streaming First . AI responses are slow. node-llm is built around AsyncIterator to make streaming text to the user as easy as a for await loop. Progressive Disclosure . Start simple with LLM.chat().ask(\"Hello\"). As your needs grow, you can access advanced features like raw API responses, custom headers, and token usage tracking without breaking your initial code. ",
    "url": "/getting_started/overview.html#design-principles",
    
    "relUrl": "/getting_started/overview.html#design-principles"
  },"101": {
    "doc": "Overview",
    "title": "How it Works",
    "content": ". | Normalization: Your inputs (text, images, files) are converted into a standardized format. | Routing: The library detects which provider to use based on the model ID (e.g., claude-* -&gt; Anthropic). | Execution: The request is sent. If tools are called, the library executes them and feeds the result back to the model. | Response: The final response is normalized into a consistent object. | . ",
    "url": "/getting_started/overview.html#how-it-works",
    
    "relUrl": "/getting_started/overview.html#how-it-works"
  },"102": {
    "doc": "Stream Responses",
    "title": "Stream Responses",
    "content": "For real-time interactions, node-llm supports streaming responses via standard JavaScript AsyncIterators. This allows you to display text to the user as it‚Äôs being generated, reducing perceived latency. ",
    "url": "/core_features/streaming.html",
    
    "relUrl": "/core_features/streaming.html"
  },"103": {
    "doc": "Stream Responses",
    "title": "Basic Streaming",
    "content": "Use the stream() method on a chat instance to get an iterator. const chat = LLM.chat(\"gpt-4o\"); process.stdout.write(\"Assistant: \"); for await (const chunk of chat.stream(\"Write a haiku about code.\")) { // Most chunks contain content if (chunk.content) { process.stdout.write(chunk.content); } } // =&gt; Code flows like water // Logic builds a new world now // Bugs swim in the stream . ",
    "url": "/core_features/streaming.html#basic-streaming",
    
    "relUrl": "/core_features/streaming.html#basic-streaming"
  },"104": {
    "doc": "Stream Responses",
    "title": "Understanding Chunks",
    "content": "Each chunk passed to your loop contains partial information about the response. | content: The text fragment for this specific chunk. Can be empty contextually. | role: Usually ‚Äúassistant‚Äù. | model: The model ID. | usage: (Optional) Token usage stats. Usually only present in the final chunk (provider dependent). | . for await (const chunk of chat.stream(\"Hello\")) { console.log(chunk); // { content: \"He\", role: \"assistant\", ... } // { content: \"llo\", role: \"assistant\", ... } } . ",
    "url": "/core_features/streaming.html#understanding-chunks",
    
    "relUrl": "/core_features/streaming.html#understanding-chunks"
  },"105": {
    "doc": "Stream Responses",
    "title": "Streaming with Tools",
    "content": "When tools are involved, node-llm handles the complexity for you. The stream will: . | Initial Text: Stream any text preceding the tool call. | Pause: Pause (yield nothing) while the tool executes automatically. | Resume: Resume streaming the model‚Äôs response after it sees the tool result. | . You don‚Äôt need to manually handle tool execution loops during streaming; the iterator abstracts this away. ",
    "url": "/core_features/streaming.html#streaming-with-tools",
    
    "relUrl": "/core_features/streaming.html#streaming-with-tools"
  },"106": {
    "doc": "Stream Responses",
    "title": "Error Handling",
    "content": "Stream interruptions (network failure, rate limits) will throw an error within the for await loop. Always wrap in a try/catch block. try { for await (const chunk of chat.stream(\"Generate a long story...\")) { process.stdout.write(chunk.content); } } catch (error) { console.error(\"\\n[Stream Error]\", error.message); } . ",
    "url": "/core_features/streaming.html#error-handling",
    
    "relUrl": "/core_features/streaming.html#error-handling"
  },"107": {
    "doc": "Stream Responses",
    "title": "Web Application Integration",
    "content": "Streaming is essential for modern web apps. Here is a simple example using Express: . import express from 'express'; import { LLM } from '@node-llm/core'; const app = express(); app.get('/chat', async (req, res) =&gt; { // Set headers for streaming text res.setHeader('Content-Type', 'text/plain; charset=utf-8'); res.setHeader('Transfer-Encoding', 'chunked'); const chat = LLM.chat(\"gpt-4o-mini\"); try { for await (const chunk of chat.stream(req.query.q as string)) { if (chunk.content) { res.write(chunk.content); } } res.end(); } catch (error) { res.write(`\\nError: ${error.message}`); res.end(); } }); . ",
    "url": "/core_features/streaming.html#web-application-integration",
    
    "relUrl": "/core_features/streaming.html#web-application-integration"
  },"108": {
    "doc": "Structured Output",
    "title": "Structured Output",
    "content": "Ensure the AI returns data exactly matching a specific structure. node-llm supports strict schema validation using Zod (recommended) or manual JSON schemas. This feature abstracts the provider-specific implementations (like OpenAI‚Äôs json_schema, Gemini‚Äôs responseSchema, or Anthropic‚Äôs tool-use workarounds) into a single, unified API. ",
    "url": "/core_features/structured_output.html",
    
    "relUrl": "/core_features/structured_output.html"
  },"109": {
    "doc": "Structured Output",
    "title": "Using Zod (Recommended)",
    "content": "The easiest way to define schemas is with Zod. import { LLM, z } from \"@node-llm/core\"; // Define a schema using Zod const personSchema = z.object({ name: z.string().describe(\"Person's full name\"), age: z.number().describe(\"Person's age in years\"), hobbies: z.array(z.string()).describe(\"List of hobbies\") }); const chat = LLM.chat(\"gpt-4o-mini\"); // Use .withSchema() to enforce the structure const response = await chat .withSchema(personSchema) .ask(\"Generate a person named Alice who likes hiking and coding\"); // The response is strictly validated and parsed const person = response.parsed; console.log(person.name); // \"Alice\" console.log(person.age); // e.g. 25 console.log(person.hobbies); // [\"hiking\", \"coding\"] . ",
    "url": "/core_features/structured_output.html#using-zod-recommended",
    
    "relUrl": "/core_features/structured_output.html#using-zod-recommended"
  },"110": {
    "doc": "Structured Output",
    "title": "Manual JSON Schemas",
    "content": "You can also provide a raw JSON schema object if you prefer not to use Zod. Note for OpenAI: You must strictly follow OpenAI‚Äôs requirements, such as setting additionalProperties: false. const schema = { type: \"object\", properties: { name: { type: \"string\" }, age: { type: \"integer\" } }, required: [\"name\", \"age\"], additionalProperties: false // Required for strict mode in OpenAI }; const response = await chat .withSchema(schema) .ask(\"Generate a person\"); console.log(response.parsed); // { name: \"...\", age: ... } . ",
    "url": "/core_features/structured_output.html#manual-json-schemas",
    
    "relUrl": "/core_features/structured_output.html#manual-json-schemas"
  },"111": {
    "doc": "Structured Output",
    "title": "JSON Mode",
    "content": "If you just need valid JSON but don‚Äôt want to enforce a rigid schema, you can enable JSON mode. This instructs the model to return valid JSON but gives it more freedom with the structure. chat.withRequestOptions({ responseFormat: { type: \"json_object\" } }); const response = await chat.ask(\"Generate a JSON object with a greeting\"); console.log(response.parsed); // { greeting: \"...\" } or whatever keys it chose . ",
    "url": "/core_features/structured_output.html#json-mode",
    
    "relUrl": "/core_features/structured_output.html#json-mode"
  },"112": {
    "doc": "Structured Output",
    "title": "Provider Support",
    "content": "| Provider | Method Used | Notes | . | OpenAI | response_format: { type: \"json_schema\" } | Fully supported with strict adherence. | . | Gemini | responseSchema | Supported natively. | . | Anthropic | Tool Use (Mock) | node-llm automatically creates a tool definition and forces the model to use it to simulate structured output. | . ",
    "url": "/core_features/structured_output.html#provider-support",
    
    "relUrl": "/core_features/structured_output.html#provider-support"
  },"113": {
    "doc": "Structured Output",
    "title": "Nested Schemas",
    "content": "Complex nested schemas are fully supported via Zod. const companySchema = z.object({ name: z.string(), employees: z.array(z.object({ name: z.string(), role: z.enum([\"developer\", \"designer\", \"manager\"]), skills: z.array(z.string()) })), metadata: z.object({ founded: z.number(), industry: z.string() }) }); const response = await chat .withSchema(companySchema) .ask(\"Generate a small tech startup\"); . ",
    "url": "/core_features/structured_output.html#nested-schemas",
    
    "relUrl": "/core_features/structured_output.html#nested-schemas"
  },"114": {
    "doc": "Testing",
    "title": "Testing",
    "content": "node-llm features a comprehensive test suite including high-level integration tests and granular unit tests. ",
    "url": "/advanced/testing.html",
    
    "relUrl": "/advanced/testing.html"
  },"115": {
    "doc": "Testing",
    "title": "Running Tests",
    "content": "Unit Tests . Test core logic and provider handlers in isolation without hitting any APIs. npm run test:unit . Integration Tests (VCR) . Uses Polly.js to record and replay real LLM interactions. Replay Mode (Default): Runs against recorded cassettes. Fast and requires no API keys. npm run test:integration . Record Mode: Update cassettes by hitting real APIs (requires API keys). VCR_MODE=record npm run test:integration . All recordings are automatically scrubbed of sensitive data (API keys, org IDs) before being saved to disk. ",
    "url": "/advanced/testing.html#running-tests",
    
    "relUrl": "/advanced/testing.html#running-tests"
  },"116": {
    "doc": "Token Usage",
    "title": "Token Usage Tracking",
    "content": "Track tokens for individual turns or the entire conversation to monitor costs and usage. ",
    "url": "/advanced/token_usage.html#token-usage-tracking",
    
    "relUrl": "/advanced/token_usage.html#token-usage-tracking"
  },"117": {
    "doc": "Token Usage",
    "title": "Per-Response Usage",
    "content": "Every response object contains usage metadata for that specific interaction. const response = await chat.ask(\"Hello!\"); console.log(response.input_tokens); // e.g. 10 console.log(response.output_tokens); // e.g. 5 console.log(response.cost); // Estimated cost in USD . ",
    "url": "/advanced/token_usage.html#per-response-usage",
    
    "relUrl": "/advanced/token_usage.html#per-response-usage"
  },"118": {
    "doc": "Token Usage",
    "title": "Session Totals",
    "content": "The Chat instance maintains a running total of usage for the life of that object. // Access aggregated usage for the whole session console.log(chat.totalUsage.total_tokens); console.log(chat.totalUsage.cost); . ",
    "url": "/advanced/token_usage.html#session-totals",
    
    "relUrl": "/advanced/token_usage.html#session-totals"
  },"119": {
    "doc": "Token Usage",
    "title": "Token Usage",
    "content": " ",
    "url": "/advanced/token_usage.html",
    
    "relUrl": "/advanced/token_usage.html"
  },"120": {
    "doc": "Tool Calling",
    "title": "Tool Calling",
    "content": "node-llm simplifies function calling (tool use) by handling the execution loop automatically. You define the tools, and the library invokes them when the model requests it. ",
    "url": "/core_features/tools.html",
    
    "relUrl": "/core_features/tools.html"
  },"121": {
    "doc": "Tool Calling",
    "title": "Defining a Tool",
    "content": "Tools are defined as standard objects with a type, function definition, and a handler. const weatherTool = { type: 'function', function: { name: 'get_weather', description: 'Get the current weather for a location', parameters: { type: 'object', properties: { location: { type: 'string', description: 'City and state' } }, required: ['location'] } }, handler: async ({ location }) =&gt; { // This function is executed when the model calls the tool const weather = await fetchWeatherAPI(location); // Return a string or serializable object return JSON.stringify({ location, temp: 22, unit: 'celsius', condition: 'Sunny' }); } }; . ",
    "url": "/core_features/tools.html#defining-a-tool",
    
    "relUrl": "/core_features/tools.html#defining-a-tool"
  },"122": {
    "doc": "Tool Calling",
    "title": "Using Tools in Chat",
    "content": "Use the fluent .withTool() API to register tools for a chat session. const chat = LLM.chat(\"gpt-4o\") .withTool(weatherTool); const reply = await chat.ask(\"What is the weather in London?\"); console.log(reply.content); // The model will use the info from the tool to answer: // \"The weather in London is currently 22¬∞C and sunny.\" . ",
    "url": "/core_features/tools.html#using-tools-in-chat",
    
    "relUrl": "/core_features/tools.html#using-tools-in-chat"
  },"123": {
    "doc": "Tool Calling",
    "title": "Parallel Tool Calling",
    "content": "If the provider supports it (like OpenAI and Anthropic), the model can call multiple tools in a single turn. node-llm handles the concurrent execution of these tools automatically. See examples/openai/chat/parallel-tools.mjs for a demo. ",
    "url": "/core_features/tools.html#parallel-tool-calling",
    
    "relUrl": "/core_features/tools.html#parallel-tool-calling"
  },"124": {
    "doc": "Tool Calling",
    "title": "Advanced Tool Metadata",
    "content": "Some providers support additional metadata in tool definitions, such as Anthropic‚Äôs Prompt Caching. You can include these fields in your tool definition, and node-llm will pass them through. const cachedTool = { type: 'function', function: { name: 'get_history', // ... params ... }, // Provider-specific metadata key (like 'cache_control' for Anthropic) cache_control: { type: 'ephemeral' }, handler: async () =&gt; { ... } }; . ",
    "url": "/core_features/tools.html#advanced-tool-metadata",
    
    "relUrl": "/core_features/tools.html#advanced-tool-metadata"
  },"125": {
    "doc": "Tool Calling",
    "title": "Error Handling in Tools",
    "content": "How you handle errors in your handler affects the conversation flow: . | Recoverable Errors: Return a JSON string describing the error. The model can often see this error and try to correct itself (e.g., retrying with different parameters). handler: async ({ id }) =&gt; { if (!id) return JSON.stringify({ error: \"Missing ID\" }); // ... } . | Fatal Errors: If you throw an exception inside a tool handler, node-llm catches it and feeds the error message back to the model as a ‚ÄúTool Error‚Äù. This allows the model to apologize to the user or attempt a different strategy. | . ",
    "url": "/core_features/tools.html#error-handling-in-tools",
    
    "relUrl": "/core_features/tools.html#error-handling-in-tools"
  },"126": {
    "doc": "Tool Calling",
    "title": "Security Considerations",
    "content": "Treat arguments passed to your handler as untrusted user input. | Validate: Always validate parameter types and ranges using libraries like zod inside the handler if critical. | Sanitize: Sanitize strings before using them in database queries or shell commands. | Avoid Eval: Never use eval() on inputs provided by the model. | . ",
    "url": "/core_features/tools.html#security-considerations",
    
    "relUrl": "/core_features/tools.html#security-considerations"
  },"127": {
    "doc": "Tool Calling",
    "title": "Debugging Tools",
    "content": "To see exactly what the model is calling and what your tool is returning, enable debug mode: . export NODELLM_DEBUG=true . You will see logs like: [NodeLLM] Tool call: get_weather { location: \"Paris\" } [NodeLLM] Tool result: { temp: 15 } . ",
    "url": "/core_features/tools.html#debugging-tools",
    
    "relUrl": "/core_features/tools.html#debugging-tools"
  }
}
